<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Unit 1 Learning from Hugging Face RL Course">

<title>Data Science Blog - Yuvraj Dhepe - Introduction to RL HF</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/lotus.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Data Science Blog - Yuvraj Dhepe - Introduction to RL HF">
<meta property="og:description" content="Unit 1 Learning from Hugging Face RL Course">
<meta property="og:image" content="https://github.com/Yuvraj-Dhepe/DataBlog_V1/docs/projects/abc/images/RL1_Introduction.jpg">
<meta property="og:site-name" content="Data Science Blog - Yuvraj Dhepe">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Hello There</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/notes/index.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Yuvraj-Dhepe" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#hugging-face-reinforcement-learning-course" id="toc-hugging-face-reinforcement-learning-course" class="nav-link active" data-scroll-target="#hugging-face-reinforcement-learning-course">Hugging Face Reinforcement Learning course</a>
  <ul class="collapse">
  <li><a href="#chapter-1-introduction-to-deep-reinforcement-learning" id="toc-chapter-1-introduction-to-deep-reinforcement-learning" class="nav-link" data-scroll-target="#chapter-1-introduction-to-deep-reinforcement-learning">Chapter 1: INTRODUCTION TO DEEP REINFORCEMENT LEARNING</a>
  <ul class="collapse">
  <li><a href="#the-reward-hypothesis-the-central-idea-of-reinforcement-learning" id="toc-the-reward-hypothesis-the-central-idea-of-reinforcement-learning" class="nav-link" data-scroll-target="#the-reward-hypothesis-the-central-idea-of-reinforcement-learning">The reward hypothesis: the central idea of Reinforcement Learning</a></li>
  <li><a href="#markov-property" id="toc-markov-property" class="nav-link" data-scroll-target="#markov-property">Markov Property</a></li>
  <li><a href="#observationsstates-space" id="toc-observationsstates-space" class="nav-link" data-scroll-target="#observationsstates-space">Observations/States Space</a></li>
  <li><a href="#action-space" id="toc-action-space" class="nav-link" data-scroll-target="#action-space">Action Space</a></li>
  <li><a href="#rewards-and-the-discounting" id="toc-rewards-and-the-discounting" class="nav-link" data-scroll-target="#rewards-and-the-discounting">Rewards and the discounting</a></li>
  <li><a href="#tasks" id="toc-tasks" class="nav-link" data-scroll-target="#tasks">Tasks</a></li>
  <li><a href="#exploration-vs-exploitation" id="toc-exploration-vs-exploitation" class="nav-link" data-scroll-target="#exploration-vs-exploitation">Exploration vs Exploitation</a></li>
  <li><a href="#two-main-approaches-to-solving-the-rl-problems" id="toc-two-main-approaches-to-solving-the-rl-problems" class="nav-link" data-scroll-target="#two-main-approaches-to-solving-the-rl-problems">Two main approaches to solving the RL problems</a></li>
  <li><a href="#the-deep-in-reinforcement-learning" id="toc-the-deep-in-reinforcement-learning" class="nav-link" data-scroll-target="#the-deep-in-reinforcement-learning">The Deep in Reinforcement learning</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#glossary" id="toc-glossary" class="nav-link" data-scroll-target="#glossary">Glossary</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/quarto-dev/quarto-demo/blob/main/docs/projects/abc/RL1_Introduction.ipynb" class="toc-action">Edit this page</a></p><p><a href="https://github.com/quarto-dev/quarto-demo/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to RL HF</h1>
  <div class="quarto-categories">
    <div class="quarto-category">End To End Project</div>
    <div class="quarto-category">Regression Project</div>
  </div>
  </div>

<div>
  <div class="description">
    Unit 1 Learning from Hugging Face RL Course
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="hugging-face-reinforcement-learning-course" class="level1">
<h1>Hugging Face Reinforcement Learning course</h1>
<section id="chapter-1-introduction-to-deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1-introduction-to-deep-reinforcement-learning">Chapter 1: INTRODUCTION TO DEEP REINFORCEMENT LEARNING</h2>
<ul>
<li>Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback</li>
<li>RL Process: Imagine an agent learning to play a platform game:
<ul>
<li>Our agent receives state <span class="math inline">\(S_0\)</span> from the environment - we receive the first frame of our game</li>
<li>Based on that state <span class="math inline">\(S_0\)</span>, the Agent takes action <span class="math inline">\(A_0\)</span> - our agent will move to the right</li>
<li>The environment goes to a new state <span class="math inline">\(S_1\)</span> - new frame</li>
<li>The environment gives some reward <span class="math inline">\(R_1\)</span> to the agent - we’re not dead (Positive Reward +1)</li>
</ul></li>
<li>This RL loop outputs a sequence of state, action, reward, and next state: <span class="math inline">\(S_0, A_0, R_1, S_1\)</span></li>
<li>The agent’s goal is to maximize it’s cumulative reward, callled the expected return.</li>
</ul>
<section id="the-reward-hypothesis-the-central-idea-of-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="the-reward-hypothesis-the-central-idea-of-reinforcement-learning">The reward hypothesis: the central idea of Reinforcement Learning</h3>
<ul>
<li>RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).</li>
<li>That’s why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.</li>
</ul>
</section>
<section id="markov-property" class="level3">
<h3 class="anchored" data-anchor-id="markov-property">Markov Property</h3>
<ul>
<li>Markov property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before.</li>
</ul>
</section>
<section id="observationsstates-space" class="level3">
<h3 class="anchored" data-anchor-id="observationsstates-space">Observations/States Space</h3>
<ul>
<li>Observations/States are the information our agent gets from he environment. In the case of a video game, it can be a frame, in case of a trading agent, it can be the value of a certain stock.</li>
<li>There is a differentiation to make between observation and state, however:
<ul>
<li>State s: is a complete description of the state of the world.
<ul>
<li>In a chess game, we have access to the whole board information, so we receive a state from the environment. In other words, the environment is fully observed.</li>
</ul></li>
<li>Observation o: is a partial description of the state.
<ul>
<li>In Super Mario Bros, we are in a partially observed environment. We receive an observation since we only see a part of the level.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="action-space" class="level3">
<h3 class="anchored" data-anchor-id="action-space">Action Space</h3>
<ul>
<li>Action space is the set of all possible actions in an environment.
<ul>
<li>The actions can come from a discrete or continuous space:
<ul>
<li>Discrete space: the number of possible actions is finite. Ex. In Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.</li>
<li>Continous space: A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21,1°, 21,2°, honk, turn right 20°.</li>
</ul></li>
<li><em>Taking this information into consideration is crucial because it will have importance when choosing the RL algorithm in the future.</em></li>
</ul></li>
</ul>
</section>
<section id="rewards-and-the-discounting" class="level3">
<h3 class="anchored" data-anchor-id="rewards-and-the-discounting">Rewards and the discounting</h3>
<ul>
<li>The rewared is fundamental in RL because it’s the only feedback for the agent. Because of this our agent knows if the action taken was good or not.
<ul>
<li>The cumulative reward at each time step t, equals the sum of all rewards in the sequence.</li>
</ul></li>
</ul>
<p><img src="./images/RL_1_rewards_2.jpg" alt="Alt text" title="Optional title" width="200" height="300"></p>
<p>However, in reality, we can’t just add them like that. The rewards that come sooner (at the beginning of the game) are more likely to happen since they are more predictable than the long-term future reward.</p>
<p><img src="./images/RL_1_rewards_3.jpg" alt="Alt text" title="Optional title" width="600" height="600"></p>
<ul>
<li><p>Let’s say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (that can move too). The mouse’s goal is to eat the maximum amount of cheese before being eaten by the cat.</p></li>
<li><p>As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).</p></li>
<li><p>Consequently, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we’re not really sure we’ll be able to eat it.</p></li>
<li><p>To discount the rewards, we proceed like this:</p>
<ul>
<li><ol type="1">
<li>We define a discount rate called gamma. It must be between 0 and 1. Most of the time between 0.95 and 0.99.</li>
</ol>
<ul>
<li>The larger the gamma, the smaller the discount. This means our agent cares more about the long-term reward.</li>
<li>On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).</li>
</ul></li>
<li><ol start="2" type="1">
<li>Then, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.</li>
</ol></li>
</ul></li>
</ul>
<p>Our discounted expected cumulative reward is:</p>
<p><img src="./images/RL_1_rewards_4.jpg" alt="Alt text" title="Optional title" width="600" height="600"></p>
</section>
<section id="tasks" class="level3">
<h3 class="anchored" data-anchor-id="tasks">Tasks</h3>
<ul>
<li>A task is an instance of a Reinforcement learning problem. We can have 2 types of tasks: episodic and continuing.</li>
<li>Episodic task
<ul>
<li>In this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States.
<ul>
<li>For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ends when you’re killed or you reached the end of the level.</li>
</ul></li>
</ul></li>
<li>Continuing task:
<ul>
<li>These are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.
<ul>
<li>For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="exploration-vs-exploitation" class="level3">
<h3 class="anchored" data-anchor-id="exploration-vs-exploitation">Exploration vs Exploitation</h3>
<ul>
<li>Exploration is exploring the environment by trying random actions in order to find more information about the environment</li>
<li>Exploitation is exploiting known information to maximize the reward</li>
<li><strong>Remember the goal of our RL agent is to maximize the expected cumulative reward. However, one can fall in the trap of exploiting the known rewards all the time.</strong></li>
</ul>
<p>Example - In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000). - However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation). - But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).</p>
<ul>
<li>If it’s still confusing, think of a real problem: the choice of picking a restaurant:
<ul>
<li>Exploitation: You go to the same one that you know is good every day and take the risk to miss another better restaurant.</li>
<li>Exploration: Try restaurants you never went to before, with the risk of having a bad experience but the probable opportunity of a fantastic experience.</li>
</ul></li>
<li>This is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment.
<ul>
<li>Therefore, we must define a rule that helps to handle this trade-off. We’ll see the different ways to handle it in the future units.</li>
</ul></li>
</ul>
</section>
<section id="two-main-approaches-to-solving-the-rl-problems" class="level3">
<h3 class="anchored" data-anchor-id="two-main-approaches-to-solving-the-rl-problems">Two main approaches to solving the RL problems</h3>
<ul>
<li>After taking a look at the RL framework
<ul>
<li>RL process which consists of:
<ul>
<li>Observations or States Space</li>
<li>Action Space</li>
<li>Rewards and it’s discounting</li>
<li>Tasks, i.e.&nbsp;an instance of reinforcement learning. Episodic or Continuing tasks</li>
</ul></li>
<li>Reward Hypothesis: Every goal can be described as a maximization of the expected return.</li>
<li>Markov Property</li>
<li>Exploration vs Exploitation.</li>
</ul></li>
<li>We now have to see how this whole RL framework can be used to solve the RL problems
<ul>
<li>In other words, how do we build an RL agent that can select the actions that maximize it’s expected cumulative rewards?</li>
</ul></li>
<li>Policy <span class="math inline">\(\pi\)</span>
<ul>
<li>The brain of our agent defining the behaviour of our agent</li>
<li>Describes which action to take in which state</li>
<li>This is what we want to learn to solve the RL problem via the framework.</li>
<li>2 Approaches to find the policy: Policy Based Methods and Value Based Methods</li>
</ul></li>
<li>Policy based methods:
<ul>
<li>Learn the policy function directly.
<ul>
<li>This function defines a mapping from each state to the best corresponding action or a probability distribution over the set of all the possible actions at that state</li>
<li>There are 2 types of policy,
<ul>
<li>Deterministic policy: Given a state this policy returns the same action</li>
<li>Stochastic policy: Outputs a probability distribution over all the actions in a given state.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg" width="600" height="600"></p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg" width="600" height="600"></p>
<ul>
<li>Value based methods:
<ul>
<li>Indirect way of learning policy</li>
<li>Learn a value function, which maps a state to the expected value of being at that state.
<ul>
<li>Value of that state is the expected discounted return the agent can get if it starts in that state, and then acts according to our policy.</li>
<li>Act according to our policy just means that our policy is going to the state with the highest value.</li>
</ul></li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg" width="600" height="600"></p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg" width="600" height="600"></p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg" width="600" height="600"></p>
</section>
<section id="the-deep-in-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="the-deep-in-reinforcement-learning">The Deep in Reinforcement learning</h3>
<ul>
<li>Deep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems — hence the name “deep”.</li>
<li>In the next unit, we’ll learn about two value-based algorithms: Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning.
<ul>
<li>In Q-learning approach, we use a traditional algorithm to create a Q table that helps us find what action to take for each state.</li>
<li>In Deep Q-learning approach, we will use a Neural Network (to approximate the Q value).</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg" width="600" height="600"></p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li><p>Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.</p></li>
<li><p>The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.</p></li>
<li><p>The RL process is a loop that outputs a sequence of state, action, reward and next state.</p></li>
<li><p>To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.</p></li>
<li><p>To solve an RL problem, you want to find an optimal policy. The policy is the “brain” of your agent, which will tell us what action to take given a state. The optimal policy is the one which gives you the actions that maximize the expected return.</p></li>
<li><p>There are two ways to find your optimal policy:</p>
<ul>
<li>By training your policy directly: policy-based methods.</li>
<li>By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.</li>
</ul></li>
<li><p>Finally, we speak about Deep RL because we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name “deep”.</p></li>
</ul>
</section>
<section id="glossary" class="level3">
<h3 class="anchored" data-anchor-id="glossary">Glossary</h3>
<ul>
<li>Markov Property
<ul>
<li>It implies that the action taken by our agent is conditional solely on the present state and independent of the past states and actions.</li>
</ul></li>
<li>Observations/State
<ul>
<li>State: Complete description of the state of the world.</li>
<li>Observation: Partial description of the state of the environment/world.</li>
</ul></li>
<li>Actions
<ul>
<li>Discrete Actions: Finite number of actions, such as left, right, up, and down.</li>
<li>Continuous Actions: Infinite possibility of actions; for example, in the case of self-driving cars, the driving scenario has an infinite possibility of actions occurring.</li>
</ul></li>
<li>Rewards and Discounting
<ul>
<li>Rewards: Fundamental factor in RL. Tells the agent whether the action taken is good/bad.</li>
<li>RL algorithms are focused on maximizing the cumulative reward.</li>
</ul></li>
<li>Reward Hypothesis: RL problems can be formulated as a maximisation of (cumulative) return.
<ul>
<li>Discounting is performed because rewards obtained at the start are more likely to happen as they are more predictable than long-term rewards.</li>
</ul></li>
<li>Tasks
<ul>
<li>Episodic: Has a starting point and an ending point.</li>
<li>Continuous: Has a starting point but no ending point.</li>
</ul></li>
<li>Exploration v/s Exploitation Trade-Off
<ul>
<li>Exploration: It’s all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.</li>
<li>Exploitation: It’s about exploiting what we know about the environment to gain maximum rewards.</li>
<li>Exploration-Exploitation Trade-Off: It balances how much we want to explore the environment and how much we want to exploit what we know about the environment.</li>
</ul></li>
<li>Policy
<ul>
<li>Policy: It is called the agent’s brain. It tells us what action to take, given the state.</li>
<li>Optimal Policy: Policy that maximizes the expected return when an agent acts according to it. It is learned through training.</li>
<li>Policy-based Methods:
<ul>
<li>An approach to solving RL problems.</li>
<li>In this method, the Policy is learned directly.</li>
<li>Will map each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.</li>
</ul></li>
<li>Value-based Methods:
<ul>
<li>Another approach to solving RL problems.</li>
<li>Here, instead of training a policy, we train a value function that maps each state to the expected value of being in that state.</li>
</ul></li>
</ul></li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="Yuvraj-Dhepe/DataBlog_V1" data-repo-id="R_kgDOJMncgA" data-category="Announcements" data-category-id="DIC_kwDOJMncgM4CVqbZ" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light_protanopia" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">This page is built with ❤️ and <a href="https://quarto.org">Quarto</a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/yuvraj-shivaji-dhepe-22974919a/">
      <i class="bi bi-linkedin" role="img" aria-label="My LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Yuvraj-Dhepe">
      <i class="bi bi-github" role="img" aria-label="My Github">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/@yuvrajdhepe">
      <i class="bi bi-medium" role="img" aria-label="My Medium">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>