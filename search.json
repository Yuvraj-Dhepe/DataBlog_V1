[
  {
    "objectID": "index.html#latest-projects",
    "href": "index.html#latest-projects",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": " Latest Projects",
    "text": "Latest Projects\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n4/10/23\n\n\nEnd to End Machine Learning Project on Student Performance Dataset\n\n\n\n\n7/28/22\n\n\nAtliQ Hardware Sales Insights\n\n\n\n\n2/9/22\n\n\nSpiritual Youtube Channels Data Scraping\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#noting-down-currently",
    "href": "index.html#noting-down-currently",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "üìù Noting Down Currently",
    "text": "üìù Noting Down Currently\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n3/10/23\n\n\nDocker Tutorial\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-blog",
    "href": "index.html#recent-blog",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "üìÆ Recent Blog",
    "text": "üìÆ Recent Blog\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nDummy Blog\n\n\n\n\n2/9/22\n\n\nDummy Blog\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html",
    "href": "docs/projects/B1_Sales_Insights.html",
    "title": "AtliQ Hardware Sales Insights",
    "section": "",
    "text": "AtliQ Hardware supplies computer hardware and peripherals to various clients across India and has a head office in Delhi and regional offices throughout India.\nBhavin Patel, the sales director of the company, is facing challenges in tracking the sales in the dynamically growing market and getting insights into his business.\nThe regional managers tend to give verbal feedback that paints a rosy picture and provide complex Excel files, making it challenging for Bhavin to get a clear picture of the business.\nBhavin wants simple and digestible insights, such as the top five customers, the weakest regions, and the year-to-date revenue, etc,. to make data-driven decisions for the business.\nThe ultimate goal is to improve the declining sales and make informed business decisions based on real-time data analytics.\nPowerBI can provide a solution to visualize the data and present simple and actionable insights for Bhavin.\n\nP.S. The problem statement is virtual and doesn‚Äôt relate to any real-world entity, any such occurence is just a matter of coincidence."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#problem-statement",
    "href": "docs/projects/B1_Sales_Insights.html#problem-statement",
    "title": "AtliQ Hardware Sales Insights",
    "section": "",
    "text": "AtliQ Hardware supplies computer hardware and peripherals to various clients across India and has a head office in Delhi and regional offices throughout India.\nBhavin Patel, the sales director of the company, is facing challenges in tracking the sales in the dynamically growing market and getting insights into his business.\nThe regional managers tend to give verbal feedback that paints a rosy picture and provide complex Excel files, making it challenging for Bhavin to get a clear picture of the business.\nBhavin wants simple and digestible insights, such as the top five customers, the weakest regions, and the year-to-date revenue, etc,. to make data-driven decisions for the business.\nThe ultimate goal is to improve the declining sales and make informed business decisions based on real-time data analytics.\nPowerBI can provide a solution to visualize the data and present simple and actionable insights for Bhavin.\n\nP.S. The problem statement is virtual and doesn‚Äôt relate to any real-world entity, any such occurence is just a matter of coincidence."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "href": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "title": "AtliQ Hardware Sales Insights",
    "section": "Project Planning and Data Discovery",
    "text": "Project Planning and Data Discovery\n\nProject planning using AIMS grid.\n\nBhavin patel who is a sales director of at Atliq hardware realizes the pains of getting sales insights and thinks of having a data analytical solution for this problem.\nHe calls a couple of people like an IT head or a head of data department and schedules a meeting on how exactly we should do this data analytics project.\nTogether with IT head, he also gathers other stake holders in the room together and they brainstorm on project planning using AIMS grid.\n\nAIMS grid\n\nAIMS grid is a project management tool having four components.\n\nFirst component is Purpose\n\nDetermining what is the pain point and what needs to be done to resolve the pain point\n\nSecond component is the Stakeholders\n\nIn this project who all will be involved\nThe main stakeholders involved for this data analytical projects are the marketing team ,the sales team who is facing these issues to do proper sales analysis\nIT team (The Falcons) a group of software engineers that AtliQ hardware has who manages all the software requirements of Atliq\nThe data analytics (The Data Masters) team which is inhouse\n\nThird component is the end result i.e., once the project is over what needs to be achieved\n\nFor the data analytics project the end result is to have a power bi dashboard something that our sales director or even Regional Managers can go and look into and it gives you a real-time information on the sales numbers\n\nFourth component is the success criteria after the project is over, how do we define that the project was successful\n\nIn our project scenario the success criteria would be, we want the costs to go down by 10% on management of these sales data\nWe want to utilize the saved business time by the use of dashboard to take data driven descisions and increase the growth of company by let‚Äôs say 10%\nDashboards uncovering sales order insights with latest data available\nSales team able to take better decisions and prove 10% cost savings of total spend\nSales Analysts stop data gathering manually (merging excel files) in order to save business time and reinvest it value added activity\n\n\n\nOther Important points\n\nFalcons team, manages the software of Atliq Sales Management which is keeping track of all the sales number so whenever they sell any computer or any hard days in any local region this software is printing the invoice so it has all the records stored in a MySQL database.\nData Masters team will reach out to Falcons and to use the AtliQ SQL database because this is the database which has all the records that we need for our analytics and what we‚Äôll do is we‚Äôll integrate MySQL. We will use MySQL as a source in our power bi tool and we will build dashboard on top of it.\n\nReal World Scenarios\n\nIn real world direct access of data from IT team is not allowed cause if the data volume is high we want to make sure that MySQL database is not affected by the queries that Falcons are doing in your PowerBi, so many times companies, have a Data Warehouse as part of the Data Ecosystem.\nMySQL which is also known as OLTP which is online transaction processing system it is a very critical system and a company cannot afford for that system to go down otherwise the regular sales operations gets hampered.\nFor any business analytics is important but then it‚Äôs a secondary thing so what companies do is they pull the data from OLTP which is MySQL in our case they do all the transformation which is also called ETL which means extract transform and load and after doing that transformation they store the data in a data warehouse.\nThere are various types of Data Warehouse management tools like Snowflake and there are various ways to transform the data like using Pandas or other data processing libraries.\nMost companies working on huge data analytical projects have another in-house teams who manage the data warehouse known as Data Engineers (The Data Miners).\n\nConclusion & Data Discovery\n\nAssuming our data is not that big, the data is simply taken from the Falcons, by our Data Masters.\nThe Data Masters will plug PowerBI directly to SQL database and build the necessary analytical dashboard. Also some times data analysts will spend their time to capture information for analytics, which might not be available in the organization at all. But for this project we will not do that.\nOnce the Data Masters have the data, they do data cleaning (data wrangling) and data merging (data munging), all this can be done via specialized software. But our data being simple we will do it via PowerBi itself."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "href": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "title": "AtliQ Hardware Sales Insights",
    "section": "Data Cleaning and Data Merging or Wrangling",
    "text": "Data Cleaning and Data Merging or Wrangling\n\nLoad the data in SQL taken from the Falcons team\n\nFind Basic Insights about the data in SQL.\nFind out few inconsistencies in data in SQL itself and make a note to remove them in PowerBI later.\n\nDo the ETL in PowerBi\n\nFirst Load the data\nDo the data modelling, i.e., link the tables well, along with their relationships.\nStart data transformations on the data to make it good enough for the data analysis.\nOnce the ETL is done, now start building the dashboards\n\n\n\nBasic Insights from DB\n\n\nDB_Schema\n\n\nCode\nfrom IPython import display\ndisplay.Image(\"./images/DB_Schema.jpg\")\n# Can use ![alternative text](path-to-image) but it works out of zsh\n\n\n\n\n\n\n\nDB_Queries\n\nCustomers Table\n\nGet the total number of customers doing business with AtliQ hardwares\n\nInsights: There are total of 38 customers doing business with AtliQ hardwares\n\nSELECT count(*) FROM sales.customers; \nGet count of customers according to their types\n\nInsights: Only 2 customer types i.e.¬†of Brick Mortar and E-Commerce\n\nSELECT count(cs.customer_code) as total_customer_per_type, cs.customer_type \nFROM sales.CUSTOMERS as cs \nGROUP BY cs.customer_type; \nChecking unique customers in the customer category.\n\nInsights: All the 38 customers are unique\n\nSELECT COUNT(DISTINCT tb.custmer_name) as total_unique_customer_name_count\nFROM \n( \n# Seeing whether each customer_code belongs to unique customer name or not. :- It does.\nSELECT cs.custmer_name, COUNT(cs.customer_code) as total_unique_customer_names\nFROM sales.customers as cs\nGROUP BY cs.custmer_name\n) as tb;\n\n\n\nDate Table\n\nInsights: Total unique dates : 1126 and that is the total dates in the table itself.\nSELECT (count(distinct dt.date)) as total_unique_dates  \nFROM sales.date as dt;\nInsight: Seeing the trend of number of transactions each year\nSELECT dt.year, count(dt.year) as total_transactions_in_each_year \nFROM sales.date as dt\nGROUP BY dt.year;\nInsights: We have on average 30 transactions per month in each year\nSELECT \n    dt.month_name,dt.year,count(dt.month_name) as transactions_per_month_per_year\nFROM\n    sales.date AS dt\nGROUP BY dt.year, dt.month_name;\n\n\n\nMarket table\n\nInsights: Total of 17 markets each having it‚Äôs own unique identifier code\nSELECT count(mk.markets_name) \nFROM sales.markets as mk;\nInconsistency:\n\nThere are only 2 international sales in the data, so it‚Äôs better to not consider them as we don‚Äôt have much information about them now.\n\n\n\n\nTransactions Table\n\nInsights: There are a total of 150283 transaction.\nSELECT count(*) FROM sales.transactions;\nInsights: The total_sales_qty: 2444415 and reverified the same via net_sales_per_month_per_year\nSELECT SUM(tr.sales_qty) as net_sales_in_all\nFROM sales.transactions as tr;\nInsights: Finding the different type of currencies used while doing the transactions.\nSELECT distinct(tr.currency) ,count(*) as number_of_transactions_currency_wise\nFROM sales.transactions as tr\nGROUP BY tr.currency;\nInconsistencies:\n\nFew transactions are in USD, they need to be converted to INR\nThe currency column, has an inherent error to show INR and USD 2 times when grouped by currency. The error was because entry in currency field was INR USDhile creating the data.\n\nRemoved inconsistency by formating the table well.\n\n\n\n\n\nProducts Table\n\nInsights: Finding the distribution of product type\nSELECT pr.product_type, count(pr.product_code) AS product_count_forthis_type\nFROM sales.products as pr\nGROUP BY pr.product_type;\nInsights: Every product code is unique\nSELECT count(distinct pr.product_code) \nFROM sales.products as pr;\nSELECT count(*) FROM sales.products;\nSELECT * FROM sales.products;\n\n\n\n\nGeneral Queries\n\nInsights: Getting revenues per year per product\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Getting revenues per year per product in Chennaicustomers\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020 and st.market_code = \"Mark001\"\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Found out which market makes the most of the sales and which one the least.\nSELECT mk.markets_code, sum(tr.sales_qty) as net_sales_per_market, mk.markets_name\nFROM sales.transactions as tr\nJOIN sales.markets as mk\nON tr.market_code = mk.markets_code\nGROUP BY mk.markets_code\nORDER BY net_sales_per_market DESC;\nInsights: Finding which year has made the most of the transactions and in which month\nSELECT sum(net_sales_per_month_per_year)\nFROM \n( \nSELECT dt.year, dt.month_name, sum(tr.sales_qty) AS net_sales_per_month_per_year\nFROM sales.transactions as tr\nJOIN sales.date as dt\nON dt.date = tr.order_date\nGROUP BY dt.year, dt.month_name\nORDER BY net_sales_per_month_per_year\n) AS tb;\nInsights: Finding sales_qty per product, looking at it seems like own brand is in more demand\nSELECT pr.product_code, sum(tr.sales_qty) as net_sales_per_product, pr.product_type\nFROM sales.transactions as tr\nJOIN sales.products as pr\nON pr.product_code = tr.product_code\nGROUP BY pr.product_code\nORDER BY net_sales_per_product DESC;\n\n\n\n\nETL\n\n\nDid a bit of Data Modelling, linking tables well of the DB\nRemoved the inconsistencies in the PowerBI from Market and Transaction table"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "href": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "title": "AtliQ Hardware Sales Insights",
    "section": "Dashboard Created in PowerBI",
    "text": "Dashboard Created in PowerBI\n\nP.S. Viewing the dashboard requires PowerBI professional account.\n\nIf the viewer doesn‚Äôt have one, kindly use the pdf below to view overall analytical dashboard or the following link to download it.\n\n\nDownload File\n\n\n\n        \n        \n        \n    \n\n\n\n\nCode\n%%html\n&lt;div class=\"row\"&gt;\n    &lt;div class=\"col-md-12\"&gt;\n        &lt;iframe title=\"sales_insight_pbi\" width=\"100%\" height=\"500\" src=\"https://app.powerbi.com/reportEmbed?reportId=43d06eb5-ec4b-4b42-98c9-923f1574632b&autoAuth=true&ctid=3531ade6-1052-4651-9242-3199d6164d67\" frameborder=\"0\"&gt;\n        &lt;/iframe&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\n\n\n    \n        \n        \n    \n\n\n\n\nMajor Insights from the Dashboard\n\n\nKey Insights Dashboard\n\nDelhi NCR seems to give major Revenue and Sales Qty followed by Mumbai, Ahemdabad and other city ares.\nBrick n Mortar Seems to contribute to majority of Revenue\nProduct revenue has an error, we can‚Äôt see the major revenue generating product code\nOver the years the revenue trend seems to decline\nRevenue seems to increase in Q3 n Q4 generally\n\nProfit Analysis Dashboard\n\nThough revenue contribution of Delhi n major city markets is more, the profit contribution is not much high for them.\n\nAction points for AtliQ:\n\n\nFocus more on the major profit% generating cities\n\n\nFind why the profit % is less in major revenue generating cities\n\n\n\nFocus on the customers which provide more Profit Margin Contribution by encouraging them like by giving them discounts\n\nPeformance Insights Dashboard\n\nAnalyze the data by breakdown to zones, markets and customers\nObserve the zones that don‚Äôt fulfill the profit margin\nObserve Previous year revenue trend as a comparision with current year revenue trend"
  },
  {
    "objectID": "docs/projects/MLConcepts.html",
    "href": "docs/projects/MLConcepts.html",
    "title": "Machine Learning Concepts",
    "section": "",
    "text": "Linear Models\n\nLogistic Regression StatQuest Vid\n\nLogistic regression needs an adequate sample to represent values across all the response categories. Without a larger, representative sample, the model may not have sufficient statistical power to detect a significant effect.\nLogistic regression can also be prone to overfitting, particularly when there is a high number of predictor variables within the model. Regularization is typically used to penalize parameters large coefficients when the model suffers from high dimensionality.\n\n\n\nMetrics\n\nAlways remember, that the precision is given by considering that particular class as 1. \\[\n\\\\\n\\]\nFor example if we are calculating precision for Normal class, then normal is class 1 and pneumonia is class 0 and vice-versa. So basically the class which is 1 is the positive wali. The class which is taken positive depends on the problem as well. In classification report of any model, given y_true and y_pred, the classes are mentioned to the left \\[\n\\\\\n\\]\nPrecision (calculated wrt class): Jitno ko positive bol raha hein unme se actual positive kitne hein. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. \\[\n\\\\\n\\] \\[\nPrecision = \\frac{TP}{TP+FP}\\\\\n\\] \\[\n\\\\\n\\]\nSpecificity(calculated wrt class): Sare Actual negative mein se sahi kitne yaad hein. The specificity is intuitively the ability of the classifier to find all the -ve samples. (1-Sensitivity gives the FPR) \\[\n\\\\\n\\] \\[\nSpecificity = \\frac{TN}{TN+FP}\\\\\n\\] \\[\n\\\\\n\\]\nRecall or Sensitivity or TPR (calculated wrt class): Sare actual positives mein se sahi kitne yaad hein. The recall is intuitively the ability of the classifier to find all the positive samples. \\[\n\\\\\n\\] \\[\nRecall = \\frac{TP}{TP+FN}\\\\\n\\] \\[\n\\\\\n\\]\nAccuracy(calculated wrt model): Sab me se sahi kitno ko pakda. \\[\n\\\\\n\\] \\[\nAccuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\\\\n\\] \\[\n\\\\\n\\]\n\\(F_{\\beta}\\text{score}\\) : (calculated wrt class and problem)\\[\n\\\\\n\\] \\[\nF_{\\beta} = (1+\\beta^2)\\frac{*Precision*Recall}{\\beta^2*Precision+Recall}\\\\\n\\] \\[\n\\\\\n\\]\nF1-Score is used as equal weight for precision and recall, while F0.5 is biased towards precision.\n\nFor pneumonia and normal problem, considering Pneumonia as positive class.\nBetter situation is to have FP from model, rather than FN. So, we have to give preference Recall where we minimize the FN tendency. Thus we will use F2 Score to give more preference to recall than precision.\n\n\n\\[\nF2= \\frac{5*Precision*Recall}{4*Precision+Recall}\n\\] \\[\n\\\\\n\\]\n\\[\nF1= \\frac{2*Precision*Recall}{Precision+Recall}= \\frac{2*TP}{2*TP+FP+FN}\\\\\n\\] \\[\n\\\\\n\\]\n\\[\nF0.5=\\frac{1.25*Precision*Recall}{0.25*Precision+Recall}\\\\\n\\]\n\nMacro Average (macro avg):\n\nThe macro average calculates the average metric value for each class independently and then takes the unweighted mean of those values.\nIt treats each class equally and gives equal importance to all classes, regardless of their support (the number of instances in each class).\nMacro average is useful when you want to evaluate the overall performance of the model without considering class imbalance.\n\nWeighted Average (weighted avg):\n\nThe weighted average calculates the average metric value for each class independently and then takes the weighted mean of those values, where the weight is proportional to the support of each class.\nIt gives more importance to classes with higher support and less importance to classes with lower support.\nWeighted average is useful when you want to evaluate the performance of the model while considering class imbalance. It reflects the overall performance taking into account the distribution of instances across different classes.\nIn the classification report you provided, the precision, recall, and F1-score are reported for each class (1, 2, 3, and 4), as well as the overall accuracy. The macro average and weighted average are additional summary metrics that provide a single value representing the average performance across all classes.\n\nInterpretting ROC Curve: StatQuestVideo\n\nROC curves make it easy to identify the best threshold for making a decision\nIf AUC for an ROC curve is higher than the other, we would use that model in reality, but it also depends on the problem at hand like in terms of false positives are good or false negatives\nAlso interpret the curve in terms of the axes, for example a point at 1,1 in ROC means we are classifying all the positive ones well, but we even have high false -ves, so if we move parallely backwards, that threshold is good, since our true positive rate keeps high and false postive rate is decreasing\n\nAUC:"
  },
  {
    "objectID": "docs/projects/images/Image Downloader.html",
    "href": "docs/projects/images/Image Downloader.html",
    "title": "Getting inside the images folder",
    "section": "",
    "text": "%cd side_projects/learn_tools/quarto/Blog/docs/projects/images/\n\n/home/yuvi_dh/side_projects/learn_tools/quarto/Blog/docs/projects/images\n\n\n\nRL Course Images"
  },
  {
    "objectID": "docs/projects/StudPerformance_end_to_end.html",
    "href": "docs/projects/StudPerformance_end_to_end.html",
    "title": "End to End Machine Learning Project on Student Performance Dataset",
    "section": "",
    "text": "These days it‚Äôs getting more common for a person in data team, to wear multiple hats in terms of bringing insights from the sea of data. To gain experience on what it means to multiple hats, I created this end to to end project on a simple data set of Students Performance Dataset from Kaggle.\nThe goal of this project is not to derive any magical insights from data, rather, to do a comprehensive work on building an end to end project which includes but is not limited to:\n\nCreating modular folder structure\nDeciding on the dataset\nSetup of the environment\nDesign and Develop Components and Pipelines, where components interact with data in backend, whereas the pipelines interact with the user and components to get predictions from a trained ML model and finally provide result to the user.\n\nFollow the actions mentioned below to make your own copy of this end to end project\nYou Can Try the WebApp created for this project before you get your hands dirty\nNotebook checkpoints\n\nSTAR ANALYSIS\nExplained every point of the star method, step by step in detail.\n\nEx. Action will be broken down to A1, A2 to follow up the notebook.\nWe use acronyms like T1 representing Task 1, A1.1 representing subaction 1 of Action 1.\n\n\n\n\n\n\n\nSituation: To gain experience in end-to-end machine learning project development and deployment\nTask: Create a machine learning project from scratch, following the entire pipeline from data collection to model deployment\nAction: Develop a project idea, collect and preprocess the data, perform EDA on data, decide on design and training of the machine learning model, evaluate the model‚Äôs performance, and deploy the model into a production - environment\nResult: Gain hands-on experience in end-to-end machine learning project development and deployment, with a fully functional machine learning system that can be used for real-world applications\n\n\n\n\nS1. Need of gaining exposure in real-world ML project development and deployment\nS2. A way to improve my Data Science profile, with such projects\nS3. Building skillset to be of use in the real-world, and not be limited to books\n\nWith the situation being clear let‚Äôs jump to a bit about task that was required to be done for this situation\n\n\n\n\nT1. Creating a folder structure, for a real-world entity project.\n\nUses: Introduction of Modularity to the project, rather than a single jupyter notebook file doing all the job.\n\nT2. Creating an environment and setup file to run this ML pipeline from scratch.\nA. Developing an End to End ML pipeline and then performing web deployment for using the ML model.\n\nWith the basic overview of task now, let‚Äôs look onto every task in details\n\n\n\nCreating a folder structure for our real-world project. This is an essential part for any real-world code project, as it introduces modularity to our code. This modularity helps us to deal with complexity of huge projects in simple way, where a team can work together on different parts of the project, re-use each others work and combine it all at the end.\n\n\n\n\nFirst setup a github repo (ML_Web_Project is my repo), keeping all the options to default.\nLocally setup a folder (END_To_END_ML_PROJECT is my base local folder setup on WSL, but the one can use windows or mac as well)\n\nOpen this directory in vscode\nOpen a terminal\n\nSecondly let‚Äôs create a conda environment named venv into this local folder, so to have packages locally to run the project. bash     conda create -p venv python==3.8 -y\n\nActivate this environment from the base folder\n\nconda activate venv/ # don't forget '/' cause it tells that this environment is in a folder named venv\nLink the local folder to the github repo\n\nFirst do git init in the local folder\nFollow all the steps mentioned in the github repo you created to do the syncing of local folder to the repo.\nAfter the update of git back in 2021, one needs to setup ssh-keys to use the github repo or use tokens, I prefer to use ssh-keys, follow the steps here.\nCreate a default .gitignore for python in github repo online.\nFinally do a git pull, to sync the changes locally as well.\nLater on whenever there are enough changes to the local code, follow the steps of git add, commit and push with a useful commit message.\n\nBy now local repo should have a .gitignore, README.md, venv/ in their local repo, after this create the following folder structure locally.\n\n- END_TO_END_ML_PROJECT\n    - setup.py # The setup script is the center of all activity in building, distributing, and installing modules that are necessary to run the ML pipeline. # Consider it as the core of the ML Pipeline. This setup.py will help to use our ML pipeline as a package itself and can even be deployed to Pypi.\n    - requirements.txt # All packages that need to be installed before running the project. # This is the part that gives energy to the core.\n    - assets\n        - data # The folder which consist of datasets used in the project.\n            - StudentsPerformance.csv\n        - files\n            - notebook # jupyter notebooks, consisting of all codes which helps to find patterns in data and give a big picture code, later to be broken down into src folders.\n                - EDA_notebook.ipynb \n                - Model_train.ipynb\n        - images # to store plot images        \n    - src # The backbone containing all the source codes for creation of ML pipeline package.\n        - __init__.py\n        - exception.py # Helps in producing custom exceptions.\n        - logger.py # Contains the code that will help in creation of logs, and trace errors if caused any during the realtime.\n        - utils.py # Contains all the utilities that can be reused across the whole project.\n        - components # The major components of the project, which deal with data cleaning, transformation, model training etc.\n            - __init__.py\n            - data_ingestion.py\n            - data_transformation.py\n            - model_trainer.py\n        - pipeline # The complete pipelines built via use of components for further deployment of the model.\n            - __init__.py\n            - predict_pipeline.py\n            - train_pipeline.py\n\n\n\n\n\nCreating an environment and setup file which later can be used to condense our ML pipeline in form of package. In this part we build the foundation for our ML pipeline, by creating the code for setup.py file.\n\n\n\n\n\nCode\nfrom setuptools import find_packages,setup\nfrom typing import List\n\ndef get_requirements(file_path:str)-&gt;List[str]:\n    '''\n    This function will return the list of requirements\n    '''\n    requirements = []\n    file = open(file_path,'r')\n    \n    for line in file:\n        if \"-e .\" not in line:\n            requirements.append(line.strip('\\n'))\n    file.close()\n    \n    #print(requirements)\n    return requirements\n    \n# With this setup we parse our requirements file to get the requirements installed for our project, one can make this static via use of package names in form of a list, instead of parsing a requirements file.\nsetup(\n    name='mlproject',\n    version='0.0.1',\n    author='&lt;Your Name&gt;',\n    author_email='&lt;Your Email&gt;',\n    packages=find_packages(), # This will use the codes or modules that we write for our ML pipeline, to ensure that our every module can be used for building the package, we have a __init__.py in src, or any directory that can be reused.\n    install_requires=get_requirements('requirements.txt') \n)\n\n\n\ncontents of requirements.txt file\n\npandas\nnumpy\nseaborn\nmatplotlib\nscikit-learn\ncatboost\nxgboost\ndill\ntensorboard\n-e . # This triggers the setup .py file automatically, but this is not readed when setup.py is called as per our above code.\n\nOnce these 2 files are setup, simply run:\n\npip install -r requirements.txt\n\nThis will install all the necessary packages in our virtual environment and create a new directory .egg-info which will help to create the ML pipeline package for us.\n\n\n\n\n\n\n\nA1. Project Idea: Using a student performance data to predict it‚Äôs grades or scores, depending on the other features of the dataset.\nA2. Data Collection and Preprocessing: We first do all EDA in a jupyter notebook to find patterns in the data and getting to know the type of preprocessing required to be done on the dataset.\n\nFor simple application the data is simply imported in form of csv file, but all this can even be done by getting data from Data Warehouse as well.\n\nA3. Design and Development of ML pipeline components: After EDA, we try to create simple modular codes in a jupyter notebook, which do the job of development, training and evaluation of ML model. Later these modular codes are more or less split into the folder structure that we created earlier.\nA4. Deployment of model into a production environment: We use cloud tools like AWS or Streamlit or Flask n Django or any other web service to deploy the ML model online to be used on realtime data provided by user or fetched from a source.\n\n\n\n\nProject Idea\n\nWe will use a simple student performance dataset, to predict the child‚Äôs maths scores via the rest of the features of the dataset.\nWe will be using this dataset, because it‚Äôs having a mixed of categorical and numerical features, we can have a good amount of EDA done on this simple data, and last but not the least train many regression algorithms on this simple data easily.\n\nData Collection & Preprocessing\n\nWe will use jupyter notebooks, to majority of the EDA, and finding the patterns.\n\nLink to EDA Ipynb File\n\nOnce the EDA is done, we will also have basic models run on the data, in another jupyter notebook, so that we have basic model pipeline code in place as well.\n\nLink to Models Ipynb File\n\n\nInsights from the EDA n Model Training, I have mentioned in a brief on my github, you can view it here, with the insights in place, let‚Äôs begin with design and development of ML pipeline.\n\n\n\n\n\nDesign and Development of ML pipeline components in form of modular codes\nSteps\n\nCreation of utility codes, logging and exception handling module that will be used all over the components, pipelines.\nCreation of Components modules inside the package consisting of Data Ingestion, Data Transformation and Model Trainer Component.\n\nCreation of train and predict pipelines modules that will be connected to the above components, and will be a pipeline connecting the frontend user and the backend model of Machine learning.\n\n\n\n\n\n\n\n\n\nCode\n#Common functionalities for the whole project.\nimport os\nimport sys\n\nimport dill\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\n\nfrom src.exception import CustomException\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\ndef save_object(file_path,obj):\n    try:\n        dir_path = os.path.dirname(file_path)\n        os.makedirs(dir_path,exist_ok=True)\n        \n        file_obj = open(file_path,\"wb\")\n        dill.dump(obj,file_obj)\n        \n    except Exception as e:\n        raise CustomException(e,sys)\n\ndef evaluate_models(X_train, y_train, X_test,y_test,models, param):\n    try:\n        report = {}\n        \n        for i in range(len(list(models))):\n            model = list(models.values())[i]\n            para=param[list(models.keys())[i]]\n\n            gs = GridSearchCV(model,para,cv=3)\n            gs.fit(X_train,y_train)\n\n            model.set_params(**gs.best_params_)\n            model.fit(X_train,y_train)\n            \n            #model.fit(X_train,y_train)\n            \n            y_train_pred = model.predict(X_train)\n            y_test_pred = model.predict(X_test)\n            \n            train_model_score = r2_score(y_train,y_train_pred)\n            test_model_score = r2_score(y_test,y_test_pred)\n            \n            report[list(models.keys())[i]] = test_model_score\n        \n        return report\n    \n    except Exception as e:\n        raise CustomException(e,sys)\n            \ndef load_object(file_path):\n    try:\n        file_obj = open(file_path,\"rb\")\n        return dill.load(file_obj)\n        file_obj.close()\n        \n    except Exception as e:\n        raise CustomException(e,sys)\n\ndef create_plot(y_test, y_pred, type, model_name, xlabel = \"Actual Math Score\", ylabel=\"Predicted Math Score\", file_name = \"Actual vs Predicted\"):\n    \"\"\"\n    A function to create a plot and save it to a file.\n    \"\"\"\n    if type == \"scatter\":\n        title = f\"{model_name}'s Actual vs Predicted Values Scatterplot\"\n        plt.scatter(y_test, y_pred)\n        plt.title(title)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n        directory = \"./assets/images/\"\n        plt.savefig(f\"{directory}{file_name}\")\n        \n    elif type == \"reg\":\n        title = f\"{model_name}'s Actual vs Predicted Values Regplot\"\n        sns.regplot(x=y_test,y=y_pred,ci=None,color ='red');\n        plt.title(title)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n        directory = \"./assets/images/\"\n        plt.savefig(f\"{directory}{file_name}_regplot\")\n\n\n\n\n\n\n\nCode\n# Logger is for the purpose of logging all the events in the program from execution to termination.\n# For example, whenever there is an exception, we can log the exception info in a file via use of logger.\n\n# Read logger documentation at https://docs.python.org/3/library/logging.html\n# Logger is for the purpose of logging all the events in the program from execution to termination.\n# For example, whenever there is an exception, we can log the exception info in a file via use of logger.\n\n# Read logger documentation at https://docs.python.org/3/library/logging.html\nimport logging\nimport os\nfrom datetime import datetime\n\nLOG_FILE_NAME = f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\nlogs_path = os.path.join(os.getcwd(), \"logs\",LOG_FILE_NAME) # This will create logs folder in the same working directory where this file is present\nos.makedirs(logs_path,exist_ok=True) # Keep appending the logs in the same directory even if there are multiple runs of the program\n\nLOG_FILE_PATH = os.path.join(logs_path,LOG_FILE_NAME)\n\nlogging.basicConfig(filename=LOG_FILE_PATH,\n                    level=logging.INFO,\n                    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s: %(message)s\",\n                    datefmt='%m/%d/%Y %I:%M:%S %p'\n                    ) #This is the change of basic configuration for the logger\n\nif __name__ == '__main__':\n    logging.info(\"This is a test log\")\n    logging.warning(\"This is a warning log\")\n    logging.error(\"This is an error log\")\n    logging.critical(\"This is a critical log\")\n\n\n\n\n\n\n\nCode\n# We use this custom exception handling in the project to handle all the errors that will come into the project, simply we can say that we are handling all the errors that will come into the project in a single place.\n\nimport sys\n\n# Sys module in python provides various functions and variables that are used to manipulate different parts of the python runtime environment. It allows operating on the python interpreter as it provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n# Read more about sys module here: https://docs.python.org/3/library/sys.html\nfrom src.logger import logging\n\n\ndef error_message_detail(error,error_detail:sys):\n    _,_,exec_tb = error_detail.exc_info()\n    file_name = exec_tb.tb_frame.f_code.co_filename\n    error_message = f\"Error occured in python script name {file_name} on line number {exec_tb.tb_lineno} and error is {str(error)}\"\n    \n    return error_message\n    \nclass CustomException(Exception):\n    def __init__(self,error_message,error_detail:sys):\n        super().__init__(error_message)\n        self.error_message = error_message_detail(error_message,error_detail= error_detail)\n        #self.error_detail = error_detail        \n        \n    def __str__(self):\n        return f\"{self.error_message}\"\n\n# Read more about custom exception handling here: https://www.programiz.com/python-programming/user-defined-exception\n\nif __name__ == '__main__':\n    try:\n        a = 10\n        b = 0\n        c = a/b\n        print(c)\n    except Exception as e:\n        logging.error(e)\n        raise CustomException(e,error_detail=sys)\n\n\n\n\n\n\n\n\n\nData being a central component of any project, in this component, we write classes such as DataIngestionConfig and DataIngestion.\n\nDataIngestionConfig consists of public path variables to train, test and raw data.\nDataIngestion helps to create an object which invokes an object of DataIngestionConfig during initialization and retrieves public path variables.\nBy use of those paths, we read data, split them up and save them to the directory by use of initiate_data_ingestion method.\n\nData ingestion is a crucial step in any project that involves handling data. This process involves extracting data from different sources, such as databases or warehouses, and loading it into a centralized location, such as a data warehouse, data lake, or data mart. Typically, this task is performed by a specialized big data team, whose responsibility is to ensure that data is obtained from various sources and stored in different formats, such as Hadoop or MongoDB.\nAs Data Scientists, it‚Äôs essential to have knowledge of how to extract data from different sources, such as Hadoop, MongoDB, MySQL, or Oracle, and make it available for analysis. Since data is a critical asset in any project, understanding the process of data ingestion is vital to ensure that the data is organized and stored in a way that facilitates analysis.\n\n\n\nCode\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom src.components.data_transformation import (DataTransformation,\n                                                DataTransformationConfig)\nfrom src.components.model_trainer import ModelTrainer, ModelTrainerConfig\nfrom src.exception import CustomException\nfrom src.logger import logging\n\n\n@dataclass\nclass DataIngestionConfig:\n    '''\n    Used for defining the configuration for data ingestion.\n    '''\n    train_data_path: str = os.path.join('artifacts', 'train.csv')\n    test_data_path: str = os.path.join('artifacts', 'test.csv') \n    raw_data_path: str = os.path.join('artifacts', 'data.csv')\n\nclass DataIngestion:\n    '''\n    Used for ingesting data by making use of the configuration defined in DataIngestionConfig.\n    '''\n    def __init__(self,ingestion_config: DataIngestionConfig = DataIngestionConfig()):\n        self.ingestion_config = ingestion_config\n    \n    def initiate_data_ingestion(self,raw_data_path: str = None):\n        try:\n            # Reading data here.\n            logging.info(\"Initiating data ingestion\")\n            if raw_data_path is not None:\n                self.ingestion_config.raw_data_path = raw_data_path\n                data = pd.read_csv(self.ingestion_config.raw_data_path)\n            else:\n                data = pd.read_csv('./assets/data/NewSPerformance.csv')\n                        \n            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)\n            data.to_csv(self.ingestion_config.train_data_path,index=False,header=True)\n            logging.info(\"Data ingestion completed\")\n            \n            logging.info(\"Train test split initiated\")\n            train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 18)\n\n            train_set.to_csv(self.ingestion_config.train_data_path,index = False, header = True)\n            test_set.to_csv(self.ingestion_config.test_data_path,index = False, header = True)\n            logging.info(\"Train test split ingestion completed\")\n            \n            return (\n                self.ingestion_config.train_data_path,\n                self.ingestion_config.test_data_path\n            )\n        except Exception as e:\n            logging.error(\"Error occured in data ingestion\")\n            raise CustomException(e,sys)\n    \n\nif __name__ == '__main__':\n    obj = DataIngestion()\n    train_data, test_data = obj.initiate_data_ingestion()\n    \n    data_transformation = DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n    train_arr, test_arr,_ = data_transformation.initiate_data_transformation(train_data,test_data)\n    \n    modeltrainer = ModelTrainer()\n    print(modeltrainer.initiate_model_trainer(train_arr, test_arr))\n\n\n\n\n\n\nOnce data ingestion is done, Data transformation component is used to transform the data, to make it useful for analysis and train models on it.\n\nDataTransformationConfig class in this component stores public path variable to store the preprocessing object in pickle type data, to be later used during building the web app.\nDataTransformation class helps to create an object which invokes DataTransformationConfig Object to get access to preprocessing object path.\nWe have a get_data_transformer_object method, that returns a preprocessor object which can preprocess numerical and categorical columns\nBy use of the get_data_transformer_object method, in initiate_data_transformation method, to do all the preprocessing on the train and test files, whose path is available from Data Ingestion component. After all the preprocessing we return train and test array consisting of feature and target variables.\n\nA data transformation component is a crucial part of the data science process, which involves transforming raw data into a format that can be used for analysis. Data Scientists play a vital role in this process as they use various techniques such as feature engineering, feature selection, feature scaling, data cleaning, and handling null values to ensure the quality of data used for analysis. By understanding the process of data transformation, Data Scientists can generate valuable insights from raw data and make informed business decisions.\n\n\n\nCode\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.utils import save_object\n\n# Defining the paths for the data ingestion\n# di_obj = DataIngestion.DataIngestionConfig() # Not required as we already are doing the Doing the addition of paths in data_ingestion.py itself.\n# di_obj.train_data_path = \"data/train_data.csv\"\n# di_obj.test_data_path = \"data/test_data.csv\"\n\n@dataclass #This is a decorator which is used to create a dataclass variables.\nclass DataTransformationConfig:\n    '''\n    We are creating a dataclass variable which will be used to store the paths for the data transformation transformer object.\n    '''\n    preprocessor_obj_file_path = os.path.join(\"artifacts\",\"preprocessor.pkl\")\n\nclass DataTransformation:\n    \n    def __init__(self,transformation_config: DataTransformationConfig = DataTransformationConfig()):\n        self.data_transformation_config = transformation_config\n\n    def get_data_transformer_object(self):\n        '''\n        This function is responsible for creating a preprocessing data transformation object.\n        '''\n        try:\n            numerical_columns = [\"writing_score\",\"reading_score\"]\n            categorical_columns = [\n                \"gender\",\n                \"race_ethnicity\",\n                \"parental_level_of_education\",\n                \"lunch\",\n                \"test_preparation_course\"\n                ]\n            \n            num_pipeline = Pipeline(\n                steps=[\n                    (\"imputer\",SimpleImputer(strategy = 'median')),\n                    (\"scaler\",StandardScaler())\n                ]\n            )\n            \n            cat_pipeline = Pipeline(\n                steps = [\n                    (\"imputer\",SimpleImputer(strategy = 'most_frequent')),\n                    (\"one_hot_encoder\",OneHotEncoder()),\n                    ('scaler',StandardScaler(with_mean=False))\n                ]\n            )\n            \n            logging.info(f\"Numerical columns:{numerical_columns}\")\n            logging.info(f\"Categorical columns:{categorical_columns}\")\n            \n            preprocessor = ColumnTransformer(\n                [\n                    (\"num_pipeline\",num_pipeline,numerical_columns),\n                    ('cat_pipeline',cat_pipeline,categorical_columns)\n                ]\n            )\n            \n            return preprocessor\n        except Exception as e:\n            raise CustomException(e,sys)\n        \n    def initiate_data_transformation(self,train_path,test_path):\n        '''\n        Here we use the preprocessing object to transform the data.\n        '''\n            \n        try:\n            train_df = pd.read_csv(train_path)\n            test_df= pd.read_csv(test_path)\n            \n            \n            logging.info(\"Read train and test data completed\") \n            \n            logging.info(\"Obtaining preprocessing object and starting processing.\")\n            preprocessing_obj = self.get_data_transformer_object()\n            target_column_name = \"math_score\"\n            numerical_columns = [\"writing_score\",\"reading_score\"]\n            \n            input_feature_train_df = train_df.drop(columns = [target_column_name],axis=1)\n            target_feature_train_df = train_df[target_column_name]\n            \n            input_feature_test_df = test_df.drop(columns = [target_column_name],axis=1)\n            target_feature_test_df = test_df[target_column_name]\n            \n            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n            input_feature_test_arr = preprocessing_obj.fit_transform(input_feature_test_df)\n            \n            train_arr = np.c_[\n                input_feature_train_arr,np.array(target_feature_train_df)\n                ]\n            \n            test_arr = np.c_[\n                input_feature_test_arr,np.array(target_feature_test_df)\n                ]\n            \n            logging.info(f\"Saved Preprocessing object at a particular filepath \")\n            save_object(\n                file_path = self.data_transformation_config.preprocessor_obj_file_path,\n                obj = preprocessing_obj\n            )\n            return(\n                train_arr,\n                test_arr,\n                self.data_transformation_config.preprocessor_obj_file_path,\n            )\n            \n                \n        except Exception as e:\n            raise CustomException(e,sys)\n\n\n\n\n\n\nModel Trainer(MT): We can run various models, once above components have turned data to desired format. This component consists of 2 classes as follows.\n\nModelTrainerConfig class stores public path variable to store the model object once trained in the pickle format.\nModelTrainer class, uses initiate_model_trainer method, that access train and test array from Data Transformation component. This method is able to train various models together on the train array and then finally make predictions on the test array, by using the best model from the various models being used on the base of r2_scores. Also in this method we use the ModelTrainerConfig object to store this trained model in local directory and last but not least we also return the r2_score for the best model on test data in this method itself.\n\nThe model trainer component is responsible for training machine learning models on the transformed data. Data Scientists use this component to select an appropriate algorithm, tune hyperparameters, and train the model on the data. The trained model is then evaluated for its performance, and if it meets the desired level of accuracy, it is deployed for production use. The role of Data Scientists in this component is to select and fine-tune the machine learning models that best fit the problem at hand, and ensure that the models meet the business requirements. Ultimately, the model trainer component helps Data Scientists to generate insights and make predictions that can drive business decisions.\n\n\n\nCode\nimport os\nimport sys\nimport pandas as pd\nfrom dataclasses import dataclass\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import (AdaBoostRegressor, GradientBoostingRegressor,\n                              RandomForestRegressor)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.utils import evaluate_models, save_object, create_plot\n\n@dataclass\nclass ModelTrainerConfig:\n    # This class is used to store the configs, or any other files generated in this particular python file.\n    trained_model_file_path = os.path.join(\"artifacts\",\"model.pkl\")\n\nclass ModelTrainer:\n    def __init__(self,model_train_config:ModelTrainerConfig = ModelTrainerConfig() ) -&gt; None:\n        self.model_trainer_config = model_train_config\n        \n    \n    def initiate_model_trainer(self,train_array,test_array):\n        try:\n            logging.info(\"Split training and test input data\")\n            X_train, y_train, X_test, y_test = (\n                train_array[:,:-1],\n                train_array[:,-1],\n                test_array[:,:-1],\n                test_array[:,-1]\n            )\n            \n            models = {\n                \"Random Forest\": RandomForestRegressor(),\n                \"Decision Tree\": DecisionTreeRegressor(),\n                \"Gradient Boosting\": GradientBoostingRegressor(),\n                \"Linear Regression\": LinearRegression(),\n                \"XGBRegressor\": XGBRegressor(),\n                \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n                \"AdaBoost Regressor\": AdaBoostRegressor(),\n                \"Ridge\": Ridge(),\n                \"Lasso\": Lasso()\n            }\n            \n            params ={\n                \"Ridge\": {\n                \"alpha\": [0.1, 1, 10],\n                \"fit_intercept\": [True, False],\n                \n                },\n                    \"Lasso\": {\n                \"alpha\": [0.1, 1, 10],\n                \"fit_intercept\": [True, False],\n                \n                },\n                \"Decision Tree\": {\n                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n                    'splitter':['best','random'],\n                    #'max_features':['sqrt','log2'],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_samples_split\": [2, 5, 10],\n                    \n                },\n                \"Random Forest\":{\n                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n                     #'max_features':['sqrt','log2',None],\n                    'n_estimators': [8,16,32,64,128,256],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_samples_split\": [2, 5, 10],\n                },\n                \"Gradient Boosting\":{\n                    #'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n                    'learning_rate':[.1,.01,.05,.001],\n                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n                    'criterion':['squared_error', 'friedman_mse'],\n                    #'max_features':['auto','sqrt','log2'],\n                    'n_estimators': [8,16,32,64,128,256],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_samples_split\": [2, 5, 10],\n                },\n                \"Linear Regression\":{ \n                    \"fit_intercept\": [True, False],\n                    #\"normalize\": [True, False],\n                    },\n                \"XGBRegressor\":{\n                    'learning_rate':[.1,.01,.05,.001],\n                    'n_estimators': [8,16,32,64,128,256],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_child_weight\": [1, 3, 5],\n                },\n                \"CatBoosting Regressor\":{\n                    'depth': [6,8,10],\n                    'learning_rate': [0.01, 0.05, 0.1],\n                    'iterations': [30, 50, 100],\n                    #\"n_estimators\": [50,100,250],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"reg_lambda\": [0.1, 1, 10],\n                },\n                \"AdaBoost Regressor\":{\n                    'learning_rate':[.1,.01,0.5,.001],\n                    #'loss':['linear','square','exponential'],\n                    'n_estimators': [8,16,32,64,128,256],\n                }\n                                \n                }\n            \n            model_report: dict = evaluate_models(\n                X_train = X_train,\n                y_train =  y_train, \n                X_test  = X_test,\n                y_test = y_test,\n                models = models,\n                param = params\n                )\n            print(model_report)\n            \n            model_report_df = pd.DataFrame(model_report, index=[0])\n            model_report_df.to_csv(\"./assets/files/model_report.csv\",index=False)\n            \n            # To get best model score from dict\n            best_model_score = max(sorted(model_report.values()))\n            \n            # To get best model name from dict\n            best_model_name = list(model_report.keys())[\n                list(model_report.values()).index(best_model_score)\n                ]\n            best_model = models[best_model_name]\n            \n            if best_model_score&lt;0.6:\n                raise CustomException(\"No best model found\")\n            logging.info(f\"Best found model on both training and testing dataset\")\n            \n            save_object(\n                file_path = self.model_trainer_config.trained_model_file_path,\n                obj = best_model\n            )\n            \n\n    \n            predicted = best_model.predict(X_test)\n            r2_square = r2_score(y_test,predicted)\n            create_plot(y_test,predicted,type = 'scatter',model_name = best_model_name)\n            create_plot(y_test,predicted, type = 'reg',model_name = best_model_name)\n            \n            return r2_square\n            \n\n        except Exception as e:\n            raise CustomException(e,sys)\n\n\n\n\n\n\n\n\n\nA pipeline that interacts with the DI, DT, MT components to process the raw data available in the frontend.\n\nThis pipeline has a TrainPipeline class, which takes in the raw data and uses the train method which interacts with the DI, DT and MT components to simply return the best models r2 score in the end.\n\n\n\n\n\nTo run this pipeline, and train models, simply run the file with appropriate raw data\n\npython3 ./src/pipeline/train_pipeline.py\n\n\nCode\n# Will use the components from src, in the train pipeline to make the model train on the database.\nimport sys\nimport pandas as pd\nfrom src.logger import logging\nfrom src.exception import CustomException\nfrom src.components import data_ingestion as di\nfrom src.components import data_transformation as dt\nfrom src.components import model_trainer as mt\n\n\nclass TrainPipeline:\n    def __init__(self, raw_data_path=None):\n        self.raw_data_path = raw_data_path\n\n    def train(self):\n        try:\n            logging.info(\"Initiating data ingestion\")\n            di_obj = di.DataIngestion()\n            train_data, test_data = di_obj.initiate_data_ingestion(raw_data_path=self.raw_data_path)\n            logging.info(\"Data ingestion completed\")\n            \n            logging.info(\"Initiating data transformation\")\n            dt_obj = dt.DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n            train_arr, test_arr,_ = dt_obj.initiate_data_transformation(train_data,test_data)\n            logging.info(\"Data transformation completed and saved preprocessor object\")\n            \n            \n            logging.info(\"Training the model\")\n            mt_obj = mt.ModelTrainer()\n            print(f\"Best Models r2_score: {mt_obj.initiate_model_trainer(train_arr, test_arr)}\")\n            logging.info(\"Model training completed and saved the best model\")\n    \n        except Exception as e:\n            raise CustomException(e, sys)\n\nif __name__ == \"__main__\":\n    train_pipeline_obj = TrainPipeline(\"data/NewSPerformance.csv\")\n    train_pipeline_obj.train()\n\n\n\n\n\n\n\nPredict Pipeline: A pipeline that takes the user inputs and makes prediction on the given data by using the trained model and other objects like preprocessor obj, created via the train pipeline.\n\nThis pipeline consists of CustomData class which takes the user inputs submitted to our application and returns a data frame out of the inputs.\nPredictPipeline class, takes the CustomData class returned df object as features, scales them via the DT component generated transformer and finally, makes predictions by using the best model, from the MT component and showcases them back to the user.\n\n\n\n\nCode\n# A prediction pipeline file.\nimport sys\nimport pandas as pd\nfrom src.exception import CustomException\nfrom src.utils import load_object\nfrom src.logger import logging\n\n\nclass PredictPipeline:\n    def __init__(self):\n        pass\n    \n    def predict(self,features):\n        try:\n            logging.info(\"Predicting the data\")\n            model_path  = \"artifacts/model.pkl\"\n            preprocessor_path = \"artifacts/preprocessor.pkl\"\n            \n            model = load_object(file_path = model_path)\n            preprocessor = load_object(file_path = preprocessor_path)\n        \n            data_scaled = preprocessor.transform(features)\n            predictions = model.predict(data_scaled)\n            logging.info(\"Predictions completed\")\n            return pd.DataFrame(predictions,columns=[\"predictions\"])\n        \n        except Exception as e:\n            raise CustomException(e,sys)    \n\n\nclass CustomData:\n    def __init__(  self,\n        gender: str,\n        race_ethnicity: str,\n        parental_level_of_education,\n        lunch: str,\n        test_preparation_course: str,\n        reading_score: int,\n        writing_score: int):\n\n        self.gender = gender\n\n        self.race_ethnicity = race_ethnicity\n\n        self.parental_level_of_education = parental_level_of_education\n\n        self.lunch = lunch\n\n        self.test_preparation_course = test_preparation_course\n\n        self.reading_score = reading_score\n\n        self.writing_score = writing_score\n\n    def get_data_as_data_frame(self):\n        try:\n            logging.info(\"Creating a data frame from the custom data\")\n            custom_data_input_dict = {\n                \"gender\": [self.gender],\n                \"race_ethnicity\": [self.race_ethnicity],\n                \"parental_level_of_education\": [self.parental_level_of_education],\n                \"lunch\": [self.lunch],\n                \"test_preparation_course\": [self.test_preparation_course],\n                \"reading_score\": [self.reading_score],\n                \"writing_score\": [self.writing_score],\n            }\n            logging.info(\"Data frame created\")\n            return pd.DataFrame(custom_data_input_dict)\n\n        except Exception as e:\n            raise CustomException(e, sys)\n\n\n\nThe train and predict pipelines are a critical component of the machine learning process. The train pipeline is responsible for training the machine learning model on the training data. This process involves selecting an appropriate algorithm, fine-tuning hyperparameters, and fitting the model to the training data.\nOnce the model is trained, it is deployed to the predict pipeline, which is responsible for making predictions on new data. The predict pipeline involves processing the data, applying any necessary transformations, and using the trained model to generate predictions.\nData Scientists play a crucial role in both the train and predict pipelines. They must ensure that the training data is representative of the problem at hand, and that the model is trained and optimized to meet the desired level of accuracy. In addition, they must ensure that the predict pipeline is efficient and reliable, and that the model generates accurate predictions in real-time.\nUltimately, the train and predict pipelines are essential to the machine learning process, as they allow Data Scientists to build and deploy models that can generate valuable insights and drive business decisions.\n\n\n\n\n\n\nDeployment of Model into a production environment:\n\nDeployment of model via Flask API locally\nDeployment of model via Codepipeline, and Beanstalk.\nDeployment of model via Docker, ECR, Github-Action-Runners, and ECS.\n\n\n\n\n\nFollowing is the code of the flask application that will use the predict pipeline and flask api.\n\nFlask api is used to fetch the data from the frontend to backend and vice-versa.\nThis prediction pipeline makes predictions in the backend and finally the results are shared with the user by use of the Flask api.\nThis local app can be directly run by running python application.py in the terminal.\n\n\n\n\nCode\nfrom flask import Flask, request, render_template\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom src.pipeline.predict_pipeline import CustomData, PredictPipeline\nfrom src.exception import CustomException\nfrom src.logger import logging\nimport os\n\napplication = Flask(__name__)\n\napp = application\n\n## Route for a home page\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n\n@app.route('/predictdata', methods=['GET','POST'])\ndef predict_datapoint():\n    if request.method == 'GET':\n        return render_template('home.html')\n    else:\n        try:\n            data = CustomData(\n                gender = request.form.get('gender'),\n                race_ethnicity=request.form.get('ethnicity'),\n                parental_level_of_education=request.form.get('parental_level_of_education'),\n                lunch = request.form.get('lunch'),\n                test_preparation_course=request.form.get('test_preparation_course'),\n                reading_score=request.form.get('reading_score'),\n                writing_score=request.form.get('writing_score'),\n                \n            )\n            pred_df = data.get_data_as_data_frame()\n            \n            print(pred_df)\n            \n            predict_pipeline = PredictPipeline()\n            results = predict_pipeline.predict(pred_df)\n            return render_template('home.html',results = results)\n        \n        except Exception as e:\n            logging.error(f\"Error occured while predicting the data:{e}\")\n            return render_template('home.html',results = e)\n\nif __name__=='__main__':\n    port = int(os.environ.get(\"PORT\", 8080))\n    app.run(host='0.0.0.0', port=port) # This is the port assigned by the beanstalk or AWS instance, if not provided by them any... else we use the 8080 port in the host to run our application. Rest the docker run \n\n\nLocal Deployment Video by Krish Naik Sir\n\n\n\n\n\n\n\nElastick Beanstalk is a service that allows you to deploy your application on AWS on instances like linux machine based on EC2.\n\nTo set up Beanstalk, you need to have an AWS account. If you don‚Äôt have one, you can create one for free.\nOpen the AWS console and search for Elastic Beanstalk. Click on it and then click on create a new application.\nFollow the steps one by one, but remember to create an IAM role for the application with the permissions of AWSElasticBeanstalkFullAccess, AWSElasticBeanstalkWebTier, AWSElasticBeanstalkEnhancedHealth and AdministratorAccess-AWSElasticBeanstalk.\nAlso in the steps, one will require ec2 key pair. You can create one on the AWS console, while creating the application.\nRest setup the EC2 instance profile via the IAM role itself, in the steps, and then keeping default options in the steps create the application.\nLocally we also need a config file, which is called .ebextensions. This file contains the configuration for the Beanstalk instance.\nThe code for the config file is given below\n\nAfter all the setup, get onto below steps, but before starting the code-pipeline, make sure that the beanstalk environment is running.\n\nAlso to view our application make sure, you setup ports in Beanstalk, by following steps given below.\nTo do that navigate to the Beanstalk console, find your environment, and click on the ‚ÄúConfiguration‚Äù tab.\nScroll down to the ‚ÄúSoftware‚Äù section and click on ‚ÄúEdit‚Äù.\nIn the ‚ÄúEnvironment Properties‚Äù section, add a new property called PORT with a value of 5000.\nSave the changes and wait for the environment to restart before you run the code-pipeline.\n\n.ebsextensions file\n\noption_settings:\n  \"aws:elasticbeanstalk:container:python\":\n    WSGIPath: application:application\n\n\n\n\nAfter setting up Beanstalk, simply push the github repo that we created at the beginning of application.\n\nThis contains all the code of our application, now to deploy the code on the linux instance, there has to be a pipeline, connecting the github repo and the beanstalk server. Such a pipeline is called as a code pipeline in AWS . Whenever any changes happens in our code‚Ä¶ the code pipelines makes real-time changes to the deployed application via help of a button.\nThis pipeline in general is called as a continuous delivery pipeline.\n\n\n\n\n\n\nTo set up the code pipeline, we need to have a code pipeline service in AWS.\n\nWe will use AWS code pipeline to create our code pipeline connecting our git repo to the elastic beanstalk.\nFollow the steps with default options, while setting up the pipeline, make sure, we have connected to github repo and the branch on which the code is present, by github version 1 connect for simplicity.\nOnce the pipeline is connected, we need to build it and deploy it.\n\nWe skip the build part, but we do deploy our application in elastic beanstalk, while deploying we specify the name of application and the env as well.\nNow ensuring that the beanstalk env is already running on port 5000, start the code pipeline, which will connect to the github repo and deploy the application on the beanstalk env.\nOnce the pipeline is running, we can view the application by clicking on the link given in domain column in elastic beanstalk application console.\nThe link will be similar to http://studperformance-env.eba-wmtvi3wb.eu-central-1.elasticbeanstalk.com/\n\n\n\nBeanstalk Deployment Video by Krish Naik Sir\n\n\n\n\n\n\n\nAssuming that reader knows the basics about docker images, containers following the below steps ensure that our application is ready for deployment via AWS.\nSetup Docker container:\n\nBuild an image using Dockerfile, contents of docker file are given below, also add a .dockerignore file to ignore adding venv/ environement to the image.\nRun the image in a container using the command docker run -p 8080:8080 -v /path/to/ml_application:/app my-app\n\nThe -p flag is used to expose a port on the host machine, so that you can access it from outside the container. Here, we are exposing port 8080 on the host machine, which is mapped to port 8080 on the container.\nThe -v flag is used to mount a volume, which allows the container to access a directory on the host machine. The first path is the path to the directory on the host machine, and the second path is the path to the directory in the container.\nThe last argument my-app is the name of the image that you want to run.\nThe container is listening on port 8080, so the application.py file should have the port number set to 8080.\nAccess the application by visiting http://localhost:8080 in your web browser.\nIf you want to run the container in the background, you can use the -d flag.\n\n\n\n\n\n\n\n\nSetup of AWS IAM role\n\nCreate a new user and allow the permissions of AmazonEC2ContainerRegistryFullAccess & AmazonEC2FullAccess permissions to the user.\nSetup access keys for the user and also download it in csv format.\n\nSetup of ECR repo\n\nGo to Elastic Container Registry and create a new repository named student performance and copy the URL for the repository to the aws yml file.\n\nSetup of EC2 instance\n\nUse the default settings, with Ubuntu instance and use all HTTP connections on this instance in one of the steps, while seeting up the instance.\nOnce the instance is up and running, connect to the instance.\nOn the instance run the following commands to install docker.\n\nsudo apt-get update -y\nsudo apt-get upgrade\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nsudo usermod -aG docker ubuntu\nnewgrp docker\n\nSingle Command:\n\nsudo apt-get update -y && sudo apt-get upgrade -y && curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh && sudo usermod -aG docker ubuntu && newgrp docker \n\nAlso in the instance configuration under security groups in network, add a new inbound rule, which will be a custom TCP rule, which will help us access our application on 8080 port.\n\n\n\n\n\n\nGithub workflow:\n\nIt refers to the idea that our code which is in github, once we update it, from github, a docker image should be created in the ecr repository and then this docker image will get installed in the ec2 instance that we have created and from this installation our application will run on the ec2 instance.\nThis same idea is represented in the aws.yaml file in our github workflow folder.\n\nSetting up runner in github to run the workflow:\n\nThis runner will trigger the workflow whenever there is a change in the code.\nGo to the instance and run all the commands to setup the runner in the ec2 instance as shown from the settings of github repo, when you create an self-hosted runner from the github-actions tab in settings for ml app repo.\nKeep default options as it is, while doing the setup, just when the name of runner is asked, I gave it as self-hosted\nRemember that after certain time, if we are not using the runner, it will go offline, to make it back online, and run the command ./run.sh from the actions-runner folder in the ec2 instance.\n\nAfter this add the following github secrets in actions tab in settings of the repo. These will be used in our workflow itself.\n\nAWS_ACCESS_KEY_ID: Created when we created the user in IAM\nAWS_SECRET_ACCESS_KEY: Created when we created the user in IAM\nAWS_DEFAULT_REGION: See it in the ec2 instance details\nAWS_ECR_LOGIN_URI: See it in the ecr repository details #kind of format. but it doesn‚Äôt include the repository name.\nAWS_ECR_REPOSITORY: studentperformance or whatever name you have given to the repository\n\n\n\n\n\n\nNow simply go to the ec2 instance url, and paste :8080 in front of it, to see the application running. Play with it and have fun predicting marks of students.\n\nAWS CI/CD Pipeline Deployment Video by Krish Naik Sir.\n\n\n\n\n\n\n\nCode\n%%html\n&lt;script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.24.1/gradio.js\"\n&gt;&lt;/script&gt;&lt;gradio-app src=\"https://yuvidhepe-studentperformance.hf.space\"&gt;&lt;/gradio-app&gt;\n\n\n\n\n\n\n\n\n\nThe whole project creation outline and execution is of Krish Naik sir, whose efforts in making Data Science simple have been enormous through such projects. You can give him a big shoutout on linkedin, learn from him on youtube."
  },
  {
    "objectID": "docs/projects/StudPerformance_end_to_end.html#a-brief-intro-of-this-project-work",
    "href": "docs/projects/StudPerformance_end_to_end.html#a-brief-intro-of-this-project-work",
    "title": "End to End Machine Learning Project on Student Performance Dataset",
    "section": "",
    "text": "These days it‚Äôs getting more common for a person in data team, to wear multiple hats in terms of bringing insights from the sea of data. To gain experience on what it means to multiple hats, I created this end to to end project on a simple data set of Students Performance Dataset from Kaggle.\nThe goal of this project is not to derive any magical insights from data, rather, to do a comprehensive work on building an end to end project which includes but is not limited to:\n\nCreating modular folder structure\nDeciding on the dataset\nSetup of the environment\nDesign and Develop Components and Pipelines, where components interact with data in backend, whereas the pipelines interact with the user and components to get predictions from a trained ML model and finally provide result to the user.\n\nFollow the actions mentioned below to make your own copy of this end to end project\nYou Can Try the WebApp created for this project before you get your hands dirty\nNotebook checkpoints\n\nSTAR ANALYSIS\nExplained every point of the star method, step by step in detail.\n\nEx. Action will be broken down to A1, A2 to follow up the notebook.\nWe use acronyms like T1 representing Task 1, A1.1 representing subaction 1 of Action 1."
  },
  {
    "objectID": "docs/projects/StudPerformance_end_to_end.html#star-analysis",
    "href": "docs/projects/StudPerformance_end_to_end.html#star-analysis",
    "title": "End to End Machine Learning Project on Student Performance Dataset",
    "section": "",
    "text": "Situation: To gain experience in end-to-end machine learning project development and deployment\nTask: Create a machine learning project from scratch, following the entire pipeline from data collection to model deployment\nAction: Develop a project idea, collect and preprocess the data, perform EDA on data, decide on design and training of the machine learning model, evaluate the model‚Äôs performance, and deploy the model into a production - environment\nResult: Gain hands-on experience in end-to-end machine learning project development and deployment, with a fully functional machine learning system that can be used for real-world applications\n\n\n\n\nS1. Need of gaining exposure in real-world ML project development and deployment\nS2. A way to improve my Data Science profile, with such projects\nS3. Building skillset to be of use in the real-world, and not be limited to books\n\nWith the situation being clear let‚Äôs jump to a bit about task that was required to be done for this situation\n\n\n\n\nT1. Creating a folder structure, for a real-world entity project.\n\nUses: Introduction of Modularity to the project, rather than a single jupyter notebook file doing all the job.\n\nT2. Creating an environment and setup file to run this ML pipeline from scratch.\nA. Developing an End to End ML pipeline and then performing web deployment for using the ML model.\n\nWith the basic overview of task now, let‚Äôs look onto every task in details\n\n\n\nCreating a folder structure for our real-world project. This is an essential part for any real-world code project, as it introduces modularity to our code. This modularity helps us to deal with complexity of huge projects in simple way, where a team can work together on different parts of the project, re-use each others work and combine it all at the end.\n\n\n\n\nFirst setup a github repo (ML_Web_Project is my repo), keeping all the options to default.\nLocally setup a folder (END_To_END_ML_PROJECT is my base local folder setup on WSL, but the one can use windows or mac as well)\n\nOpen this directory in vscode\nOpen a terminal\n\nSecondly let‚Äôs create a conda environment named venv into this local folder, so to have packages locally to run the project. bash     conda create -p venv python==3.8 -y\n\nActivate this environment from the base folder\n\nconda activate venv/ # don't forget '/' cause it tells that this environment is in a folder named venv\nLink the local folder to the github repo\n\nFirst do git init in the local folder\nFollow all the steps mentioned in the github repo you created to do the syncing of local folder to the repo.\nAfter the update of git back in 2021, one needs to setup ssh-keys to use the github repo or use tokens, I prefer to use ssh-keys, follow the steps here.\nCreate a default .gitignore for python in github repo online.\nFinally do a git pull, to sync the changes locally as well.\nLater on whenever there are enough changes to the local code, follow the steps of git add, commit and push with a useful commit message.\n\nBy now local repo should have a .gitignore, README.md, venv/ in their local repo, after this create the following folder structure locally.\n\n- END_TO_END_ML_PROJECT\n    - setup.py # The setup script is the center of all activity in building, distributing, and installing modules that are necessary to run the ML pipeline. # Consider it as the core of the ML Pipeline. This setup.py will help to use our ML pipeline as a package itself and can even be deployed to Pypi.\n    - requirements.txt # All packages that need to be installed before running the project. # This is the part that gives energy to the core.\n    - assets\n        - data # The folder which consist of datasets used in the project.\n            - StudentsPerformance.csv\n        - files\n            - notebook # jupyter notebooks, consisting of all codes which helps to find patterns in data and give a big picture code, later to be broken down into src folders.\n                - EDA_notebook.ipynb \n                - Model_train.ipynb\n        - images # to store plot images        \n    - src # The backbone containing all the source codes for creation of ML pipeline package.\n        - __init__.py\n        - exception.py # Helps in producing custom exceptions.\n        - logger.py # Contains the code that will help in creation of logs, and trace errors if caused any during the realtime.\n        - utils.py # Contains all the utilities that can be reused across the whole project.\n        - components # The major components of the project, which deal with data cleaning, transformation, model training etc.\n            - __init__.py\n            - data_ingestion.py\n            - data_transformation.py\n            - model_trainer.py\n        - pipeline # The complete pipelines built via use of components for further deployment of the model.\n            - __init__.py\n            - predict_pipeline.py\n            - train_pipeline.py\n\n\n\n\n\nCreating an environment and setup file which later can be used to condense our ML pipeline in form of package. In this part we build the foundation for our ML pipeline, by creating the code for setup.py file.\n\n\n\n\n\nCode\nfrom setuptools import find_packages,setup\nfrom typing import List\n\ndef get_requirements(file_path:str)-&gt;List[str]:\n    '''\n    This function will return the list of requirements\n    '''\n    requirements = []\n    file = open(file_path,'r')\n    \n    for line in file:\n        if \"-e .\" not in line:\n            requirements.append(line.strip('\\n'))\n    file.close()\n    \n    #print(requirements)\n    return requirements\n    \n# With this setup we parse our requirements file to get the requirements installed for our project, one can make this static via use of package names in form of a list, instead of parsing a requirements file.\nsetup(\n    name='mlproject',\n    version='0.0.1',\n    author='&lt;Your Name&gt;',\n    author_email='&lt;Your Email&gt;',\n    packages=find_packages(), # This will use the codes or modules that we write for our ML pipeline, to ensure that our every module can be used for building the package, we have a __init__.py in src, or any directory that can be reused.\n    install_requires=get_requirements('requirements.txt') \n)\n\n\n\ncontents of requirements.txt file\n\npandas\nnumpy\nseaborn\nmatplotlib\nscikit-learn\ncatboost\nxgboost\ndill\ntensorboard\n-e . # This triggers the setup .py file automatically, but this is not readed when setup.py is called as per our above code.\n\nOnce these 2 files are setup, simply run:\n\npip install -r requirements.txt\n\nThis will install all the necessary packages in our virtual environment and create a new directory .egg-info which will help to create the ML pipeline package for us.\n\n\n\n\n\n\n\nA1. Project Idea: Using a student performance data to predict it‚Äôs grades or scores, depending on the other features of the dataset.\nA2. Data Collection and Preprocessing: We first do all EDA in a jupyter notebook to find patterns in the data and getting to know the type of preprocessing required to be done on the dataset.\n\nFor simple application the data is simply imported in form of csv file, but all this can even be done by getting data from Data Warehouse as well.\n\nA3. Design and Development of ML pipeline components: After EDA, we try to create simple modular codes in a jupyter notebook, which do the job of development, training and evaluation of ML model. Later these modular codes are more or less split into the folder structure that we created earlier.\nA4. Deployment of model into a production environment: We use cloud tools like AWS or Streamlit or Flask n Django or any other web service to deploy the ML model online to be used on realtime data provided by user or fetched from a source.\n\n\n\n\nProject Idea\n\nWe will use a simple student performance dataset, to predict the child‚Äôs maths scores via the rest of the features of the dataset.\nWe will be using this dataset, because it‚Äôs having a mixed of categorical and numerical features, we can have a good amount of EDA done on this simple data, and last but not the least train many regression algorithms on this simple data easily.\n\nData Collection & Preprocessing\n\nWe will use jupyter notebooks, to majority of the EDA, and finding the patterns.\n\nLink to EDA Ipynb File\n\nOnce the EDA is done, we will also have basic models run on the data, in another jupyter notebook, so that we have basic model pipeline code in place as well.\n\nLink to Models Ipynb File\n\n\nInsights from the EDA n Model Training, I have mentioned in a brief on my github, you can view it here, with the insights in place, let‚Äôs begin with design and development of ML pipeline.\n\n\n\n\n\nDesign and Development of ML pipeline components in form of modular codes\nSteps\n\nCreation of utility codes, logging and exception handling module that will be used all over the components, pipelines.\nCreation of Components modules inside the package consisting of Data Ingestion, Data Transformation and Model Trainer Component.\n\nCreation of train and predict pipelines modules that will be connected to the above components, and will be a pipeline connecting the frontend user and the backend model of Machine learning.\n\n\n\n\n\n\n\n\n\nCode\n#Common functionalities for the whole project.\nimport os\nimport sys\n\nimport dill\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\n\nfrom src.exception import CustomException\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\ndef save_object(file_path,obj):\n    try:\n        dir_path = os.path.dirname(file_path)\n        os.makedirs(dir_path,exist_ok=True)\n        \n        file_obj = open(file_path,\"wb\")\n        dill.dump(obj,file_obj)\n        \n    except Exception as e:\n        raise CustomException(e,sys)\n\ndef evaluate_models(X_train, y_train, X_test,y_test,models, param):\n    try:\n        report = {}\n        \n        for i in range(len(list(models))):\n            model = list(models.values())[i]\n            para=param[list(models.keys())[i]]\n\n            gs = GridSearchCV(model,para,cv=3)\n            gs.fit(X_train,y_train)\n\n            model.set_params(**gs.best_params_)\n            model.fit(X_train,y_train)\n            \n            #model.fit(X_train,y_train)\n            \n            y_train_pred = model.predict(X_train)\n            y_test_pred = model.predict(X_test)\n            \n            train_model_score = r2_score(y_train,y_train_pred)\n            test_model_score = r2_score(y_test,y_test_pred)\n            \n            report[list(models.keys())[i]] = test_model_score\n        \n        return report\n    \n    except Exception as e:\n        raise CustomException(e,sys)\n            \ndef load_object(file_path):\n    try:\n        file_obj = open(file_path,\"rb\")\n        return dill.load(file_obj)\n        file_obj.close()\n        \n    except Exception as e:\n        raise CustomException(e,sys)\n\ndef create_plot(y_test, y_pred, type, model_name, xlabel = \"Actual Math Score\", ylabel=\"Predicted Math Score\", file_name = \"Actual vs Predicted\"):\n    \"\"\"\n    A function to create a plot and save it to a file.\n    \"\"\"\n    if type == \"scatter\":\n        title = f\"{model_name}'s Actual vs Predicted Values Scatterplot\"\n        plt.scatter(y_test, y_pred)\n        plt.title(title)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n        directory = \"./assets/images/\"\n        plt.savefig(f\"{directory}{file_name}\")\n        \n    elif type == \"reg\":\n        title = f\"{model_name}'s Actual vs Predicted Values Regplot\"\n        sns.regplot(x=y_test,y=y_pred,ci=None,color ='red');\n        plt.title(title)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n        directory = \"./assets/images/\"\n        plt.savefig(f\"{directory}{file_name}_regplot\")\n\n\n\n\n\n\n\nCode\n# Logger is for the purpose of logging all the events in the program from execution to termination.\n# For example, whenever there is an exception, we can log the exception info in a file via use of logger.\n\n# Read logger documentation at https://docs.python.org/3/library/logging.html\n# Logger is for the purpose of logging all the events in the program from execution to termination.\n# For example, whenever there is an exception, we can log the exception info in a file via use of logger.\n\n# Read logger documentation at https://docs.python.org/3/library/logging.html\nimport logging\nimport os\nfrom datetime import datetime\n\nLOG_FILE_NAME = f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\nlogs_path = os.path.join(os.getcwd(), \"logs\",LOG_FILE_NAME) # This will create logs folder in the same working directory where this file is present\nos.makedirs(logs_path,exist_ok=True) # Keep appending the logs in the same directory even if there are multiple runs of the program\n\nLOG_FILE_PATH = os.path.join(logs_path,LOG_FILE_NAME)\n\nlogging.basicConfig(filename=LOG_FILE_PATH,\n                    level=logging.INFO,\n                    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s: %(message)s\",\n                    datefmt='%m/%d/%Y %I:%M:%S %p'\n                    ) #This is the change of basic configuration for the logger\n\nif __name__ == '__main__':\n    logging.info(\"This is a test log\")\n    logging.warning(\"This is a warning log\")\n    logging.error(\"This is an error log\")\n    logging.critical(\"This is a critical log\")\n\n\n\n\n\n\n\nCode\n# We use this custom exception handling in the project to handle all the errors that will come into the project, simply we can say that we are handling all the errors that will come into the project in a single place.\n\nimport sys\n\n# Sys module in python provides various functions and variables that are used to manipulate different parts of the python runtime environment. It allows operating on the python interpreter as it provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n# Read more about sys module here: https://docs.python.org/3/library/sys.html\nfrom src.logger import logging\n\n\ndef error_message_detail(error,error_detail:sys):\n    _,_,exec_tb = error_detail.exc_info()\n    file_name = exec_tb.tb_frame.f_code.co_filename\n    error_message = f\"Error occured in python script name {file_name} on line number {exec_tb.tb_lineno} and error is {str(error)}\"\n    \n    return error_message\n    \nclass CustomException(Exception):\n    def __init__(self,error_message,error_detail:sys):\n        super().__init__(error_message)\n        self.error_message = error_message_detail(error_message,error_detail= error_detail)\n        #self.error_detail = error_detail        \n        \n    def __str__(self):\n        return f\"{self.error_message}\"\n\n# Read more about custom exception handling here: https://www.programiz.com/python-programming/user-defined-exception\n\nif __name__ == '__main__':\n    try:\n        a = 10\n        b = 0\n        c = a/b\n        print(c)\n    except Exception as e:\n        logging.error(e)\n        raise CustomException(e,error_detail=sys)\n\n\n\n\n\n\n\n\n\nData being a central component of any project, in this component, we write classes such as DataIngestionConfig and DataIngestion.\n\nDataIngestionConfig consists of public path variables to train, test and raw data.\nDataIngestion helps to create an object which invokes an object of DataIngestionConfig during initialization and retrieves public path variables.\nBy use of those paths, we read data, split them up and save them to the directory by use of initiate_data_ingestion method.\n\nData ingestion is a crucial step in any project that involves handling data. This process involves extracting data from different sources, such as databases or warehouses, and loading it into a centralized location, such as a data warehouse, data lake, or data mart. Typically, this task is performed by a specialized big data team, whose responsibility is to ensure that data is obtained from various sources and stored in different formats, such as Hadoop or MongoDB.\nAs Data Scientists, it‚Äôs essential to have knowledge of how to extract data from different sources, such as Hadoop, MongoDB, MySQL, or Oracle, and make it available for analysis. Since data is a critical asset in any project, understanding the process of data ingestion is vital to ensure that the data is organized and stored in a way that facilitates analysis.\n\n\n\nCode\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom src.components.data_transformation import (DataTransformation,\n                                                DataTransformationConfig)\nfrom src.components.model_trainer import ModelTrainer, ModelTrainerConfig\nfrom src.exception import CustomException\nfrom src.logger import logging\n\n\n@dataclass\nclass DataIngestionConfig:\n    '''\n    Used for defining the configuration for data ingestion.\n    '''\n    train_data_path: str = os.path.join('artifacts', 'train.csv')\n    test_data_path: str = os.path.join('artifacts', 'test.csv') \n    raw_data_path: str = os.path.join('artifacts', 'data.csv')\n\nclass DataIngestion:\n    '''\n    Used for ingesting data by making use of the configuration defined in DataIngestionConfig.\n    '''\n    def __init__(self,ingestion_config: DataIngestionConfig = DataIngestionConfig()):\n        self.ingestion_config = ingestion_config\n    \n    def initiate_data_ingestion(self,raw_data_path: str = None):\n        try:\n            # Reading data here.\n            logging.info(\"Initiating data ingestion\")\n            if raw_data_path is not None:\n                self.ingestion_config.raw_data_path = raw_data_path\n                data = pd.read_csv(self.ingestion_config.raw_data_path)\n            else:\n                data = pd.read_csv('./assets/data/NewSPerformance.csv')\n                        \n            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)\n            data.to_csv(self.ingestion_config.train_data_path,index=False,header=True)\n            logging.info(\"Data ingestion completed\")\n            \n            logging.info(\"Train test split initiated\")\n            train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 18)\n\n            train_set.to_csv(self.ingestion_config.train_data_path,index = False, header = True)\n            test_set.to_csv(self.ingestion_config.test_data_path,index = False, header = True)\n            logging.info(\"Train test split ingestion completed\")\n            \n            return (\n                self.ingestion_config.train_data_path,\n                self.ingestion_config.test_data_path\n            )\n        except Exception as e:\n            logging.error(\"Error occured in data ingestion\")\n            raise CustomException(e,sys)\n    \n\nif __name__ == '__main__':\n    obj = DataIngestion()\n    train_data, test_data = obj.initiate_data_ingestion()\n    \n    data_transformation = DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n    train_arr, test_arr,_ = data_transformation.initiate_data_transformation(train_data,test_data)\n    \n    modeltrainer = ModelTrainer()\n    print(modeltrainer.initiate_model_trainer(train_arr, test_arr))\n\n\n\n\n\n\nOnce data ingestion is done, Data transformation component is used to transform the data, to make it useful for analysis and train models on it.\n\nDataTransformationConfig class in this component stores public path variable to store the preprocessing object in pickle type data, to be later used during building the web app.\nDataTransformation class helps to create an object which invokes DataTransformationConfig Object to get access to preprocessing object path.\nWe have a get_data_transformer_object method, that returns a preprocessor object which can preprocess numerical and categorical columns\nBy use of the get_data_transformer_object method, in initiate_data_transformation method, to do all the preprocessing on the train and test files, whose path is available from Data Ingestion component. After all the preprocessing we return train and test array consisting of feature and target variables.\n\nA data transformation component is a crucial part of the data science process, which involves transforming raw data into a format that can be used for analysis. Data Scientists play a vital role in this process as they use various techniques such as feature engineering, feature selection, feature scaling, data cleaning, and handling null values to ensure the quality of data used for analysis. By understanding the process of data transformation, Data Scientists can generate valuable insights from raw data and make informed business decisions.\n\n\n\nCode\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.utils import save_object\n\n# Defining the paths for the data ingestion\n# di_obj = DataIngestion.DataIngestionConfig() # Not required as we already are doing the Doing the addition of paths in data_ingestion.py itself.\n# di_obj.train_data_path = \"data/train_data.csv\"\n# di_obj.test_data_path = \"data/test_data.csv\"\n\n@dataclass #This is a decorator which is used to create a dataclass variables.\nclass DataTransformationConfig:\n    '''\n    We are creating a dataclass variable which will be used to store the paths for the data transformation transformer object.\n    '''\n    preprocessor_obj_file_path = os.path.join(\"artifacts\",\"preprocessor.pkl\")\n\nclass DataTransformation:\n    \n    def __init__(self,transformation_config: DataTransformationConfig = DataTransformationConfig()):\n        self.data_transformation_config = transformation_config\n\n    def get_data_transformer_object(self):\n        '''\n        This function is responsible for creating a preprocessing data transformation object.\n        '''\n        try:\n            numerical_columns = [\"writing_score\",\"reading_score\"]\n            categorical_columns = [\n                \"gender\",\n                \"race_ethnicity\",\n                \"parental_level_of_education\",\n                \"lunch\",\n                \"test_preparation_course\"\n                ]\n            \n            num_pipeline = Pipeline(\n                steps=[\n                    (\"imputer\",SimpleImputer(strategy = 'median')),\n                    (\"scaler\",StandardScaler())\n                ]\n            )\n            \n            cat_pipeline = Pipeline(\n                steps = [\n                    (\"imputer\",SimpleImputer(strategy = 'most_frequent')),\n                    (\"one_hot_encoder\",OneHotEncoder()),\n                    ('scaler',StandardScaler(with_mean=False))\n                ]\n            )\n            \n            logging.info(f\"Numerical columns:{numerical_columns}\")\n            logging.info(f\"Categorical columns:{categorical_columns}\")\n            \n            preprocessor = ColumnTransformer(\n                [\n                    (\"num_pipeline\",num_pipeline,numerical_columns),\n                    ('cat_pipeline',cat_pipeline,categorical_columns)\n                ]\n            )\n            \n            return preprocessor\n        except Exception as e:\n            raise CustomException(e,sys)\n        \n    def initiate_data_transformation(self,train_path,test_path):\n        '''\n        Here we use the preprocessing object to transform the data.\n        '''\n            \n        try:\n            train_df = pd.read_csv(train_path)\n            test_df= pd.read_csv(test_path)\n            \n            \n            logging.info(\"Read train and test data completed\") \n            \n            logging.info(\"Obtaining preprocessing object and starting processing.\")\n            preprocessing_obj = self.get_data_transformer_object()\n            target_column_name = \"math_score\"\n            numerical_columns = [\"writing_score\",\"reading_score\"]\n            \n            input_feature_train_df = train_df.drop(columns = [target_column_name],axis=1)\n            target_feature_train_df = train_df[target_column_name]\n            \n            input_feature_test_df = test_df.drop(columns = [target_column_name],axis=1)\n            target_feature_test_df = test_df[target_column_name]\n            \n            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n            input_feature_test_arr = preprocessing_obj.fit_transform(input_feature_test_df)\n            \n            train_arr = np.c_[\n                input_feature_train_arr,np.array(target_feature_train_df)\n                ]\n            \n            test_arr = np.c_[\n                input_feature_test_arr,np.array(target_feature_test_df)\n                ]\n            \n            logging.info(f\"Saved Preprocessing object at a particular filepath \")\n            save_object(\n                file_path = self.data_transformation_config.preprocessor_obj_file_path,\n                obj = preprocessing_obj\n            )\n            return(\n                train_arr,\n                test_arr,\n                self.data_transformation_config.preprocessor_obj_file_path,\n            )\n            \n                \n        except Exception as e:\n            raise CustomException(e,sys)\n\n\n\n\n\n\nModel Trainer(MT): We can run various models, once above components have turned data to desired format. This component consists of 2 classes as follows.\n\nModelTrainerConfig class stores public path variable to store the model object once trained in the pickle format.\nModelTrainer class, uses initiate_model_trainer method, that access train and test array from Data Transformation component. This method is able to train various models together on the train array and then finally make predictions on the test array, by using the best model from the various models being used on the base of r2_scores. Also in this method we use the ModelTrainerConfig object to store this trained model in local directory and last but not least we also return the r2_score for the best model on test data in this method itself.\n\nThe model trainer component is responsible for training machine learning models on the transformed data. Data Scientists use this component to select an appropriate algorithm, tune hyperparameters, and train the model on the data. The trained model is then evaluated for its performance, and if it meets the desired level of accuracy, it is deployed for production use. The role of Data Scientists in this component is to select and fine-tune the machine learning models that best fit the problem at hand, and ensure that the models meet the business requirements. Ultimately, the model trainer component helps Data Scientists to generate insights and make predictions that can drive business decisions.\n\n\n\nCode\nimport os\nimport sys\nimport pandas as pd\nfrom dataclasses import dataclass\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import (AdaBoostRegressor, GradientBoostingRegressor,\n                              RandomForestRegressor)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.utils import evaluate_models, save_object, create_plot\n\n@dataclass\nclass ModelTrainerConfig:\n    # This class is used to store the configs, or any other files generated in this particular python file.\n    trained_model_file_path = os.path.join(\"artifacts\",\"model.pkl\")\n\nclass ModelTrainer:\n    def __init__(self,model_train_config:ModelTrainerConfig = ModelTrainerConfig() ) -&gt; None:\n        self.model_trainer_config = model_train_config\n        \n    \n    def initiate_model_trainer(self,train_array,test_array):\n        try:\n            logging.info(\"Split training and test input data\")\n            X_train, y_train, X_test, y_test = (\n                train_array[:,:-1],\n                train_array[:,-1],\n                test_array[:,:-1],\n                test_array[:,-1]\n            )\n            \n            models = {\n                \"Random Forest\": RandomForestRegressor(),\n                \"Decision Tree\": DecisionTreeRegressor(),\n                \"Gradient Boosting\": GradientBoostingRegressor(),\n                \"Linear Regression\": LinearRegression(),\n                \"XGBRegressor\": XGBRegressor(),\n                \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n                \"AdaBoost Regressor\": AdaBoostRegressor(),\n                \"Ridge\": Ridge(),\n                \"Lasso\": Lasso()\n            }\n            \n            params ={\n                \"Ridge\": {\n                \"alpha\": [0.1, 1, 10],\n                \"fit_intercept\": [True, False],\n                \n                },\n                    \"Lasso\": {\n                \"alpha\": [0.1, 1, 10],\n                \"fit_intercept\": [True, False],\n                \n                },\n                \"Decision Tree\": {\n                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n                    'splitter':['best','random'],\n                    #'max_features':['sqrt','log2'],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_samples_split\": [2, 5, 10],\n                    \n                },\n                \"Random Forest\":{\n                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n                     #'max_features':['sqrt','log2',None],\n                    'n_estimators': [8,16,32,64,128,256],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_samples_split\": [2, 5, 10],\n                },\n                \"Gradient Boosting\":{\n                    #'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n                    'learning_rate':[.1,.01,.05,.001],\n                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n                    'criterion':['squared_error', 'friedman_mse'],\n                    #'max_features':['auto','sqrt','log2'],\n                    'n_estimators': [8,16,32,64,128,256],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_samples_split\": [2, 5, 10],\n                },\n                \"Linear Regression\":{ \n                    \"fit_intercept\": [True, False],\n                    #\"normalize\": [True, False],\n                    },\n                \"XGBRegressor\":{\n                    'learning_rate':[.1,.01,.05,.001],\n                    'n_estimators': [8,16,32,64,128,256],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"min_child_weight\": [1, 3, 5],\n                },\n                \"CatBoosting Regressor\":{\n                    'depth': [6,8,10],\n                    'learning_rate': [0.01, 0.05, 0.1],\n                    'iterations': [30, 50, 100],\n                    #\"n_estimators\": [50,100,250],\n                    #\"max_depth\": [None, 5, 10],\n                    #\"reg_lambda\": [0.1, 1, 10],\n                },\n                \"AdaBoost Regressor\":{\n                    'learning_rate':[.1,.01,0.5,.001],\n                    #'loss':['linear','square','exponential'],\n                    'n_estimators': [8,16,32,64,128,256],\n                }\n                                \n                }\n            \n            model_report: dict = evaluate_models(\n                X_train = X_train,\n                y_train =  y_train, \n                X_test  = X_test,\n                y_test = y_test,\n                models = models,\n                param = params\n                )\n            print(model_report)\n            \n            model_report_df = pd.DataFrame(model_report, index=[0])\n            model_report_df.to_csv(\"./assets/files/model_report.csv\",index=False)\n            \n            # To get best model score from dict\n            best_model_score = max(sorted(model_report.values()))\n            \n            # To get best model name from dict\n            best_model_name = list(model_report.keys())[\n                list(model_report.values()).index(best_model_score)\n                ]\n            best_model = models[best_model_name]\n            \n            if best_model_score&lt;0.6:\n                raise CustomException(\"No best model found\")\n            logging.info(f\"Best found model on both training and testing dataset\")\n            \n            save_object(\n                file_path = self.model_trainer_config.trained_model_file_path,\n                obj = best_model\n            )\n            \n\n    \n            predicted = best_model.predict(X_test)\n            r2_square = r2_score(y_test,predicted)\n            create_plot(y_test,predicted,type = 'scatter',model_name = best_model_name)\n            create_plot(y_test,predicted, type = 'reg',model_name = best_model_name)\n            \n            return r2_square\n            \n\n        except Exception as e:\n            raise CustomException(e,sys)\n\n\n\n\n\n\n\n\n\nA pipeline that interacts with the DI, DT, MT components to process the raw data available in the frontend.\n\nThis pipeline has a TrainPipeline class, which takes in the raw data and uses the train method which interacts with the DI, DT and MT components to simply return the best models r2 score in the end.\n\n\n\n\n\nTo run this pipeline, and train models, simply run the file with appropriate raw data\n\npython3 ./src/pipeline/train_pipeline.py\n\n\nCode\n# Will use the components from src, in the train pipeline to make the model train on the database.\nimport sys\nimport pandas as pd\nfrom src.logger import logging\nfrom src.exception import CustomException\nfrom src.components import data_ingestion as di\nfrom src.components import data_transformation as dt\nfrom src.components import model_trainer as mt\n\n\nclass TrainPipeline:\n    def __init__(self, raw_data_path=None):\n        self.raw_data_path = raw_data_path\n\n    def train(self):\n        try:\n            logging.info(\"Initiating data ingestion\")\n            di_obj = di.DataIngestion()\n            train_data, test_data = di_obj.initiate_data_ingestion(raw_data_path=self.raw_data_path)\n            logging.info(\"Data ingestion completed\")\n            \n            logging.info(\"Initiating data transformation\")\n            dt_obj = dt.DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n            train_arr, test_arr,_ = dt_obj.initiate_data_transformation(train_data,test_data)\n            logging.info(\"Data transformation completed and saved preprocessor object\")\n            \n            \n            logging.info(\"Training the model\")\n            mt_obj = mt.ModelTrainer()\n            print(f\"Best Models r2_score: {mt_obj.initiate_model_trainer(train_arr, test_arr)}\")\n            logging.info(\"Model training completed and saved the best model\")\n    \n        except Exception as e:\n            raise CustomException(e, sys)\n\nif __name__ == \"__main__\":\n    train_pipeline_obj = TrainPipeline(\"data/NewSPerformance.csv\")\n    train_pipeline_obj.train()\n\n\n\n\n\n\n\nPredict Pipeline: A pipeline that takes the user inputs and makes prediction on the given data by using the trained model and other objects like preprocessor obj, created via the train pipeline.\n\nThis pipeline consists of CustomData class which takes the user inputs submitted to our application and returns a data frame out of the inputs.\nPredictPipeline class, takes the CustomData class returned df object as features, scales them via the DT component generated transformer and finally, makes predictions by using the best model, from the MT component and showcases them back to the user.\n\n\n\n\nCode\n# A prediction pipeline file.\nimport sys\nimport pandas as pd\nfrom src.exception import CustomException\nfrom src.utils import load_object\nfrom src.logger import logging\n\n\nclass PredictPipeline:\n    def __init__(self):\n        pass\n    \n    def predict(self,features):\n        try:\n            logging.info(\"Predicting the data\")\n            model_path  = \"artifacts/model.pkl\"\n            preprocessor_path = \"artifacts/preprocessor.pkl\"\n            \n            model = load_object(file_path = model_path)\n            preprocessor = load_object(file_path = preprocessor_path)\n        \n            data_scaled = preprocessor.transform(features)\n            predictions = model.predict(data_scaled)\n            logging.info(\"Predictions completed\")\n            return pd.DataFrame(predictions,columns=[\"predictions\"])\n        \n        except Exception as e:\n            raise CustomException(e,sys)    \n\n\nclass CustomData:\n    def __init__(  self,\n        gender: str,\n        race_ethnicity: str,\n        parental_level_of_education,\n        lunch: str,\n        test_preparation_course: str,\n        reading_score: int,\n        writing_score: int):\n\n        self.gender = gender\n\n        self.race_ethnicity = race_ethnicity\n\n        self.parental_level_of_education = parental_level_of_education\n\n        self.lunch = lunch\n\n        self.test_preparation_course = test_preparation_course\n\n        self.reading_score = reading_score\n\n        self.writing_score = writing_score\n\n    def get_data_as_data_frame(self):\n        try:\n            logging.info(\"Creating a data frame from the custom data\")\n            custom_data_input_dict = {\n                \"gender\": [self.gender],\n                \"race_ethnicity\": [self.race_ethnicity],\n                \"parental_level_of_education\": [self.parental_level_of_education],\n                \"lunch\": [self.lunch],\n                \"test_preparation_course\": [self.test_preparation_course],\n                \"reading_score\": [self.reading_score],\n                \"writing_score\": [self.writing_score],\n            }\n            logging.info(\"Data frame created\")\n            return pd.DataFrame(custom_data_input_dict)\n\n        except Exception as e:\n            raise CustomException(e, sys)\n\n\n\nThe train and predict pipelines are a critical component of the machine learning process. The train pipeline is responsible for training the machine learning model on the training data. This process involves selecting an appropriate algorithm, fine-tuning hyperparameters, and fitting the model to the training data.\nOnce the model is trained, it is deployed to the predict pipeline, which is responsible for making predictions on new data. The predict pipeline involves processing the data, applying any necessary transformations, and using the trained model to generate predictions.\nData Scientists play a crucial role in both the train and predict pipelines. They must ensure that the training data is representative of the problem at hand, and that the model is trained and optimized to meet the desired level of accuracy. In addition, they must ensure that the predict pipeline is efficient and reliable, and that the model generates accurate predictions in real-time.\nUltimately, the train and predict pipelines are essential to the machine learning process, as they allow Data Scientists to build and deploy models that can generate valuable insights and drive business decisions.\n\n\n\n\n\n\nDeployment of Model into a production environment:\n\nDeployment of model via Flask API locally\nDeployment of model via Codepipeline, and Beanstalk.\nDeployment of model via Docker, ECR, Github-Action-Runners, and ECS.\n\n\n\n\n\nFollowing is the code of the flask application that will use the predict pipeline and flask api.\n\nFlask api is used to fetch the data from the frontend to backend and vice-versa.\nThis prediction pipeline makes predictions in the backend and finally the results are shared with the user by use of the Flask api.\nThis local app can be directly run by running python application.py in the terminal.\n\n\n\n\nCode\nfrom flask import Flask, request, render_template\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom src.pipeline.predict_pipeline import CustomData, PredictPipeline\nfrom src.exception import CustomException\nfrom src.logger import logging\nimport os\n\napplication = Flask(__name__)\n\napp = application\n\n## Route for a home page\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n\n@app.route('/predictdata', methods=['GET','POST'])\ndef predict_datapoint():\n    if request.method == 'GET':\n        return render_template('home.html')\n    else:\n        try:\n            data = CustomData(\n                gender = request.form.get('gender'),\n                race_ethnicity=request.form.get('ethnicity'),\n                parental_level_of_education=request.form.get('parental_level_of_education'),\n                lunch = request.form.get('lunch'),\n                test_preparation_course=request.form.get('test_preparation_course'),\n                reading_score=request.form.get('reading_score'),\n                writing_score=request.form.get('writing_score'),\n                \n            )\n            pred_df = data.get_data_as_data_frame()\n            \n            print(pred_df)\n            \n            predict_pipeline = PredictPipeline()\n            results = predict_pipeline.predict(pred_df)\n            return render_template('home.html',results = results)\n        \n        except Exception as e:\n            logging.error(f\"Error occured while predicting the data:{e}\")\n            return render_template('home.html',results = e)\n\nif __name__=='__main__':\n    port = int(os.environ.get(\"PORT\", 8080))\n    app.run(host='0.0.0.0', port=port) # This is the port assigned by the beanstalk or AWS instance, if not provided by them any... else we use the 8080 port in the host to run our application. Rest the docker run \n\n\nLocal Deployment Video by Krish Naik Sir\n\n\n\n\n\n\n\nElastick Beanstalk is a service that allows you to deploy your application on AWS on instances like linux machine based on EC2.\n\nTo set up Beanstalk, you need to have an AWS account. If you don‚Äôt have one, you can create one for free.\nOpen the AWS console and search for Elastic Beanstalk. Click on it and then click on create a new application.\nFollow the steps one by one, but remember to create an IAM role for the application with the permissions of AWSElasticBeanstalkFullAccess, AWSElasticBeanstalkWebTier, AWSElasticBeanstalkEnhancedHealth and AdministratorAccess-AWSElasticBeanstalk.\nAlso in the steps, one will require ec2 key pair. You can create one on the AWS console, while creating the application.\nRest setup the EC2 instance profile via the IAM role itself, in the steps, and then keeping default options in the steps create the application.\nLocally we also need a config file, which is called .ebextensions. This file contains the configuration for the Beanstalk instance.\nThe code for the config file is given below\n\nAfter all the setup, get onto below steps, but before starting the code-pipeline, make sure that the beanstalk environment is running.\n\nAlso to view our application make sure, you setup ports in Beanstalk, by following steps given below.\nTo do that navigate to the Beanstalk console, find your environment, and click on the ‚ÄúConfiguration‚Äù tab.\nScroll down to the ‚ÄúSoftware‚Äù section and click on ‚ÄúEdit‚Äù.\nIn the ‚ÄúEnvironment Properties‚Äù section, add a new property called PORT with a value of 5000.\nSave the changes and wait for the environment to restart before you run the code-pipeline.\n\n.ebsextensions file\n\noption_settings:\n  \"aws:elasticbeanstalk:container:python\":\n    WSGIPath: application:application\n\n\n\n\nAfter setting up Beanstalk, simply push the github repo that we created at the beginning of application.\n\nThis contains all the code of our application, now to deploy the code on the linux instance, there has to be a pipeline, connecting the github repo and the beanstalk server. Such a pipeline is called as a code pipeline in AWS . Whenever any changes happens in our code‚Ä¶ the code pipelines makes real-time changes to the deployed application via help of a button.\nThis pipeline in general is called as a continuous delivery pipeline.\n\n\n\n\n\n\nTo set up the code pipeline, we need to have a code pipeline service in AWS.\n\nWe will use AWS code pipeline to create our code pipeline connecting our git repo to the elastic beanstalk.\nFollow the steps with default options, while setting up the pipeline, make sure, we have connected to github repo and the branch on which the code is present, by github version 1 connect for simplicity.\nOnce the pipeline is connected, we need to build it and deploy it.\n\nWe skip the build part, but we do deploy our application in elastic beanstalk, while deploying we specify the name of application and the env as well.\nNow ensuring that the beanstalk env is already running on port 5000, start the code pipeline, which will connect to the github repo and deploy the application on the beanstalk env.\nOnce the pipeline is running, we can view the application by clicking on the link given in domain column in elastic beanstalk application console.\nThe link will be similar to http://studperformance-env.eba-wmtvi3wb.eu-central-1.elasticbeanstalk.com/\n\n\n\nBeanstalk Deployment Video by Krish Naik Sir\n\n\n\n\n\n\n\nAssuming that reader knows the basics about docker images, containers following the below steps ensure that our application is ready for deployment via AWS.\nSetup Docker container:\n\nBuild an image using Dockerfile, contents of docker file are given below, also add a .dockerignore file to ignore adding venv/ environement to the image.\nRun the image in a container using the command docker run -p 8080:8080 -v /path/to/ml_application:/app my-app\n\nThe -p flag is used to expose a port on the host machine, so that you can access it from outside the container. Here, we are exposing port 8080 on the host machine, which is mapped to port 8080 on the container.\nThe -v flag is used to mount a volume, which allows the container to access a directory on the host machine. The first path is the path to the directory on the host machine, and the second path is the path to the directory in the container.\nThe last argument my-app is the name of the image that you want to run.\nThe container is listening on port 8080, so the application.py file should have the port number set to 8080.\nAccess the application by visiting http://localhost:8080 in your web browser.\nIf you want to run the container in the background, you can use the -d flag.\n\n\n\n\n\n\n\n\nSetup of AWS IAM role\n\nCreate a new user and allow the permissions of AmazonEC2ContainerRegistryFullAccess & AmazonEC2FullAccess permissions to the user.\nSetup access keys for the user and also download it in csv format.\n\nSetup of ECR repo\n\nGo to Elastic Container Registry and create a new repository named student performance and copy the URL for the repository to the aws yml file.\n\nSetup of EC2 instance\n\nUse the default settings, with Ubuntu instance and use all HTTP connections on this instance in one of the steps, while seeting up the instance.\nOnce the instance is up and running, connect to the instance.\nOn the instance run the following commands to install docker.\n\nsudo apt-get update -y\nsudo apt-get upgrade\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nsudo usermod -aG docker ubuntu\nnewgrp docker\n\nSingle Command:\n\nsudo apt-get update -y && sudo apt-get upgrade -y && curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh && sudo usermod -aG docker ubuntu && newgrp docker \n\nAlso in the instance configuration under security groups in network, add a new inbound rule, which will be a custom TCP rule, which will help us access our application on 8080 port.\n\n\n\n\n\n\nGithub workflow:\n\nIt refers to the idea that our code which is in github, once we update it, from github, a docker image should be created in the ecr repository and then this docker image will get installed in the ec2 instance that we have created and from this installation our application will run on the ec2 instance.\nThis same idea is represented in the aws.yaml file in our github workflow folder.\n\nSetting up runner in github to run the workflow:\n\nThis runner will trigger the workflow whenever there is a change in the code.\nGo to the instance and run all the commands to setup the runner in the ec2 instance as shown from the settings of github repo, when you create an self-hosted runner from the github-actions tab in settings for ml app repo.\nKeep default options as it is, while doing the setup, just when the name of runner is asked, I gave it as self-hosted\nRemember that after certain time, if we are not using the runner, it will go offline, to make it back online, and run the command ./run.sh from the actions-runner folder in the ec2 instance.\n\nAfter this add the following github secrets in actions tab in settings of the repo. These will be used in our workflow itself.\n\nAWS_ACCESS_KEY_ID: Created when we created the user in IAM\nAWS_SECRET_ACCESS_KEY: Created when we created the user in IAM\nAWS_DEFAULT_REGION: See it in the ec2 instance details\nAWS_ECR_LOGIN_URI: See it in the ecr repository details #kind of format. but it doesn‚Äôt include the repository name.\nAWS_ECR_REPOSITORY: studentperformance or whatever name you have given to the repository\n\n\n\n\n\n\nNow simply go to the ec2 instance url, and paste :8080 in front of it, to see the application running. Play with it and have fun predicting marks of students.\n\nAWS CI/CD Pipeline Deployment Video by Krish Naik Sir."
  },
  {
    "objectID": "docs/projects/StudPerformance_end_to_end.html#gradio-webapp",
    "href": "docs/projects/StudPerformance_end_to_end.html#gradio-webapp",
    "title": "End to End Machine Learning Project on Student Performance Dataset",
    "section": "",
    "text": "Code\n%%html\n&lt;script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.24.1/gradio.js\"\n&gt;&lt;/script&gt;&lt;gradio-app src=\"https://yuvidhepe-studentperformance.hf.space\"&gt;&lt;/gradio-app&gt;"
  },
  {
    "objectID": "docs/projects/StudPerformance_end_to_end.html#thanks-to-the-support-of-krish-naik-sir-in-this-project-journey",
    "href": "docs/projects/StudPerformance_end_to_end.html#thanks-to-the-support-of-krish-naik-sir-in-this-project-journey",
    "title": "End to End Machine Learning Project on Student Performance Dataset",
    "section": "",
    "text": "The whole project creation outline and execution is of Krish Naik sir, whose efforts in making Data Science simple have been enormous through such projects. You can give him a big shoutout on linkedin, learn from him on youtube."
  },
  {
    "objectID": "docs/projects/RL2_IntroductionToQ-Learning.html",
    "href": "docs/projects/RL2_IntroductionToQ-Learning.html",
    "title": "RL Unit 2: Introduction to Q Learning",
    "section": "",
    "text": "Back in previous class we learned about Reinforcement Learning, the RL process and the different methods to solve an RL problem.\nFor this Unit we will be learning about:\n\nValue-based Methods\nDifference between Monte Carlo and Temporal Difference Learning\nStudy and implement our first RL algorithm: Q-learning.\n\n\n\n\n\nThe goal of RL to build an agent that can make smart decisions.\nSmart decisions will occur, when the agent will learn from the env, by interacting with it through trial and error and receiving rewards as unique feedback.\nIt‚Äôs goal is to maximize it‚Äôs expected cumulative reward.\nThus we need to train the agent‚Äôs brain i.e.¬†the policy for this.\n\n\n\nOur goal now from maximizing the expected cumulative reward, now changes to learning a policy which maximizes the expected cumulative reward for us\nWe do this by 2 methods:\n\nPolicy based methods: Train the policy directly to learn which action to take given a state\nValue based methods: Train a value function to learn which state is more valuable and use this value function to take the action that leads to it\n\nWe will be focusing on value based methods for this unit\n\n\n\n\n\n\nRL agent‚Äôs goal is to have an optimal policy \\(\\pi^*\\) - To find this policy we have 2 methods: - Policy Based Methods: Here we don‚Äôt need any value function. - We don‚Äôt define by hand the behavior of our policy; it‚Äôs the training that will define it.\n\n\nValue Based Methods: Indirectly, by training a value function that outputs the value of a state or a state-action pair.\n\nGiven this value function, our policy will take an action.\nSince the policy is not trained/learned, we need to specify it‚Äôs behaviour by hand.\nFor instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we‚Äôll have a Greedy Policy\n\n\n\n\nConsequently, whatever method you use to solve your problem, you will have a policy. In the case of value-based methods, you don‚Äôt train the policy: your policy is just a simple pre-specified function (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.\nSo the difference is:\n\nIn policy-based training, the optimal policy (denoted œÄ*) is found by training the policy directly.\nIn value-based training, finding an optimal value function (denoted Q* or V*, we‚Äôll study the difference below) leads to having an optimal policy.\n\n\n\n\nIn Value based methods we have 2 types of value based functions:\nState value function under a policy \\(\\pi\\)\n\nFor each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer)\nIn Value based methods we have 2 types of value based functions:- If we take the state with value -7: it‚Äôs the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.\n\n\n \n\nAction value function In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.\nThe value of taking action \\(a\\) in the state \\(s\\) under a policy \\(\\pi\\) is :\n\n\n\nWe see that the difference is:\n\nFor the state-value function, we calculate the value of a state \\(S_t\\)\nFor the action-value functions, we calculate the value of the state-action pair \\((S_t, A_t)\\) hence the value of taking that action at that state\n\n\n\n\nIn either case, whichever value function we choose (state-value or action-value function), the returned value is the expected return.\nHowever, the problem is that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.\nThis can be a computationally expensive process, and that‚Äôs where the Bellman equation comes in to help us.\n\n\n\n\n\nWith what we have learned so far, we know that if we calculate \\(V(S_t)\\), we need to calculate the return starting at that state and then follow the policy forever after. (The policy we defined in the following example is a Greedy Policy; for simplification, we don‚Äôt discount the reward).\nSo to calculate \\(V(S_t)\\), we need to calculate the sum of the expected rewards. Hence: \nThen to calculate \\(V(S_{t+1})\\), we need to calculate the return starting at that state \\(S_{t+1}\\) \nSo basically we‚Äôre repeating the computation for the value of different states, which can be tedious if needs to be done for each state value or state-action value.\nSo to simplify this we use Bellman equation which is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:\n\nThe immedicate reward \\(R_{t+1}+\\) the discounted value of the state that follows (gamma * \\(V(S_{t+1})\\))\nIf we go back to our example, we can say that the value of State 1 is equal to the expected cumulative return if we start at that state.\nTo calculate the value of State 1: the sum of rewards if the agent started in that state 1 and then followed the policy for all the time steps.\nThis is equivalent to $V(S_t) = $ Immediate reward \\(R_{t+1}\\) + Discounted value of the next state (\\(\\gamma * V(S_{t+1}))\\)\n\nIn the interest of simplicity, here we don‚Äôt discount, so gamma= 1. But you‚Äôll study an example with gamma = 0.99 in the Q-Learning section of this unit.\n\nThe value of $V(S_{t+1}) = $ Immediate reward \\(R_{t+2}\\) + Discounted value of the next state (\\(\\gamma * V(S_{t+2}))\\)\n\nTo recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows.\nBefore going to the next section, think about the role of gamma in the Bellman equation. What happens if the value of gamma is very low (e.g.¬†0.1 or even 0)? What happens if the value is 1? What happens if the value is very high, such as a million?\n\n\n\n\n\nSince we know that the RL agent learns by interacting with the environment.\nThe idea is that given the experience and the received reward, the agent will update it‚Äôs value function of policy.\nThere are 2 different strategies on how to train our value function or policy function.\n\nBoth of them use experience to solve the RL problem, i.e.¬†the SARSA\nThe 2 strategies are Monte Carlo and Temporal Differnce\n\nMonte Carlo uses an entire episode of experience before learning.\nTemporal difference uses only a step. \\((S_t, A_t, R_{t+1}, S{t+1})\\)\n\n\n\n\n\n\nMonte Carlo waits until the end of the episode, calculates \\(G_t\\) (return) and uses it as a target for updating \\(V(S_t)\\)\nSo it requires a complete episode of interaction before updating our value function.\n\n\n\nWe always start the episode at the same starting point.\nThe agent takes actions using the policy. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.\nWe get the reward and the next state.\nWe terminate the episode if the cat eats the mouse or if the mouse moves &gt; 10 steps.\nAt the end of the episode, we have a list of State, Actions, Rewards, and Next States tuples For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]‚Ä¶]\nThe agent will sum the total rewards \\(G_t\\) (to see how well it did).\nIt will then update \\(V(S_t)\\) based on this formula\nThen start a new game with this new knowledge\n\n \n\nFor instance, if we train a state-value function using Monte Carlo:\nWe initialize our value function so that it returns 0 value for each state\nOur learning rate (lr) is 0.1 and our discount rate is 1 (= no discount)\nOur mouse explores the environment and takes random actions\nThe mouse made more than 10 steps, so the episode ends .\nWe have a list of state, action rewards, next_state, we need to calculate the return \\(G_t\\)\n$G_t = R_{t+1} +R_{t+2}+R_{t+3} $‚Ä¶.\n$G_t = R_{t+1} +R_{t+2}+R_{t+3} $‚Ä¶.(for simplicity we don‚Äôt discount the rewards)\n\\(G_t = 1+0+0+0+0+0+1+1+0+0\\)\n\\(G_t = 3\\) \nI think it‚Äôs a hyperparameter, about how many times we iterate before we update the single state\n\n\n\n\n\nTemporal differnce, on the other hand, waits for only one interaction (one step) \\(S_{t+1}\\) to form a TD target and update \\(V(S_t)\\) using \\(R_{t+1}\\) and \\(\\gamma*V(S_{t+1})\\)\nThe idea with TD is to update the \\(V(S_t)\\) at each step\nBut because we didn‚Äôt experience an entire episode, we don‚Äôt have \\(G_t\\) (expected return). Instead, we estimate \\(G_t\\) by adding \\(R_{t+1}\\)(reward that came by current action) and the discounted value of the next state\nThis is called bootstrapping. It‚Äôs called this because TD bases it‚Äôs update in part on an existing estimate \\(V(S_{t+1})\\) and not a complete sample \\(G_t\\)\nThis method is called TD(0) or one-step TD (update the value function after any individual step)\n\n \n\nFor the mouse cat example we would have something as follows\n\nWe initialize our value function so that it returns 0 value for each state.\nOur learning rate (lr) is 0.1, and our discount rate is 1 (no discount).\nOur mouse begins to explore the environment and takes a random action: going to the left\nIt gets a reward \\(R_{t+1}\\) since it eats a piece of cheese\n\n\n  \n\nHere as well, I believe it is a hyper parameter, how many times is a state updated\n\n\n\n\n\nSo basically first we decide which way to train our policy, once that is decided, we ask how do we train that‚Äôs where these 2 strategies come into picture.\nWith Monte Carlo, we update the value function from a complete episode, and so we use the actual accurate discounted return of this episode.\nWith temporal difference learning, we update the value function from a step, and we replace \\(G_t\\), which we don‚Äôt know with an estimated return called the TD target. \n\n\n\n\n\nWhat is Q-Learning ? - Q learning is an off-policy value-based method that uses a temporal difference approach to train it‚Äôs action value function: - Off-policy: We‚Äôll see this at the end. - Value-based method: Finds the optimal policy indirectly by training a value or action-value function that will tell us the value of each state or each state-action pair. - TD approach: updates its action-value function at each step intead of at the end of the episode.\n\nQ-Learning is the algorithm we use to train our Q-function, an action-value function that determines the value of being at a particular state and taking a specific action at that state. \nIn Q-Learning the Q stands for quality (the value) of that action at that state. Also to recap here is the difference between value and reward:\nThe value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to it‚Äôs policy.\nThe reward is the feedback the agent gets‚Äô from the environment after performing an action at a state.\nInternally, our Q-function is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function.\nSo overall in Q-learning we train our action value function known as Q-function. This Q-function is encoded as a Q-table, where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function.\nLet‚Äôs take an example with this simple maze: \nThe Q-table is initialized. That‚Äôs why all the values are = 0. This table contains, for each state and action, the corresponding state-action values. \nHere we see that the state-action value of the initial state and going up is 0: \nSo: the Q-function uses a Q-table that has the value of each state-action pair. Given a state and action, our Q-function will seacrch inside it‚Äôs Q-table to output the value. \n\nIf we recap, Q-Learning is the RL algorithm that:\n\nTrains a Q-function (an action-value function), which internally is a Q-table that contains all the state-action pair values.\nGiven a state and action, our Q-function will search its Q-table for the corresponding value.\nWhen the training is done, we have an optimal Q-function, which means we have optimal Q-table.\nAnd if we have an optimal Q-function, we have an optimal policy since we know the best action to take at each state.\n\n\n\nIn the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time, we initialize the Q-table to 0). As the agent explores the environment and we update the Q-table, it will give us a better and better approximation to the optimal policy.\n\n\n\nNow that we understand what Q-Learning, Q-functions, and Q-tables are, let‚Äôs dive deeper into the Q-Learning algorithm.\n\n\n\n\nThis is the Q-Learning pseudocode; let‚Äôs study each part and see how it works with a simple example before implementing it. Don‚Äôt be intimidated by it, it‚Äôs simpler than it looks! We‚Äôll go over each step.\n\n - Step 1: We initialize the Q-table, most of the time, we initialize with values of 0\n\n\nStep 2: Choose an action using the epsilon-greedy strategy\n\n\n\nThe idea is that, with an initial value of …õ = 1.0:\n\nWith probability 1 ‚Äî …õ : we do exploitation (aka our agent selects the action with the highest state-action pair value).\nWith probability …õ: we do exploration (trying random action).\n\nAt the beginning of the training, the probability of doing exploration will be huge since …õ is very high, so most of the time, we‚Äôll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n\n\n\nStep 3: Perform action \\(A_t\\), get reward \\(R_{t+1}\\) and the next state \\(S_{t+1}\\)\n\n\n\nStep 4: Update Q(\\(S_t, A_t\\))\n\nRemember that in TD Learning, we update our policy or value function (depending on the RL method we choose) after one step of the interaction.\n\nTo produce our TD target, we used the immediate reward \\(R_{t+1}\\) plus the discounted value of the next state, computed by finding the action that maximizes the current Q-function at the next state. (We call that bootstrap).\n\n\n\nTherefore, our Q(\\(S_t, A_t\\)) update formula goes like this:\n\n\n\nThis means to update our \\(Q(S_t, A_t)\\):\n\nWe need \\(S_t, A_t, R_{t+1}, S_{t+1}\\)\nTo update our Q-value at a given state-action pair, we use the TD target.\n\n\nHow do we form the TD target? - We obtain the reward after taking the action \\(R_{t+1}\\) - To get this best state-action pair value for the next state, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value (So there is no probability involved here simply choose that action which will take us to the next state having max Q-value and thus our Q-value for this state becomes optimal) - Then when the update of this Q-value is done, we start in a new state (which will come by the action that leads us to that state which has the best Q-value) and select our action using a epsilon-greedy policy again - This is why we say that Q Learning is an off-policy algorithm\n\n\n\n\nThe difference is subtle:\n\nOff-policy: using a different policy for acting (inference) and updating (training).\n\nFor instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).\nEach update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained.\n\n\nActing Policy: \n\nIs different from the policy we use during the training part:\n\n\nOn-policy: using the same policy for acting and updating.\n\nFor instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy\nEach update only usees data collected while acting according to the most recent version of the policy\n\n\n\n\n\n\n\n        |  \n\n\n\n\n\n\n\nWe have two types of value-based functions:\n\nState-value function: outputs the expected return if the agent starts at a given state and acts according to the policy forever after.\nAction-value function: outputs the expected return if the agent starts in a given state, takes a given action at that state and then acts accordingly to the policy forever after.\nIn value-based methods, rather than learning the policy, we define the policy by hand and we learn a value function. If we have an optimal value function, we will have an optimal policy.\n\nThere are two types of methods to learn a policy for a value function:\n\nWith the Monte Carlo method, we update the value function from a complete episode, and so we use the actual discounted return of this episode.\nWith the TD Learning method, we update the value function from a step, replacing the unknown \\(G_t\\) with an estimated return called the TD target.\n\n\n\n\n\n\n\nQ-Learning is the RL algorithm that :\n\nTrains a Q-function, an action-value function encoded, in internal memory, by a Q-table containing all the state-action pair values.\nGiven a state and action, our Q-function will search its Q-table for the corresponding value. \n\nWhen the training is done, we have an optimal Q-function, or, equivalently, an optimal Q-table.\nAnd if we have an optimal Q-function, we have an optimal policy, since we know, for each state, the best action to take. \nBut, in the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time we initialize the Q-table to 0 values). But, as we explore the environment and update our Q-table it will give us a better and better approximation. \nThis is the Q-Learning pseudocode: \n\n\n\n\n\n\nStrategies to find the optimal policy\n\nPolicy-based methods. The policy is usually trained with a neural network to select what action to take given a state. In this case it is the neural network which outputs the action that the agent should take instead of using a value function. Depending on the experience received by the environment, the neural network will be re-adjusted and will provide better actions.\nValue-based methods. In this case, a value function is trained to output the value of a state or a state-action pair that will represent our policy. However, this value doesn‚Äôt define what action the agent should take. In contrast, we need to specify the behavior of the agent given the output of the value function. For example, we could decide to adopt a policy to take the action that always leads to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy (or whatever decision the user takes) that uses the values of the value-function to decide the actions to take.\n\nAmong the value-based methods, we can find two main strategies\n\nThe state-value function. For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.\nThe action-value function. In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state and takes an action. Then it follows the policy forever after.\n\nEpsilon-greedy strategy:\n\nCommon strategy used in reinforcement learning that involves balancing exploration and exploitation.\nChooses the action with the highest expected reward with a probability of 1-epsilon.\nChooses a random action with a probability of epsilon.\nEpsilon is typically decreased over time to shift focus towards exploitation.\n\nGreedy strategy:\n\nInvolves always choosing the action that is expected to lead to the highest reward, based on the current knowledge of the environment. (Only exploitation)\nAlways chooses the action with the highest expected reward.\nDoes not include any exploration.\nCan be disadvantageous in environments with uncertainty or unknown optimal actions.\n\nOff-policy vs on-policy algorithms\n\nOff-policy algorithms: A different policy is used at training time and inference time\nOn-policy algorithms: The same policy is used during training and inference"
  },
  {
    "objectID": "docs/projects/RL2_IntroductionToQ-Learning.html#chapter-2-introduction-to-q-learning",
    "href": "docs/projects/RL2_IntroductionToQ-Learning.html#chapter-2-introduction-to-q-learning",
    "title": "RL Unit 2: Introduction to Q Learning",
    "section": "",
    "text": "Back in previous class we learned about Reinforcement Learning, the RL process and the different methods to solve an RL problem.\nFor this Unit we will be learning about:\n\nValue-based Methods\nDifference between Monte Carlo and Temporal Difference Learning\nStudy and implement our first RL algorithm: Q-learning.\n\n\n\n\n\nThe goal of RL to build an agent that can make smart decisions.\nSmart decisions will occur, when the agent will learn from the env, by interacting with it through trial and error and receiving rewards as unique feedback.\nIt‚Äôs goal is to maximize it‚Äôs expected cumulative reward.\nThus we need to train the agent‚Äôs brain i.e.¬†the policy for this.\n\n\n\nOur goal now from maximizing the expected cumulative reward, now changes to learning a policy which maximizes the expected cumulative reward for us\nWe do this by 2 methods:\n\nPolicy based methods: Train the policy directly to learn which action to take given a state\nValue based methods: Train a value function to learn which state is more valuable and use this value function to take the action that leads to it\n\nWe will be focusing on value based methods for this unit\n\n\n\n\n\n\nRL agent‚Äôs goal is to have an optimal policy \\(\\pi^*\\) - To find this policy we have 2 methods: - Policy Based Methods: Here we don‚Äôt need any value function. - We don‚Äôt define by hand the behavior of our policy; it‚Äôs the training that will define it.\n\n\nValue Based Methods: Indirectly, by training a value function that outputs the value of a state or a state-action pair.\n\nGiven this value function, our policy will take an action.\nSince the policy is not trained/learned, we need to specify it‚Äôs behaviour by hand.\nFor instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we‚Äôll have a Greedy Policy\n\n\n\n\nConsequently, whatever method you use to solve your problem, you will have a policy. In the case of value-based methods, you don‚Äôt train the policy: your policy is just a simple pre-specified function (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.\nSo the difference is:\n\nIn policy-based training, the optimal policy (denoted œÄ*) is found by training the policy directly.\nIn value-based training, finding an optimal value function (denoted Q* or V*, we‚Äôll study the difference below) leads to having an optimal policy.\n\n\n\n\nIn Value based methods we have 2 types of value based functions:\nState value function under a policy \\(\\pi\\)\n\nFor each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer)\nIn Value based methods we have 2 types of value based functions:- If we take the state with value -7: it‚Äôs the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.\n\n\n \n\nAction value function In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.\nThe value of taking action \\(a\\) in the state \\(s\\) under a policy \\(\\pi\\) is :\n\n\n\nWe see that the difference is:\n\nFor the state-value function, we calculate the value of a state \\(S_t\\)\nFor the action-value functions, we calculate the value of the state-action pair \\((S_t, A_t)\\) hence the value of taking that action at that state\n\n\n\n\nIn either case, whichever value function we choose (state-value or action-value function), the returned value is the expected return.\nHowever, the problem is that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.\nThis can be a computationally expensive process, and that‚Äôs where the Bellman equation comes in to help us.\n\n\n\n\n\nWith what we have learned so far, we know that if we calculate \\(V(S_t)\\), we need to calculate the return starting at that state and then follow the policy forever after. (The policy we defined in the following example is a Greedy Policy; for simplification, we don‚Äôt discount the reward).\nSo to calculate \\(V(S_t)\\), we need to calculate the sum of the expected rewards. Hence: \nThen to calculate \\(V(S_{t+1})\\), we need to calculate the return starting at that state \\(S_{t+1}\\) \nSo basically we‚Äôre repeating the computation for the value of different states, which can be tedious if needs to be done for each state value or state-action value.\nSo to simplify this we use Bellman equation which is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:\n\nThe immedicate reward \\(R_{t+1}+\\) the discounted value of the state that follows (gamma * \\(V(S_{t+1})\\))\nIf we go back to our example, we can say that the value of State 1 is equal to the expected cumulative return if we start at that state.\nTo calculate the value of State 1: the sum of rewards if the agent started in that state 1 and then followed the policy for all the time steps.\nThis is equivalent to $V(S_t) = $ Immediate reward \\(R_{t+1}\\) + Discounted value of the next state (\\(\\gamma * V(S_{t+1}))\\)\n\nIn the interest of simplicity, here we don‚Äôt discount, so gamma= 1. But you‚Äôll study an example with gamma = 0.99 in the Q-Learning section of this unit.\n\nThe value of $V(S_{t+1}) = $ Immediate reward \\(R_{t+2}\\) + Discounted value of the next state (\\(\\gamma * V(S_{t+2}))\\)\n\nTo recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows.\nBefore going to the next section, think about the role of gamma in the Bellman equation. What happens if the value of gamma is very low (e.g.¬†0.1 or even 0)? What happens if the value is 1? What happens if the value is very high, such as a million?\n\n\n\n\n\nSince we know that the RL agent learns by interacting with the environment.\nThe idea is that given the experience and the received reward, the agent will update it‚Äôs value function of policy.\nThere are 2 different strategies on how to train our value function or policy function.\n\nBoth of them use experience to solve the RL problem, i.e.¬†the SARSA\nThe 2 strategies are Monte Carlo and Temporal Differnce\n\nMonte Carlo uses an entire episode of experience before learning.\nTemporal difference uses only a step. \\((S_t, A_t, R_{t+1}, S{t+1})\\)\n\n\n\n\n\n\nMonte Carlo waits until the end of the episode, calculates \\(G_t\\) (return) and uses it as a target for updating \\(V(S_t)\\)\nSo it requires a complete episode of interaction before updating our value function.\n\n\n\nWe always start the episode at the same starting point.\nThe agent takes actions using the policy. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.\nWe get the reward and the next state.\nWe terminate the episode if the cat eats the mouse or if the mouse moves &gt; 10 steps.\nAt the end of the episode, we have a list of State, Actions, Rewards, and Next States tuples For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]‚Ä¶]\nThe agent will sum the total rewards \\(G_t\\) (to see how well it did).\nIt will then update \\(V(S_t)\\) based on this formula\nThen start a new game with this new knowledge\n\n \n\nFor instance, if we train a state-value function using Monte Carlo:\nWe initialize our value function so that it returns 0 value for each state\nOur learning rate (lr) is 0.1 and our discount rate is 1 (= no discount)\nOur mouse explores the environment and takes random actions\nThe mouse made more than 10 steps, so the episode ends .\nWe have a list of state, action rewards, next_state, we need to calculate the return \\(G_t\\)\n$G_t = R_{t+1} +R_{t+2}+R_{t+3} $‚Ä¶.\n$G_t = R_{t+1} +R_{t+2}+R_{t+3} $‚Ä¶.(for simplicity we don‚Äôt discount the rewards)\n\\(G_t = 1+0+0+0+0+0+1+1+0+0\\)\n\\(G_t = 3\\) \nI think it‚Äôs a hyperparameter, about how many times we iterate before we update the single state\n\n\n\n\n\nTemporal differnce, on the other hand, waits for only one interaction (one step) \\(S_{t+1}\\) to form a TD target and update \\(V(S_t)\\) using \\(R_{t+1}\\) and \\(\\gamma*V(S_{t+1})\\)\nThe idea with TD is to update the \\(V(S_t)\\) at each step\nBut because we didn‚Äôt experience an entire episode, we don‚Äôt have \\(G_t\\) (expected return). Instead, we estimate \\(G_t\\) by adding \\(R_{t+1}\\)(reward that came by current action) and the discounted value of the next state\nThis is called bootstrapping. It‚Äôs called this because TD bases it‚Äôs update in part on an existing estimate \\(V(S_{t+1})\\) and not a complete sample \\(G_t\\)\nThis method is called TD(0) or one-step TD (update the value function after any individual step)\n\n \n\nFor the mouse cat example we would have something as follows\n\nWe initialize our value function so that it returns 0 value for each state.\nOur learning rate (lr) is 0.1, and our discount rate is 1 (no discount).\nOur mouse begins to explore the environment and takes a random action: going to the left\nIt gets a reward \\(R_{t+1}\\) since it eats a piece of cheese\n\n\n  \n\nHere as well, I believe it is a hyper parameter, how many times is a state updated\n\n\n\n\n\nSo basically first we decide which way to train our policy, once that is decided, we ask how do we train that‚Äôs where these 2 strategies come into picture.\nWith Monte Carlo, we update the value function from a complete episode, and so we use the actual accurate discounted return of this episode.\nWith temporal difference learning, we update the value function from a step, and we replace \\(G_t\\), which we don‚Äôt know with an estimated return called the TD target. \n\n\n\n\n\nWhat is Q-Learning ? - Q learning is an off-policy value-based method that uses a temporal difference approach to train it‚Äôs action value function: - Off-policy: We‚Äôll see this at the end. - Value-based method: Finds the optimal policy indirectly by training a value or action-value function that will tell us the value of each state or each state-action pair. - TD approach: updates its action-value function at each step intead of at the end of the episode.\n\nQ-Learning is the algorithm we use to train our Q-function, an action-value function that determines the value of being at a particular state and taking a specific action at that state. \nIn Q-Learning the Q stands for quality (the value) of that action at that state. Also to recap here is the difference between value and reward:\nThe value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to it‚Äôs policy.\nThe reward is the feedback the agent gets‚Äô from the environment after performing an action at a state.\nInternally, our Q-function is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function.\nSo overall in Q-learning we train our action value function known as Q-function. This Q-function is encoded as a Q-table, where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function.\nLet‚Äôs take an example with this simple maze: \nThe Q-table is initialized. That‚Äôs why all the values are = 0. This table contains, for each state and action, the corresponding state-action values. \nHere we see that the state-action value of the initial state and going up is 0: \nSo: the Q-function uses a Q-table that has the value of each state-action pair. Given a state and action, our Q-function will seacrch inside it‚Äôs Q-table to output the value. \n\nIf we recap, Q-Learning is the RL algorithm that:\n\nTrains a Q-function (an action-value function), which internally is a Q-table that contains all the state-action pair values.\nGiven a state and action, our Q-function will search its Q-table for the corresponding value.\nWhen the training is done, we have an optimal Q-function, which means we have optimal Q-table.\nAnd if we have an optimal Q-function, we have an optimal policy since we know the best action to take at each state.\n\n\n\nIn the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time, we initialize the Q-table to 0). As the agent explores the environment and we update the Q-table, it will give us a better and better approximation to the optimal policy.\n\n\n\nNow that we understand what Q-Learning, Q-functions, and Q-tables are, let‚Äôs dive deeper into the Q-Learning algorithm.\n\n\n\n\nThis is the Q-Learning pseudocode; let‚Äôs study each part and see how it works with a simple example before implementing it. Don‚Äôt be intimidated by it, it‚Äôs simpler than it looks! We‚Äôll go over each step.\n\n - Step 1: We initialize the Q-table, most of the time, we initialize with values of 0\n\n\nStep 2: Choose an action using the epsilon-greedy strategy\n\n\n\nThe idea is that, with an initial value of …õ = 1.0:\n\nWith probability 1 ‚Äî …õ : we do exploitation (aka our agent selects the action with the highest state-action pair value).\nWith probability …õ: we do exploration (trying random action).\n\nAt the beginning of the training, the probability of doing exploration will be huge since …õ is very high, so most of the time, we‚Äôll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n\n\n\nStep 3: Perform action \\(A_t\\), get reward \\(R_{t+1}\\) and the next state \\(S_{t+1}\\)\n\n\n\nStep 4: Update Q(\\(S_t, A_t\\))\n\nRemember that in TD Learning, we update our policy or value function (depending on the RL method we choose) after one step of the interaction.\n\nTo produce our TD target, we used the immediate reward \\(R_{t+1}\\) plus the discounted value of the next state, computed by finding the action that maximizes the current Q-function at the next state. (We call that bootstrap).\n\n\n\nTherefore, our Q(\\(S_t, A_t\\)) update formula goes like this:\n\n\n\nThis means to update our \\(Q(S_t, A_t)\\):\n\nWe need \\(S_t, A_t, R_{t+1}, S_{t+1}\\)\nTo update our Q-value at a given state-action pair, we use the TD target.\n\n\nHow do we form the TD target? - We obtain the reward after taking the action \\(R_{t+1}\\) - To get this best state-action pair value for the next state, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value (So there is no probability involved here simply choose that action which will take us to the next state having max Q-value and thus our Q-value for this state becomes optimal) - Then when the update of this Q-value is done, we start in a new state (which will come by the action that leads us to that state which has the best Q-value) and select our action using a epsilon-greedy policy again - This is why we say that Q Learning is an off-policy algorithm\n\n\n\n\nThe difference is subtle:\n\nOff-policy: using a different policy for acting (inference) and updating (training).\n\nFor instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).\nEach update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained.\n\n\nActing Policy: \n\nIs different from the policy we use during the training part:\n\n\nOn-policy: using the same policy for acting and updating.\n\nFor instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy\nEach update only usees data collected while acting according to the most recent version of the policy\n\n\n\n\n\n\n\n        |  \n\n\n\n\n\n\n\nWe have two types of value-based functions:\n\nState-value function: outputs the expected return if the agent starts at a given state and acts according to the policy forever after.\nAction-value function: outputs the expected return if the agent starts in a given state, takes a given action at that state and then acts accordingly to the policy forever after.\nIn value-based methods, rather than learning the policy, we define the policy by hand and we learn a value function. If we have an optimal value function, we will have an optimal policy.\n\nThere are two types of methods to learn a policy for a value function:\n\nWith the Monte Carlo method, we update the value function from a complete episode, and so we use the actual discounted return of this episode.\nWith the TD Learning method, we update the value function from a step, replacing the unknown \\(G_t\\) with an estimated return called the TD target.\n\n\n\n\n\n\n\nQ-Learning is the RL algorithm that :\n\nTrains a Q-function, an action-value function encoded, in internal memory, by a Q-table containing all the state-action pair values.\nGiven a state and action, our Q-function will search its Q-table for the corresponding value. \n\nWhen the training is done, we have an optimal Q-function, or, equivalently, an optimal Q-table.\nAnd if we have an optimal Q-function, we have an optimal policy, since we know, for each state, the best action to take. \nBut, in the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time we initialize the Q-table to 0 values). But, as we explore the environment and update our Q-table it will give us a better and better approximation. \nThis is the Q-Learning pseudocode: \n\n\n\n\n\n\nStrategies to find the optimal policy\n\nPolicy-based methods. The policy is usually trained with a neural network to select what action to take given a state. In this case it is the neural network which outputs the action that the agent should take instead of using a value function. Depending on the experience received by the environment, the neural network will be re-adjusted and will provide better actions.\nValue-based methods. In this case, a value function is trained to output the value of a state or a state-action pair that will represent our policy. However, this value doesn‚Äôt define what action the agent should take. In contrast, we need to specify the behavior of the agent given the output of the value function. For example, we could decide to adopt a policy to take the action that always leads to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy (or whatever decision the user takes) that uses the values of the value-function to decide the actions to take.\n\nAmong the value-based methods, we can find two main strategies\n\nThe state-value function. For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.\nThe action-value function. In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state and takes an action. Then it follows the policy forever after.\n\nEpsilon-greedy strategy:\n\nCommon strategy used in reinforcement learning that involves balancing exploration and exploitation.\nChooses the action with the highest expected reward with a probability of 1-epsilon.\nChooses a random action with a probability of epsilon.\nEpsilon is typically decreased over time to shift focus towards exploitation.\n\nGreedy strategy:\n\nInvolves always choosing the action that is expected to lead to the highest reward, based on the current knowledge of the environment. (Only exploitation)\nAlways chooses the action with the highest expected reward.\nDoes not include any exploration.\nCan be disadvantageous in environments with uncertainty or unknown optimal actions.\n\nOff-policy vs on-policy algorithms\n\nOff-policy algorithms: A different policy is used at training time and inference time\nOn-policy algorithms: The same policy is used during training and inference"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html",
    "href": "docs/projects/project-4-us-accidents-model-runs.html",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "",
    "text": "Data processing can be accessed from this Kaggle Notebook\nIn this notebook Classification models are trained and compared\nThe training is done on partial data cause few models take too much time to train and find best params using grid search\nAfter training, we retrain the best models on the whole data and compare between them\nIn the end we conclude with our findings\nWe will be using these models:\n\nLogistic Regression\nDecision Tree Classifier\nRandom Forest Classifier\nNaive Bayes\nGaussian Naive Bayes\nMultinomial Naive Bayes\nBernoulli Naive Bayes\nXGBoost Classifier\nMulti Layer Perceptron Classifier\nSVM Classifier\n\nWe will also try to search for best hyper-params to produce the best results for models.\nFinally we will compare all the models using different metrics of Precision, Recall, Accuracy, ROC, PR curves\nComments are put wherever necessary and if the code is repetitive it‚Äôs not commented since the logic is same as earlier commented code"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html#importing-the-necessary-libraries",
    "href": "docs/projects/project-4-us-accidents-model-runs.html#importing-the-necessary-libraries",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "Importing the necessary Libraries",
    "text": "Importing the necessary Libraries\n\nimport pandas as pd\nimport pyarrow as pa\nimport polars as pl\nimport vaex as vx\n#import pyarrow.parquet as pq\n#import dask.dataframe as dd\n\nimport numpy as np\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n\nimport plotly.graph_objects as go\nfrom nltk.corpus import stopwords\n\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve, precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.naive_bayes import CategoricalNB, BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler, label_binarize\nfrom sklearn.impute import SimpleImputer\n#from ydata_profiling import ProfileReport"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html#loading-splitting-sampling-the-dataset",
    "href": "docs/projects/project-4-us-accidents-model-runs.html#loading-splitting-sampling-the-dataset",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "Loading, Splitting & Sampling the Dataset",
    "text": "Loading, Splitting & Sampling the Dataset\n\ndf_model = pd.read_pickle(\"/kaggle/input/us-accidents-processed/final_df.pickle\")\n\n\ndf_model.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 252372 entries, A-516257 to A-4833008\nData columns (total 56 columns):\n #   Column                              Non-Null Count   Dtype  \n---  ------                              --------------   -----  \n 0   Severity                            252372 non-null  uint8  \n 1   Start_Lat                           252372 non-null  float32\n 2   Start_Lng                           252372 non-null  float32\n 3   Distance(mi)                        252372 non-null  float32\n 4   Temperature(F)                      252372 non-null  float32\n 5   Humidity(%)                         252372 non-null  float32\n 6   Pressure(in)                        252372 non-null  float32\n 7   Wind_Speed(mph)                     252372 non-null  float32\n 8   Amenity                             252372 non-null  uint8  \n 9   Crossing                            252372 non-null  uint8  \n 10  Junction                            252372 non-null  uint8  \n 11  No_Exit                             252372 non-null  uint8  \n 12  Railway                             252372 non-null  uint8  \n 13  Station                             252372 non-null  uint8  \n 14  Stop                                252372 non-null  uint8  \n 15  Traffic_Signal                      252372 non-null  uint8  \n 16  Year                                252372 non-null  float32\n 17  Hour                                252372 non-null  float32\n 18  Weekday_Mon                         252372 non-null  uint8  \n 19  Weekday_Sat                         252372 non-null  uint8  \n 20  Weekday_Sun                         252372 non-null  uint8  \n 21  Weekday_Thu                         252372 non-null  uint8  \n 22  Weekday_Tue                         252372 non-null  uint8  \n 23  Weekday_Wed                         252372 non-null  uint8  \n 24  Wind_Direction_North                252372 non-null  uint8  \n 25  Wind_Direction_South                252372 non-null  uint8  \n 26  Wind_Direction_Variable             252372 non-null  uint8  \n 27  Wind_Direction_West                 252372 non-null  uint8  \n 28  Weather_Condition_Cloudy            252372 non-null  uint8  \n 29  Weather_Condition_Fog               252372 non-null  uint8  \n 30  Weather_Condition_Funnel Cloud      252372 non-null  uint8  \n 31  Weather_Condition_Hail              252372 non-null  uint8  \n 32  Weather_Condition_Haze              252372 non-null  uint8  \n 33  Weather_Condition_Light Haze        252372 non-null  uint8  \n 34  Weather_Condition_Rain              252372 non-null  uint8  \n 35  Weather_Condition_Sand              252372 non-null  uint8  \n 36  Weather_Condition_Scattered Clouds  252372 non-null  uint8  \n 37  Weather_Condition_Smoke             252372 non-null  uint8  \n 38  Weather_Condition_Snow              252372 non-null  uint8  \n 39  Weather_Condition_Thunderstorm      252372 non-null  uint8  \n 40  Weather_Condition_Tornado           252372 non-null  uint8  \n 41  Weather_Condition_Windy             252372 non-null  uint8  \n 42  Month_Aug                           252372 non-null  uint8  \n 43  Month_Dec                           252372 non-null  uint8  \n 44  Month_Feb                           252372 non-null  uint8  \n 45  Month_Jan                           252372 non-null  uint8  \n 46  Month_Jul                           252372 non-null  uint8  \n 47  Month_Jun                           252372 non-null  uint8  \n 48  Month_Mar                           252372 non-null  uint8  \n 49  Month_May                           252372 non-null  uint8  \n 50  Month_Nov                           252372 non-null  uint8  \n 51  Month_Oct                           252372 non-null  uint8  \n 52  Month_Sep                           252372 non-null  uint8  \n 53  Source_Source2                      252372 non-null  uint8  \n 54  Source_Source3                      252372 non-null  uint8  \n 55  city_mean_encoded                   252372 non-null  float32\ndtypes: float32(10), uint8(46)\nmemory usage: 22.6+ MB\n\n\n\nFormulas\n\\[\nPrecision = \\frac{TP}{TP+FP}\n\\]\n\\[\nTPR/Recall = \\frac{TP}{TP+FN}\n\\]\n\\[\nAccuracy: \\frac{TP+TN}{TP+TN+FP+FN}\n\\]\n\\[\nFPR = \\frac{FP}{TN+FP}\n\\] ‚Äù \\[\nF1 = \\frac{2*Precision*Recall}{Precision+Recall}= \\frac{2*TP}{2*TP+FP+FN}\n\\]\n\nMetrics\n\n### Storing every models following metrics in dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict() \ntpr = dict()\nbest_params = dict() #Storing the best parameters for models that use gridsearchCV\nmodels = dict() # Storing the trained model in dictionary\n\n\n\nSplitting the data into test, train and validation sets\n\n# Train/Validation - Test split\nX,X_test = train_test_split(df_model,test_size = .2, random_state = 42)\nprint(X.shape, X_test.shape)\n\n(201897, 56) (50475, 56)\n\n\n\ny = X[\"Severity\"] # Separating the target variable from the dataset\n\n\nX_model = X.drop([\"Severity\"],axis = 1) # Createing a df of only features\n\n\n# Train/Validation Split\nX_train, X_val, y_train, y_val = train_test_split(X_model,y,random_state = 42,test_size = 0.20)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n\n(161517, 55) (40380, 55) (161517,) (40380,)\n\n\n\n## We will run the grid search cv only on 24_000 samples, to use less compute but find the best params\n## Doing this is necessary cause few model being complex like SVM take too much time to find best params \nX_sample = X.sample(24_000, random_state = 42)\ny_sample_train = X_sample[\"Severity\"]\nX_sample_train = X_sample.drop([\"Severity\"],axis =1)\nX_train_sample, X_val_sample, y_train_sample, y_val_sample = train_test_split(X_sample_train, y_sample_train, test_size = .2, random_state = 42)\n\n\nX_train_sample.shape, X_val_sample.shape\n\n((19200, 55), (4800, 55))"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html#model-training",
    "href": "docs/projects/project-4-us-accidents-model-runs.html#model-training",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "Model Training",
    "text": "Model Training\n\nLogistic Regression Model\n\nGrid Search CV(5-folds) RUN to find best Params\n\n# Defining the model and it's parameters\nlr = LogisticRegression(random_state = 42, n_jobs =-1,max_iter = 100)\nparams = {\"solver\": [\"sag\",\"saga\"]}\n\n# Performing Grid Search to find best params\ngrid = GridSearchCV(lr, params, n_jobs =1,verbose = 0,)\ngrid.fit(X_train_sample, y_train_sample)\n\nprint(\"Best parameter scores:\")\nprint(grid.best_params_)\n\n# Storing the best Params to be used in training on whole dataset if it's a best model compared to others\nbest_params[\"lr\"] = grid.best_params_\nprint(f\"Train score: {grid.score(X_train_sample, y_train_sample)}\")\n\nBest parameter scores:\n{'solver': 'saga'}\nTrain score: 0.6977083333333334\n\n\n\n# Observing the data frame on fitted models \ndf = (pd.DataFrame(grid.cv_results_,index = ['sag','saga'])).drop([\"param_solver\"],axis = 1)\ndf = df.rename_axis('Solver', axis='index')\ndf\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\nSolver\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsag\n1.262635\n0.397594\n0.006319\n0.003799\n{'solver': 'sag'}\n0.692448\n0.698438\n0.678125\n0.701823\n0.698177\n0.693802\n0.008397\n2\n\n\nsaga\n2.885478\n0.244466\n0.004469\n0.000096\n{'solver': 'saga'}\n0.692708\n0.698698\n0.678385\n0.701823\n0.698177\n0.693958\n0.008321\n1\n\n\n\n\n\n\n\n\n\nRunning the best params logistic regression model\n\nprint(\"Best Param scores for Logistic Regression:\")\nlr = LogisticRegression(**grid.best_params_, random_state = 42,)\nlr.fit(X_train_sample, y_train_sample)\n\n# Storing the model trained with best params later used for visualizations\nmodels[\"Logistic Regression\"] = lr\n\n\nprint(\"Train score:\", lr.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", lr.score(X_val_sample, y_val_sample))\n\nBest Param scores for Logistic Regression:\nTrain score: 0.6977083333333334\nValidation score: 0.6827083333333334\n\n\n\n\nLogistic Regression Conf. Matrix on Validation set\n\ny_pred = lr.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Logistic Regression with best params\n\n# Observing the classification report for Logistic Regression\ny_pred = lr.predict(X_val_sample)\n\n# Store the respective scores\naccuracy[\"Logistic Regression\"] = accuracy_score(y_val_sample,y_pred)\nf1[\"Logistic Regression\"] = f1_score(y_val_sample,y_pred,average = 'macro')\n\n# Classification report on train and Val data\nprint(classification_report(y_train_sample,lr.predict(X_train_sample)))\nprint(classification_report(y_val_sample,y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.75      0.86      0.80      4822\n           2       0.61      0.39      0.47      4734\n           3       0.72      0.79      0.75      4865\n           4       0.67      0.75      0.71      4779\n\n    accuracy                           0.70     19200\n   macro avg       0.69      0.70      0.68     19200\nweighted avg       0.69      0.70      0.68     19200\n\n              precision    recall  f1-score   support\n\n           1       0.72      0.85      0.78      1193\n           2       0.60      0.39      0.47      1232\n           3       0.72      0.75      0.73      1206\n           4       0.66      0.75      0.70      1169\n\n    accuracy                           0.68      4800\n   macro avg       0.67      0.69      0.67      4800\nweighted avg       0.67      0.68      0.67      4800\n\n\n\n\n\nLogistic Regression Precision vs Recall Curve on Validation set\n\n# Creates a 2d array from 1d array of y_val_sample, where each instance represents presence of one severity via 1 and rest are 0's.\n# This is necessary since predictions are in 2d format \n# Creating a pr curve tests these arrays i.e. true_y_values and predictions on different \n# thresholds, so binarizing is helpful to make y_val and preds of same size to be compared\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = lr.predict_proba(X_val_sample)\n\n#Storing the metrics for later viz's\nprecision[\"Logistic Regression\"], recall[\"Logistic Regression\"], thresholds = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\n# Plotting the PR Curve\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Logistic Regression\"], precision[\"Logistic Regression\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Logisitc Regression\")\nplt.show()\n# The curve is better the one which is near to right hand corner [1,1]\n\n\n\n\n\n\nLogistic Regression ROC Curve on Validation set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], where=\"post\")\n\nplt.title(\"ROC curve - Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n# The curve is better the one which is near to left hand corner [0,1]\n\n\n\n\n\n# thresholds.shape, y_score.ravel().shape # We have this difference because thresholds are chosen as many different probability instances we havex\n# print(Y.shape,Y.ravel().shape,y_score.shape,y_score.ravel().shape)\n# y = np.array([[1,2,3,4],[4,3,2,1],[1,2,3,4]])\n# unique,counts = np.unique(y,return_counts=True)\n# print(unique)\n# unique,counts = np.unique(y_score,return_counts = True)\n# print(len(unique)) # So we only have these many instances, which have unique prediction probabilities\n\n\n\n\nDecision Tree Model\n\nDecision Tree Grid Search CV(5-folds) Run to find best params\n\ndtc = DecisionTreeClassifier(random_state=42)\nparameters = [{\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [5, 10, 15, 30]}]\n\ngrid = GridSearchCV(dtc, parameters, verbose = 0, n_jobs=-1)\ngrid.fit(X_train_sample, y_train_sample)\n\nprint(\"Best param scores for Decision Tree Classifier:\")\nprint(grid.best_params_)\nbest_params[\"dtc\"] = grid.best_params_\n\nprint(\"Train score:\", grid.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", grid.score(X_val_sample, y_val_sample))\n\nBest param scores for Decision Tree Classifier:\n{'criterion': 'entropy', 'max_depth': 10}\nTrain score: 0.823125\nValidation score: 0.7672916666666667\n\n\n\npd.DataFrame(grid.cv_results_).sort_values(by = 'rank_test_score')\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_criterion\nparam_max_depth\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n5\n0.255957\n0.014855\n0.005243\n0.000257\nentropy\n10\n{'criterion': 'entropy', 'max_depth': 10}\n0.752865\n0.757812\n0.752604\n0.768750\n0.759375\n0.758281\n0.005875\n1\n\n\n1\n0.194180\n0.013044\n0.005411\n0.000337\ngini\n10\n{'criterion': 'gini', 'max_depth': 10}\n0.749479\n0.763802\n0.748958\n0.763281\n0.763281\n0.757760\n0.006979\n2\n\n\n4\n0.140707\n0.008412\n0.004997\n0.000269\nentropy\n5\n{'criterion': 'entropy', 'max_depth': 5}\n0.739062\n0.742969\n0.744792\n0.741406\n0.742969\n0.742240\n0.001916\n3\n\n\n0\n0.104274\n0.002262\n0.005177\n0.000280\ngini\n5\n{'criterion': 'gini', 'max_depth': 5}\n0.738802\n0.744792\n0.728125\n0.746094\n0.747917\n0.741146\n0.007192\n4\n\n\n2\n0.266166\n0.012563\n0.005544\n0.000080\ngini\n15\n{'criterion': 'gini', 'max_depth': 15}\n0.736979\n0.741927\n0.727604\n0.734115\n0.736979\n0.735521\n0.004689\n5\n\n\n6\n0.328951\n0.009996\n0.005584\n0.000248\nentropy\n15\n{'criterion': 'entropy', 'max_depth': 15}\n0.727604\n0.743490\n0.724740\n0.735417\n0.733854\n0.733021\n0.006541\n6\n\n\n7\n0.348827\n0.017845\n0.005383\n0.000513\nentropy\n30\n{'criterion': 'entropy', 'max_depth': 30}\n0.710938\n0.717448\n0.710938\n0.713281\n0.710938\n0.712708\n0.002538\n7\n\n\n3\n0.334237\n0.034283\n0.007774\n0.003789\ngini\n30\n{'criterion': 'gini', 'max_depth': 30}\n0.712760\n0.711979\n0.706771\n0.709375\n0.711719\n0.710521\n0.002189\n8\n\n\n\n\n\n\n\n\n\nRunning the best params Decision Tree model\n\nprint(\"Best Param scores for Decision Tree Classifier:\")\ndtc = DecisionTreeClassifier(**grid.best_params_, random_state = 42)\n\ndtc.fit(X_train_sample, y_train_sample)\n\n# Storing the model trained with best params later used for visualizations\nmodels[\"Decision Tree\"] = dtc\n\nprint(\"Train score:\", dtc.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", dtc.score(X_val_sample, y_val_sample))\n\nBest Param scores for Decision Tree Classifier:\nTrain score: 0.823125\nValidation score: 0.7672916666666667\n\n\n\n\nDecision Tree Conf. Matrix on Validation set\n\ny_pred = dtc.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Decision Tree with best params\n\ny_pred = dtc.predict(X_val_sample)\n\naccuracy[\"Decision Tree\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"Decision Tree\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, dtc.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.91      0.96      0.94      4822\n           2       0.80      0.65      0.72      4734\n           3       0.80      0.83      0.81      4865\n           4       0.77      0.85      0.81      4779\n\n    accuracy                           0.82     19200\n   macro avg       0.82      0.82      0.82     19200\nweighted avg       0.82      0.82      0.82     19200\n\n              precision    recall  f1-score   support\n\n           1       0.89      0.94      0.91      1193\n           2       0.72      0.59      0.65      1232\n           3       0.75      0.75      0.75      1206\n           4       0.70      0.79      0.75      1169\n\n    accuracy                           0.77      4800\n   macro avg       0.77      0.77      0.76      4800\nweighted avg       0.77      0.77      0.76      4800\n\n\n\n\n\nDecision Tree Precision vs Recall Curve on Validation set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = dtc.predict_proba(X_val_sample)\n\nprecision[\"Decision Tree\"], recall[\"Decision Tree\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Decision Tree\"], tpr[\"Decision Tree\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Decision Tree\"], precision[\"Decision Tree\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Decision Tree\")\nplt.show()\n\n\n\n\n\n\nDecision Tree ROC Curve on Validation set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Decision Tree\"], tpr[\"Decision Tree\"], where=\"post\")\n\nplt.title(\"ROC curve - Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\nimportances = pd.DataFrame(np.zeros((X_train_sample.shape[1], 1)), columns=[\"importance\"], index=X_train_sample.columns)\n\nimportances.iloc[:,0] = dtc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(30, 15))\nplot_tree(dtc, max_depth=4, fontsize=10, feature_names=X_train_sample.columns.to_list(), class_names = True, filled=True)\nplt.show()\n\n\n\n\n\n\n\nRandom Forest Classifier\n\nRandom Forest Classifier Grid Search CV (5 - folds) Run to find best params\n\nrfc = RandomForestClassifier(n_jobs=-1, random_state=42)\nparameters = [{\"n_estimators\": [50, 100, 200, 500], \"max_depth\": [5, 10, 15, 30]}]\n\ngrid = GridSearchCV(rfc, parameters, verbose = 0, n_jobs=-1)\ngrid.fit(X_train_sample, y_train_sample)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nbest_params[\"rfc\"] = grid.best_params_\n\n\nprint(\"Train score:\", grid.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", grid.score(X_val_sample, y_val_sample))\n\n/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n\n\nBest parameters scores:\n{'max_depth': 30, 'n_estimators': 500}\nTrain score: 0.9999479166666667\nValidation score: 0.788125\n\n\n\npd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_depth\nparam_n_estimators\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n15\n18.515080\n2.652032\n0.737646\n0.164817\n30\n500\n{'max_depth': 30, 'n_estimators': 500}\n0.790365\n0.788021\n0.785417\n0.792448\n0.790365\n0.789323\n0.002404\n1\n\n\n13\n3.962996\n0.360099\n0.180050\n0.025820\n30\n100\n{'max_depth': 30, 'n_estimators': 100}\n0.786979\n0.786458\n0.782292\n0.790104\n0.789062\n0.786979\n0.002696\n2\n\n\n14\n7.875134\n0.286106\n0.332275\n0.026725\n30\n200\n{'max_depth': 30, 'n_estimators': 200}\n0.785937\n0.786979\n0.783854\n0.788021\n0.788542\n0.786667\n0.001667\n3\n\n\n11\n16.402865\n0.826905\n0.633739\n0.024163\n15\n500\n{'max_depth': 15, 'n_estimators': 500}\n0.783594\n0.784635\n0.779948\n0.784896\n0.783594\n0.783333\n0.001774\n4\n\n\n12\n1.989083\n0.078801\n0.100145\n0.012897\n30\n50\n{'max_depth': 30, 'n_estimators': 50}\n0.779427\n0.782552\n0.777083\n0.788802\n0.785677\n0.782708\n0.004204\n5\n\n\n10\n7.016634\n0.366032\n0.271259\n0.017576\n15\n200\n{'max_depth': 15, 'n_estimators': 200}\n0.782292\n0.783333\n0.777865\n0.784115\n0.783073\n0.782135\n0.002213\n6\n\n\n9\n3.129164\n0.103627\n0.140270\n0.007632\n15\n100\n{'max_depth': 15, 'n_estimators': 100}\n0.783333\n0.780990\n0.777344\n0.784115\n0.780729\n0.781302\n0.002372\n7\n\n\n8\n1.563844\n0.036690\n0.075092\n0.009221\n15\n50\n{'max_depth': 15, 'n_estimators': 50}\n0.780729\n0.783594\n0.775521\n0.782813\n0.778385\n0.780208\n0.002960\n8\n\n\n5\n2.597278\n0.374912\n0.108515\n0.014335\n10\n100\n{'max_depth': 10, 'n_estimators': 100}\n0.763802\n0.766667\n0.759115\n0.770573\n0.762500\n0.764531\n0.003875\n9\n\n\n4\n1.120571\n0.027130\n0.056506\n0.004969\n10\n50\n{'max_depth': 10, 'n_estimators': 50}\n0.762500\n0.765885\n0.755990\n0.777604\n0.760417\n0.764479\n0.007304\n10\n\n\n7\n11.801545\n0.430042\n0.521564\n0.112087\n10\n500\n{'max_depth': 10, 'n_estimators': 500}\n0.763281\n0.769271\n0.758594\n0.764062\n0.764583\n0.763958\n0.003402\n11\n\n\n6\n4.509795\n0.102956\n0.187681\n0.009292\n10\n200\n{'max_depth': 10, 'n_estimators': 200}\n0.762500\n0.767708\n0.758854\n0.764583\n0.764583\n0.763646\n0.002917\n12\n\n\n0\n0.677395\n0.020604\n0.046187\n0.006181\n5\n50\n{'max_depth': 5, 'n_estimators': 50}\n0.734115\n0.738542\n0.724740\n0.734635\n0.737240\n0.733854\n0.004841\n13\n\n\n1\n1.642113\n0.265302\n0.084260\n0.015849\n5\n100\n{'max_depth': 5, 'n_estimators': 100}\n0.737240\n0.738281\n0.723177\n0.733333\n0.736458\n0.733698\n0.005513\n14\n\n\n2\n2.776722\n0.182507\n0.145607\n0.010656\n5\n200\n{'max_depth': 5, 'n_estimators': 200}\n0.734115\n0.738542\n0.727604\n0.730469\n0.734375\n0.733021\n0.003725\n15\n\n\n3\n6.944975\n0.057636\n0.334258\n0.010734\n5\n500\n{'max_depth': 5, 'n_estimators': 500}\n0.729167\n0.734896\n0.727083\n0.729948\n0.733854\n0.730990\n0.002937\n16\n\n\n\n\n\n\n\n\n\nRunning the best params Random Forest Classifier Model\n\nprint(\"Best Param scores for Random Forest Classifier:\")\nrfc = RandomForestClassifier(**grid.best_params_, random_state = 42)\n\nrfc.fit(X_train_sample, y_train_sample)\n# Storing the model trained with best params later used for visualizations\nmodels[\"Random Forest\"] = rfc\n\nprint(\"Train Score: \", rfc.score(X_train_sample, y_train_sample))\nprint(\"Validation Score: \", rfc.score(X_val_sample, y_val_sample))\n\nBest Param scores for Random Forest Classifier:\nTrain Score:  0.9999479166666667\nValidation Score:  0.788125\n\n\n\n\nRandom Forest Classifier Conf. Matrix on Validation Set\n\ny_pred = rfc.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Random Forest Classifier with best params\n\ny_pred = rfc.predict(X_val_sample)\n\naccuracy[\"Random Forest\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"Random Forest\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, rfc.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00      4822\n           2       1.00      1.00      1.00      4734\n           3       1.00      1.00      1.00      4865\n           4       1.00      1.00      1.00      4779\n\n    accuracy                           1.00     19200\n   macro avg       1.00      1.00      1.00     19200\nweighted avg       1.00      1.00      1.00     19200\n\n              precision    recall  f1-score   support\n\n           1       0.91      0.95      0.93      1193\n           2       0.76      0.61      0.68      1232\n           3       0.77      0.76      0.76      1206\n           4       0.72      0.83      0.77      1169\n\n    accuracy                           0.79      4800\n   macro avg       0.79      0.79      0.79      4800\nweighted avg       0.79      0.79      0.78      4800\n\n\n\n\n\nRandom Forest Classifier Precision vs Recall Curve on Validation Set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = rfc.predict_proba(X_val_sample)\n\nprecision[\"Random Forest\"], recall[\"Random Forest\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Random Forest\"], tpr[\"Random Forest\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Random Forest\"], precision[\"Random Forest\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Random Forest\")\nplt.show()\n\n\n\n\n\n\nRandom Forest Classifier ROC Curve on Validation Set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Random Forest\"], tpr[\"Random Forest\"], where=\"post\")\n\nplt.title(\"ROC curve - Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\nimportances = pd.DataFrame(np.zeros((X_train_sample.shape[1], 1)), columns=[\"importance\"], index=X_train_sample.columns)\n\nimportances.iloc[:,0] = rfc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()\n\n\n\n\n\n\n\nNaive Bayes\n\nRunning algo without Grid-Search CV\n\nThere are not major hyper params to be tweaked for this algo.\n\n\ngnb = GaussianNB()\n\ngnb.fit(X_train_sample, y_train_sample)\n\n# Storing the model trained with best params later used for visualizations\nmodels[\"Gaussian Naive Bayes\"] = gnb\n\nprint(\"Train score:\", gnb.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", gnb.score(X_val_sample, y_val_sample))\n\nTrain score: 0.4340104166666667\nValidation score: 0.42645833333333333\n\n\n\n\nGaussian Naive Bayes Conf. Matrix on Validation set\n\ny_pred = gnb.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Gaussian Naive Bayes\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Gaussian Naive Bayes with best params\n\ny_pred = gnb.predict(X_val_sample)\n\naccuracy[\"Gaussian Naive Bayes\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"Gaussian Naive Bayes\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, gnb.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.45      0.94      0.61      4822\n           2       0.61      0.06      0.11      4734\n           3       0.63      0.06      0.10      4865\n           4       0.40      0.67      0.50      4779\n\n    accuracy                           0.43     19200\n   macro avg       0.52      0.43      0.33     19200\nweighted avg       0.52      0.43      0.33     19200\n\n              precision    recall  f1-score   support\n\n           1       0.44      0.94      0.60      1193\n           2       0.59      0.06      0.11      1232\n           3       0.66      0.06      0.11      1206\n           4       0.39      0.66      0.49      1169\n\n    accuracy                           0.43      4800\n   macro avg       0.52      0.43      0.33      4800\nweighted avg       0.52      0.43      0.32      4800\n\n\n\n\n\nGaussian Naive Bayes Precision vs Recall Curve on Validation set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = gnb.predict_proba(X_val_sample)\n\nprecision[\"Gaussian Naive Bayes\"], recall[\"Gaussian Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Gaussian Naive Bayes\"], tpr[\"Gaussian Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Gaussian Naive Bayes\"], precision[\"Gaussian Naive Bayes\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Gaussian Naive Bayes\")\nplt.show()\n\n\n\n\n\n\nGaussian Naive Bayes ROC Curve on Validation set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Gaussian Naive Bayes\"], tpr[\"Gaussian Naive Bayes\"], where=\"post\")\n\nplt.title(\"ROC curve - Gaussian Naive Bayes\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\n\n\nMultinomial Naive Bayes\n\nRunning algo without Grid-Search CV\n\nThere are not major hyper params to be tweaked for this algo.\n\n\nmnb = MultinomialNB()\nmnb.fit(X_train_sample, y_train_sample)\n\n# Storing the model trained with best params later used for visualizations\nmodels[\"Multinomial Naive Bayes\"] = mnb\n\nprint(\"Train score:\", mnb.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", mnb.score(X_val_sample, y_val_sample))\n\nTrain score: 0.6055208333333333\nValidation score: 0.6039583333333334\n\n\n\n\nMultinomial Naive Bayes Conf. Matrix on Validation set\n\ny_pred = mnb.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Multinomial Naive Bayes\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Multinomial Naive Bayes with best params\n\ny_pred = mnb.predict(X_val_sample)\n\naccuracy[\"Multinomial Naive Bayes\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"Multinomial Naive Bayes\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, mnb.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.62      0.80      0.70      4822\n           2       0.55      0.18      0.28      4734\n           3       0.69      0.68      0.68      4865\n           4       0.55      0.75      0.63      4779\n\n    accuracy                           0.61     19200\n   macro avg       0.60      0.60      0.57     19200\nweighted avg       0.60      0.61      0.57     19200\n\n              precision    recall  f1-score   support\n\n           1       0.64      0.78      0.70      1193\n           2       0.55      0.19      0.28      1232\n           3       0.69      0.69      0.69      1206\n           4       0.53      0.77      0.63      1169\n\n    accuracy                           0.60      4800\n   macro avg       0.60      0.61      0.57      4800\nweighted avg       0.60      0.60      0.57      4800\n\n\n\n\n\nMultinomial Naive Bayes Precision vs Recall Curve on Validation set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = mnb.predict_proba(X_val_sample)\n\nprecision[\"Multinomial Naive Bayes\"], recall[\"Multinomial Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Multinomial Naive Bayes\"], tpr[\"Multinomial Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Multinomial Naive Bayes\"], precision[\"Multinomial Naive Bayes\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Multinomial Naive Bayes\")\nplt.show()\n\n\n\n\n\n\nMultinomial Naive Bayes ROC Curve on Validation set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Multinomial Naive Bayes\"], tpr[\"Multinomial Naive Bayes\"], where=\"post\")\n\nplt.title(\"ROC curve - Multinomial Naive Bayes\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\n\n\nBernoulli Naive Bayes\n\nRunning algo without Grid-Search CV\n\nThere are not major hyper params to be tweaked for this algo.\n\n\nbnb = BernoulliNB()\nbnb.fit(X_train_sample, y_train_sample)\n\n# Storing the model trained with best params later used for visualizations\nmodels[\"Bernoulli Naive Bayes\"] = bnb\n\nprint(\"Train score:\", bnb.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", bnb.score(X_val_sample, y_val_sample))\n\nTrain score: 0.611875\nValidation score: 0.6035416666666666\n\n\n\n\nBernoulli Naive Bayes Conf. Matrix on Validation set\n\ny_pred = bnb.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Bernoulli Naive Bayes\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Bernoulli Naive Bayes with best params\n\ny_pred = bnb.predict(X_val_sample)\n\naccuracy[\"Bernoulli Naive Bayes\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"Bernoulli Naive Bayes\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, bnb.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.71      0.77      0.74      4822\n           2       0.40      0.12      0.19      4734\n           3       0.63      0.70      0.66      4865\n           4       0.57      0.85      0.68      4779\n\n    accuracy                           0.61     19200\n   macro avg       0.58      0.61      0.57     19200\nweighted avg       0.58      0.61      0.57     19200\n\n              precision    recall  f1-score   support\n\n           1       0.71      0.77      0.74      1193\n           2       0.39      0.12      0.18      1232\n           3       0.64      0.69      0.66      1206\n           4       0.55      0.85      0.67      1169\n\n    accuracy                           0.60      4800\n   macro avg       0.57      0.61      0.56      4800\nweighted avg       0.57      0.60      0.56      4800\n\n\n\n\n\nBernoulli Naive Bayes Precision vs Recall Curve on Validation set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = bnb.predict_proba(X_val_sample)\n\nprecision[\"Bernoulli Naive Bayes\"], recall[\"Bernoulli Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Bernoulli Naive Bayes\"], tpr[\"Bernoulli Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Bernoulli Naive Bayes\"], precision[\"Bernoulli Naive Bayes\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Bernoulli Naive Bayes\")\nplt.show()\n\n\n\n\n\n\nBernoulli Naive Bayes ROC Curve on Validation set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Bernoulli Naive Bayes\"], tpr[\"Bernoulli Naive Bayes\"], where=\"post\")\n\nplt.title(\"ROC curve - Bernoulli Naive Bayes\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\n\n\nXGBoost Classifier\n\nThis algo requires the target labels to start from 0 so we transform the target | Original Label | Encoded Label | | ‚Äî‚Äî‚Äî‚Äî‚Äì | ‚Äî‚Äî‚Äî‚Äî- | | 1 | 0 | | 2 | 1 | | 3 | 2 | | 4 | 3 |\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create an instance of LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the target variable\ny_train_encoded = label_encoder.fit_transform(y_train_sample)\ny_val_encoded = label_encoder.transform(y_val_sample)\n\n\n\nparameters = [\n    {'learning_rate': [0.1, 0.01, 0.05],\n     'subsample': [0.6,0.8,0.9],\n     'n_estimators': [16,64,128]}\n]\n\nxgb = XGBClassifier(tree_method = 'gpu_hist',random_state=42)\ngrid = GridSearchCV(xgb, parameters, verbose=0, n_jobs=-1)\ngrid.fit(X_train_sample, y_train_encoded)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nbest_params[\"xgb\"] = grid.best_params_\nprint(\"Train score: \", grid.score(X_train_sample, y_train_encoded))\n\n/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n\n\nBest parameters scores:\n{'learning_rate': 0.1, 'n_estimators': 128, 'subsample': 0.9}\nTrain score:  0.8716145833333333\n\n\n\npd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_learning_rate\nparam_n_estimators\nparam_subsample\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n8\n3.067073\n0.128407\n0.077126\n0.001278\n0.1\n128\n0.9\n{'learning_rate': 0.1, 'n_estimators': 128, 's...\n0.800521\n0.792708\n0.786979\n0.801823\n0.793750\n0.795156\n0.005442\n1\n\n\n7\n3.132010\n0.157575\n0.077569\n0.000420\n0.1\n128\n0.8\n{'learning_rate': 0.1, 'n_estimators': 128, 's...\n0.800521\n0.792969\n0.788542\n0.797917\n0.790885\n0.794167\n0.004434\n2\n\n\n6\n3.214932\n0.094857\n0.077695\n0.001827\n0.1\n128\n0.6\n{'learning_rate': 0.1, 'n_estimators': 128, 's...\n0.796875\n0.791146\n0.788802\n0.794792\n0.794010\n0.793125\n0.002837\n3\n\n\n25\n3.262971\n0.023262\n0.081692\n0.006806\n0.05\n128\n0.8\n{'learning_rate': 0.05, 'n_estimators': 128, '...\n0.796615\n0.789844\n0.781510\n0.796875\n0.788542\n0.790677\n0.005709\n4\n\n\n24\n3.273804\n0.056599\n0.078601\n0.002916\n0.05\n128\n0.6\n{'learning_rate': 0.05, 'n_estimators': 128, '...\n0.794531\n0.791146\n0.779167\n0.797396\n0.789583\n0.790365\n0.006220\n5\n\n\n26\n2.902239\n0.472337\n0.090681\n0.035054\n0.05\n128\n0.9\n{'learning_rate': 0.05, 'n_estimators': 128, '...\n0.793750\n0.788021\n0.780729\n0.796094\n0.789062\n0.789531\n0.005307\n6\n\n\n5\n1.885446\n0.033292\n0.046460\n0.002780\n0.1\n64\n0.9\n{'learning_rate': 0.1, 'n_estimators': 64, 'su...\n0.792969\n0.788802\n0.779687\n0.794010\n0.789583\n0.789010\n0.005059\n7\n\n\n4\n1.903320\n0.040210\n0.046029\n0.002750\n0.1\n64\n0.8\n{'learning_rate': 0.1, 'n_estimators': 64, 'su...\n0.791146\n0.789323\n0.779687\n0.794531\n0.786979\n0.788333\n0.004978\n8\n\n\n3\n1.908362\n0.046139\n0.048469\n0.002625\n0.1\n64\n0.6\n{'learning_rate': 0.1, 'n_estimators': 64, 'su...\n0.790365\n0.787760\n0.782031\n0.790625\n0.788021\n0.787760\n0.003094\n9\n\n\n23\n1.799945\n0.035620\n0.043470\n0.001835\n0.05\n64\n0.9\n{'learning_rate': 0.05, 'n_estimators': 64, 's...\n0.785677\n0.781510\n0.778385\n0.792188\n0.780208\n0.783594\n0.004922\n10\n\n\n21\n1.614771\n0.134312\n0.058990\n0.018729\n0.05\n64\n0.6\n{'learning_rate': 0.05, 'n_estimators': 64, 's...\n0.782813\n0.780990\n0.777865\n0.791667\n0.782552\n0.783177\n0.004596\n11\n\n\n22\n1.707692\n0.017075\n0.042289\n0.001484\n0.05\n64\n0.8\n{'learning_rate': 0.05, 'n_estimators': 64, 's...\n0.781250\n0.782031\n0.777344\n0.793229\n0.781510\n0.783073\n0.005345\n12\n\n\n2\n0.572961\n0.046966\n0.019913\n0.003506\n0.1\n16\n0.9\n{'learning_rate': 0.1, 'n_estimators': 16, 'su...\n0.776823\n0.772396\n0.765625\n0.784635\n0.777083\n0.775313\n0.006240\n13\n\n\n1\n0.624717\n0.047690\n0.025584\n0.009838\n0.1\n16\n0.8\n{'learning_rate': 0.1, 'n_estimators': 16, 'su...\n0.773698\n0.771354\n0.769531\n0.783333\n0.776823\n0.774948\n0.004851\n14\n\n\n0\n1.088350\n0.267042\n0.020474\n0.001141\n0.1\n16\n0.6\n{'learning_rate': 0.1, 'n_estimators': 16, 'su...\n0.773958\n0.771354\n0.769792\n0.782031\n0.775260\n0.774479\n0.004234\n15\n\n\n15\n3.299162\n0.051481\n0.086740\n0.006313\n0.01\n128\n0.6\n{'learning_rate': 0.01, 'n_estimators': 128, '...\n0.776042\n0.770052\n0.766927\n0.783073\n0.774219\n0.774062\n0.005516\n16\n\n\n16\n3.348214\n0.025955\n0.079925\n0.002310\n0.01\n128\n0.8\n{'learning_rate': 0.01, 'n_estimators': 128, '...\n0.773958\n0.770312\n0.767448\n0.781250\n0.775521\n0.773698\n0.004711\n17\n\n\n17\n3.244455\n0.226082\n0.081706\n0.004990\n0.01\n128\n0.9\n{'learning_rate': 0.01, 'n_estimators': 128, '...\n0.772135\n0.770573\n0.767188\n0.779687\n0.774740\n0.772865\n0.004197\n18\n\n\n19\n0.321747\n0.030242\n0.019229\n0.002176\n0.05\n16\n0.8\n{'learning_rate': 0.05, 'n_estimators': 16, 's...\n0.772917\n0.766667\n0.769010\n0.781250\n0.774219\n0.772813\n0.005008\n19\n\n\n13\n1.583413\n0.019087\n0.044925\n0.000997\n0.01\n64\n0.8\n{'learning_rate': 0.01, 'n_estimators': 64, 's...\n0.773438\n0.767969\n0.767448\n0.778646\n0.772656\n0.772031\n0.004090\n20\n\n\n18\n0.431797\n0.144698\n0.019914\n0.002237\n0.05\n16\n0.6\n{'learning_rate': 0.05, 'n_estimators': 16, 's...\n0.772396\n0.767188\n0.765625\n0.781771\n0.771615\n0.771719\n0.005642\n21\n\n\n12\n1.603554\n0.050520\n0.044817\n0.000602\n0.01\n64\n0.6\n{'learning_rate': 0.01, 'n_estimators': 64, 's...\n0.772135\n0.765625\n0.766667\n0.782552\n0.769792\n0.771354\n0.006054\n22\n\n\n9\n0.425097\n0.140382\n0.018360\n0.000347\n0.01\n16\n0.6\n{'learning_rate': 0.01, 'n_estimators': 16, 's...\n0.773177\n0.766927\n0.767188\n0.779687\n0.769531\n0.771302\n0.004755\n23\n\n\n10\n0.368848\n0.027881\n0.023753\n0.012120\n0.01\n16\n0.8\n{'learning_rate': 0.01, 'n_estimators': 16, 's...\n0.771094\n0.769792\n0.769010\n0.776302\n0.768750\n0.770990\n0.002779\n24\n\n\n11\n0.466228\n0.029073\n0.022136\n0.004022\n0.01\n16\n0.9\n{'learning_rate': 0.01, 'n_estimators': 16, 's...\n0.767708\n0.767708\n0.770833\n0.776042\n0.771615\n0.770781\n0.003074\n25\n\n\n14\n1.631280\n0.042346\n0.052837\n0.012606\n0.01\n64\n0.9\n{'learning_rate': 0.01, 'n_estimators': 64, 's...\n0.770052\n0.768229\n0.767969\n0.776042\n0.770573\n0.770573\n0.002914\n26\n\n\n20\n0.407006\n0.015373\n0.018176\n0.000642\n0.05\n16\n0.9\n{'learning_rate': 0.05, 'n_estimators': 16, 's...\n0.768750\n0.767188\n0.766146\n0.775521\n0.772135\n0.769948\n0.003445\n27\n\n\n\n\n\n\n\n\nRunning the best params XGB Model\n\nprint(\"Best Param scores for XGB:\")\nxgb = XGBClassifier(**grid.best_params_,random_state=42)\n\nxgb.fit(X_train_sample, y_train_encoded)\n# Storing the model trained with best params later used for visualizations\nmodels[\"XGB Classifier\"] = xgb\n\nprint(\"Train Score: \", xgb.score(X_train_sample, y_train_encoded))\nprint(\"Validation Score: \", xgb.score(X_val_sample, y_val_encoded))\n\nBest Param scores for XGB:\nTrain Score:  0.8707291666666667\nValidation Score:  0.795\n\n\n\n\nXGB Conf. Matrix on Validation Set\n\ny_pred = xgb.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_encoded, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - XGB\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for XGB with best params\n\ny_pred = xgb.predict(X_val_sample)\n\naccuracy[\"XGB\"] = accuracy_score(y_val_encoded, y_pred)\nf1[\"XGB\"] = f1_score(y_val_encoded, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_encoded, xgb.predict(X_train_sample)))\nprint(classification_report(y_val_encoded, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.97      0.96      4822\n           1       0.86      0.74      0.80      4734\n           2       0.84      0.87      0.86      4865\n           3       0.84      0.89      0.86      4779\n\n    accuracy                           0.87     19200\n   macro avg       0.87      0.87      0.87     19200\nweighted avg       0.87      0.87      0.87     19200\n\n              precision    recall  f1-score   support\n\n           0       0.91      0.96      0.94      1193\n           1       0.75      0.63      0.69      1232\n           2       0.76      0.78      0.77      1206\n           3       0.74      0.82      0.78      1169\n\n    accuracy                           0.80      4800\n   macro avg       0.79      0.80      0.79      4800\nweighted avg       0.79      0.80      0.79      4800\n\n\n\n\n\nXGB Precision vs Recall Curve on Validation Set\n\nY = label_binarize(y_val_encoded, classes=[0, 1, 2, 3])\n\ny_score = xgb.predict_proba(X_val_sample)\n\nprecision[\"XGB\"], recall[\"XGB\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"XGB\"], tpr[\"XGB\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"XGB\"], precision[\"XGB\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - XGB\")\nplt.show()\n\n\n\n\n\n\nXGB ROC Curve on Validation Set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"XGB\"], tpr[\"XGB\"], where=\"post\")\n\nplt.title(\"ROC curve - XGB\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\n\n\nMulti Layer Perceptron\n\nMulti Layer Perceptron Grid Search CV (5 - folds) Run to find best params\n\nmlp = MLPClassifier(random_state=42, verbose=False)\nparameters = [{\"hidden_layer_sizes\": [(64, 32), (32, 64, 32)], \"max_iter\": [200], \"solver\": [\"sgd\", \"adam\"], \"activation\": [\"tanh\", \"relu\"]}]\ngrid = GridSearchCV(mlp, parameters, verbose = 0, n_jobs=-1)\n\ngrid.fit(X_train_sample, y_train_sample)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nbest_params[\"mlp\"] = grid.best_params_\nprint(\"Train score:\", grid.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", grid.score(X_val_sample, y_val_sample))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\nBest parameters scores:\n{'activation': 'tanh', 'hidden_layer_sizes': (64, 32), 'max_iter': 200, 'solver': 'adam'}\nTrain score: 0.8189583333333333\nValidation score: 0.7510416666666667\n\n\n\npd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_activation\nparam_hidden_layer_sizes\nparam_max_iter\nparam_solver\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n1\n19.106705\n0.435495\n0.009255\n0.000457\ntanh\n(64, 32)\n200\nadam\n{'activation': 'tanh', 'hidden_layer_sizes': (...\n0.756250\n0.752865\n0.740885\n0.762500\n0.755208\n0.753542\n0.007085\n1\n\n\n6\n22.971275\n0.389052\n0.008846\n0.000471\nrelu\n(32, 64, 32)\n200\nsgd\n{'activation': 'relu', 'hidden_layer_sizes': (...\n0.754167\n0.746615\n0.741146\n0.761458\n0.757812\n0.752240\n0.007409\n2\n\n\n3\n23.115873\n0.363470\n0.010795\n0.002369\ntanh\n(32, 64, 32)\n200\nadam\n{'activation': 'tanh', 'hidden_layer_sizes': (...\n0.742969\n0.749740\n0.746354\n0.761458\n0.754687\n0.751042\n0.006491\n3\n\n\n2\n20.719785\n0.305517\n0.009857\n0.000625\ntanh\n(32, 64, 32)\n200\nsgd\n{'activation': 'tanh', 'hidden_layer_sizes': (...\n0.748958\n0.743490\n0.734635\n0.752604\n0.757812\n0.747500\n0.007951\n4\n\n\n5\n24.164772\n0.728618\n0.011189\n0.004261\nrelu\n(64, 32)\n200\nadam\n{'activation': 'relu', 'hidden_layer_sizes': (...\n0.742708\n0.752344\n0.733333\n0.754167\n0.737760\n0.744062\n0.008091\n5\n\n\n4\n19.729288\n0.366591\n0.009005\n0.000415\nrelu\n(64, 32)\n200\nsgd\n{'activation': 'relu', 'hidden_layer_sizes': (...\n0.744531\n0.736198\n0.731510\n0.751563\n0.747396\n0.742240\n0.007354\n6\n\n\n7\n28.641310\n3.188170\n0.009927\n0.002859\nrelu\n(32, 64, 32)\n200\nadam\n{'activation': 'relu', 'hidden_layer_sizes': (...\n0.749740\n0.742188\n0.734375\n0.745313\n0.738542\n0.742031\n0.005310\n7\n\n\n0\n16.518863\n0.566479\n0.011748\n0.003198\ntanh\n(64, 32)\n200\nsgd\n{'activation': 'tanh', 'hidden_layer_sizes': (...\n0.730729\n0.725260\n0.714323\n0.737760\n0.732812\n0.728177\n0.008004\n8\n\n\n\n\n\n\n\n\n\nRunning the best params Multi Layer Perceptron Model\n\nprint(\"Best Param scores for Multi Layer Perceptron:\")\nmlp = MLPClassifier(**grid.best_params_, random_state=42)\n\nmlp.fit(X_train_sample, y_train_sample)\n# Storing the model trained with best params later used for visualizations\nmodels[\"Multi Layer Perceptron\"] = mlp\n\nprint(\"Train Score: \", mlp.score(X_train_sample, y_train_sample))\nprint(\"Validation Score: \", mlp.score(X_val_sample, y_val_sample))\n\nBest Param scores for Multi Layer Perceptron:\nTrain Score:  0.8189583333333333\nValidation Score:  0.7510416666666667\n\n\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\n\n\nMulti Layer Perceptron Conf. Matrix on Validation Set\n\ny_pred = mlp.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Multi Layer Perceptron\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Multi Layer Perceptron with best params\n\ny_pred = mlp.predict(X_val_sample)\n\naccuracy[\"Multi Layer Perceptron\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"Multi Layer Perceptron\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, mlp.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.91      0.97      0.94      4822\n           2       0.75      0.72      0.73      4734\n           3       0.80      0.79      0.80      4865\n           4       0.81      0.79      0.80      4779\n\n    accuracy                           0.82     19200\n   macro avg       0.82      0.82      0.82     19200\nweighted avg       0.82      0.82      0.82     19200\n\n              precision    recall  f1-score   support\n\n           1       0.88      0.95      0.91      1193\n           2       0.66      0.62      0.64      1232\n           3       0.74      0.71      0.73      1206\n           4       0.71      0.72      0.72      1169\n\n    accuracy                           0.75      4800\n   macro avg       0.75      0.75      0.75      4800\nweighted avg       0.75      0.75      0.75      4800\n\n\n\n\n\nMulti Layer Perceptron Precision vs Recall Curve on Validation Set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = mlp.predict_proba(X_val_sample)\n\nprecision[\"Multi Layer Perceptron\"], recall[\"Multi Layer Perceptron\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Multi Layer Perceptron\"], tpr[\"Multi Layer Perceptron\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Multi Layer Perceptron\"], precision[\"Multi Layer Perceptron\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Multi Layer Perceptron\")\nplt.show()\n\n\n\n\n\n\nMulti Layer Perceptron ROC Curve on Validation Set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Multi Layer Perceptron\"], tpr[\"Multi Layer Perceptron\"], where=\"post\")\n\nplt.title(\"ROC curve - Multi Layer Perceptron\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\n\n\nSupport Vector Machine\n\nGrid Search CV(5-folds) RUN to find best Params\n\nparameters = [{\"kernel\": [\"linear\", \"rbf\", \"sigmoid\"], \"C\": [.2, .5, .8, 1.]}, {\"kernel\": [\"poly\"], \"C\": [.2, .5, .8, 1.], \"degree\": [2, 3, 4]}]\nsvc = svm.SVC(verbose = 0, random_state = 42, shrinking = False)\ngrid = GridSearchCV(svc, parameters, verbose = 0, n_jobs = -1)\n\ngrid.fit(X_sample_train,y_sample_train)\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nbest_params[\"svc\"] = grid.best_params_\nprint(\"Train score: \", grid.score(X_sample_train, y_sample_train))\n\nBest parameters scores:\n{'C': 1.0, 'kernel': 'rbf'}\nTrain score:  0.7429166666666667\n\n\n\npd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_C\nparam_kernel\nparam_degree\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n10\n32.260032\n0.143948\n8.945977\n0.487535\n1.0\nrbf\nNaN\n{'C': 1.0, 'kernel': 'rbf'}\n0.732500\n0.729792\n0.716875\n0.716875\n0.737708\n0.726750\n0.008455\n1\n\n\n23\n27.574225\n4.497365\n3.685331\n0.800828\n1.0\npoly\n4\n{'C': 1.0, 'degree': 4, 'kernel': 'poly'}\n0.734375\n0.731042\n0.716875\n0.716250\n0.728125\n0.725333\n0.007432\n2\n\n\n22\n26.673938\n0.587722\n4.095733\n0.269865\n1.0\npoly\n3\n{'C': 1.0, 'degree': 3, 'kernel': 'poly'}\n0.734583\n0.730208\n0.714375\n0.715833\n0.730625\n0.725125\n0.008336\n3\n\n\n7\n31.650738\n0.603531\n8.659305\n0.425884\n0.8\nrbf\nNaN\n{'C': 0.8, 'kernel': 'rbf'}\n0.730625\n0.727708\n0.717292\n0.715625\n0.733542\n0.724958\n0.007201\n4\n\n\n20\n28.384177\n0.358249\n4.026727\n0.120665\n0.8\npoly\n4\n{'C': 0.8, 'degree': 4, 'kernel': 'poly'}\n0.735000\n0.728750\n0.716250\n0.716042\n0.727083\n0.724625\n0.007410\n5\n\n\n19\n26.849181\n0.535798\n3.905945\n0.042064\n0.8\npoly\n3\n{'C': 0.8, 'degree': 3, 'kernel': 'poly'}\n0.731875\n0.727083\n0.713333\n0.715625\n0.727708\n0.723125\n0.007285\n6\n\n\n17\n28.644837\n0.279786\n4.240965\n0.122288\n0.5\npoly\n4\n{'C': 0.5, 'degree': 4, 'kernel': 'poly'}\n0.729375\n0.725833\n0.714583\n0.711667\n0.724792\n0.721250\n0.006868\n7\n\n\n4\n30.592421\n0.220757\n8.872729\n0.275204\n0.5\nrbf\nNaN\n{'C': 0.5, 'kernel': 'rbf'}\n0.727083\n0.724375\n0.712917\n0.711042\n0.725417\n0.720167\n0.006767\n8\n\n\n16\n26.924972\n0.411861\n4.175091\n0.081210\n0.5\npoly\n3\n{'C': 0.5, 'degree': 3, 'kernel': 'poly'}\n0.728125\n0.725208\n0.712083\n0.710417\n0.723542\n0.719875\n0.007213\n9\n\n\n21\n25.660745\n0.220622\n3.786673\n0.033971\n1.0\npoly\n2\n{'C': 1.0, 'degree': 2, 'kernel': 'poly'}\n0.728333\n0.723333\n0.710000\n0.710208\n0.724583\n0.719292\n0.007680\n10\n\n\n18\n27.656106\n0.190797\n4.025929\n0.132961\n0.8\npoly\n2\n{'C': 0.8, 'degree': 2, 'kernel': 'poly'}\n0.727083\n0.720208\n0.707708\n0.708542\n0.722708\n0.717250\n0.007773\n11\n\n\n15\n26.182213\n0.646138\n4.284178\n0.215900\n0.5\npoly\n2\n{'C': 0.5, 'degree': 2, 'kernel': 'poly'}\n0.721875\n0.717500\n0.704167\n0.701250\n0.718333\n0.712625\n0.008281\n12\n\n\n14\n26.844376\n0.167522\n4.344861\n0.056929\n0.2\npoly\n4\n{'C': 0.2, 'degree': 4, 'kernel': 'poly'}\n0.718125\n0.714583\n0.707708\n0.701042\n0.719792\n0.712250\n0.006973\n13\n\n\n13\n26.483538\n0.216705\n4.263598\n0.081068\n0.2\npoly\n3\n{'C': 0.2, 'degree': 3, 'kernel': 'poly'}\n0.718750\n0.717917\n0.703125\n0.700625\n0.717917\n0.711667\n0.008040\n14\n\n\n1\n32.616739\n0.553255\n9.559076\n0.390377\n0.2\nrbf\nNaN\n{'C': 0.2, 'kernel': 'rbf'}\n0.716042\n0.710833\n0.701875\n0.697917\n0.715833\n0.708500\n0.007376\n15\n\n\n12\n26.256505\n0.277525\n4.265289\n0.069630\n0.2\npoly\n2\n{'C': 0.2, 'degree': 2, 'kernel': 'poly'}\n0.710208\n0.707917\n0.694792\n0.690000\n0.709792\n0.702542\n0.008457\n16\n\n\n9\n39.759664\n3.014081\n4.062565\n0.184938\n1.0\nlinear\nNaN\n{'C': 1.0, 'kernel': 'linear'}\n0.707500\n0.703958\n0.688542\n0.695000\n0.707917\n0.700583\n0.007605\n17\n\n\n6\n37.558125\n1.867674\n4.326769\n0.308217\n0.8\nlinear\nNaN\n{'C': 0.8, 'kernel': 'linear'}\n0.707500\n0.703542\n0.688750\n0.695208\n0.707292\n0.700458\n0.007354\n18\n\n\n3\n32.760351\n1.825576\n4.020657\n0.227193\n0.5\nlinear\nNaN\n{'C': 0.5, 'kernel': 'linear'}\n0.706250\n0.701250\n0.688333\n0.694375\n0.706667\n0.699375\n0.007083\n19\n\n\n0\n27.609676\n0.675959\n3.929613\n0.140739\n0.2\nlinear\nNaN\n{'C': 0.2, 'kernel': 'linear'}\n0.704792\n0.701458\n0.686250\n0.693125\n0.705000\n0.698125\n0.007333\n20\n\n\n2\n48.653246\n0.532096\n9.786308\n0.296511\n0.2\nsigmoid\nNaN\n{'C': 0.2, 'kernel': 'sigmoid'}\n0.485625\n0.496250\n0.490208\n0.499167\n0.492500\n0.492750\n0.004706\n21\n\n\n5\n44.946205\n1.873551\n9.529393\n0.316044\n0.5\nsigmoid\nNaN\n{'C': 0.5, 'kernel': 'sigmoid'}\n0.433125\n0.441458\n0.439375\n0.453750\n0.443958\n0.442333\n0.006742\n22\n\n\n8\n40.732482\n1.281626\n8.942905\n0.369305\n0.8\nsigmoid\nNaN\n{'C': 0.8, 'kernel': 'sigmoid'}\n0.415000\n0.425000\n0.422292\n0.451042\n0.432292\n0.429125\n0.012277\n23\n\n\n11\n38.995517\n1.054987\n8.819515\n0.168747\n1.0\nsigmoid\nNaN\n{'C': 1.0, 'kernel': 'sigmoid'}\n0.411042\n0.422292\n0.420208\n0.452708\n0.429375\n0.427125\n0.014067\n24\n\n\n\n\n\n\n\n\n\nRunning the best params SVM model\n\n### We run the model on partial Dataset for fast training and observing the op/s.\nprint(\"Best Param scores for SVM:\")\nsvc = svm.SVC(verbose = 0, random_state = 42, shrinking = False, C=1, kernel = 'rbf', degree = 4)\n\nsvc.fit(X_train_sample, y_train_sample)\n# Storing the model trained with best params later used for visualizations\nmodels[\"Support Vector Machine\"] = svc\n\nprint(\"Train score:\", svc.score(X_train_sample, y_train_sample))\nprint(\"Validation score:\", svc.score(X_val_sample, y_val_sample))\n\nBest Param scores for SVM:\nTrain score: 0.7433333333333333\nValidation score: 0.7204166666666667\n\n\n\n\nSVM Conf. Matrix on Validation set\n\ny_pred = svc.predict(X_val_sample)\nconfmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Support Vector Machine\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for SVM with best params\n\ny_pred = svc.predict(X_val_sample)\n\naccuracy[\"SVM\"] = accuracy_score(y_val_sample, y_pred)\nf1[\"SVM\"] = f1_score(y_val_sample, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_sample, svc.predict(X_train_sample)))\nprint(classification_report(y_val_sample, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.80      0.91      0.85      4822\n           2       0.72      0.50      0.59      4734\n           3       0.74      0.80      0.77      4865\n           4       0.70      0.76      0.73      4779\n\n    accuracy                           0.74     19200\n   macro avg       0.74      0.74      0.73     19200\nweighted avg       0.74      0.74      0.74     19200\n\n              precision    recall  f1-score   support\n\n           1       0.77      0.90      0.83      1193\n           2       0.69      0.48      0.56      1232\n           3       0.74      0.77      0.75      1206\n           4       0.67      0.74      0.70      1169\n\n    accuracy                           0.72      4800\n   macro avg       0.72      0.72      0.71      4800\nweighted avg       0.72      0.72      0.71      4800\n\n\n\n\n\nSVM Precision vs Recall Curve on Validation set\n\nY = label_binarize(y_val_sample, classes=[1, 2, 3, 4])\n\ny_score = svc.decision_function(X_val_sample)\n\nprecision[\"SVM\"], recall[\"SVM\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"SVM\"], tpr[\"SVM\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"SVM\"], precision[\"SVM\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Support Vector Machine\")\nplt.show()\n\n\n\n\n\n\nSVM ROC Curve on Validation set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"SVM\"], tpr[\"SVM\"], where=\"post\")\n\nplt.title(\"ROC curve - Support Vector Machine\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html#result-and-comparisions-after-training-on-partial-data",
    "href": "docs/projects/project-4-us-accidents-model-runs.html#result-and-comparisions-after-training-on-partial-data",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "Result and Comparisions after Training on Partial Data",
    "text": "Result and Comparisions after Training on Partial Data\n\nPartial data is used to train since models like SVM can‚Äôt train on the whole dataset, efficiently, until Kaggle notebook Time‚Äôs Out.\nSo we use partial data to train the model and then select the best of these by comparing results on Validation data\n\n\nAccuracy Metric Comparision between Models\n\nplt.figure(figsize=(20, 8))\nplt.title(\"Accuracy on Validation Set for Each Model\")\nsorted_accuracy = sorted(accuracy.items(), key=lambda x: x[1])\nsorted_labels = [item[0] for item in sorted_accuracy]\nsorted_values = [item[1] for item in sorted_accuracy]\nsns.barplot(x=sorted_labels, y=sorted_values)\nplt.xticks(rotation=90)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.savefig(\"All_Models_Accuracy_Score_Plot_on_Val_Set.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\n\nF1-Score Metric Comparision between Models\n\nplt.figure(figsize=(20, 8))\nplt.title(\"F1 Score on Validation set for each model\")\nsorted_f1 = sorted(f1.items(), key=lambda x: x[1])\nsorted_labels = [item[0] for item in sorted_f1]\nsorted_values = [item[1] for item in sorted_f1]\nsns.barplot(x=sorted_labels, y=sorted_values)\nplt.xticks(rotation=90)\nplt.xlabel(\"Model\")\nplt.ylabel(\"F1- Score\")\nplt.savefig(\"All_Models_F1_Score_Plot_on_Val_Set.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\n\nPR Curve Comparision between Models\n\nplt.figure(figsize=(20, 8))\nfor key in f1.keys():\n    plt.step(recall[key], precision[key], where=\"post\", label=key)\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR curve\")\nplt.legend()\nplt.savefig(\"All_Models_PR_Curve_plot.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\n\nROC Curve Comparision between Models\n\nplt.figure(figsize=(20, 8))\nfor key in f1.keys():\n    plt.step(fpr[key], tpr[key], where=\"post\", label=key)\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"ROC curve\")\nplt.legend()\nplt.savefig(\"All_Models_ROC_Curve_plot.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\naxs = axs.ravel()\nfor i, (name, model) in enumerate(models.items()):\n    ax = axs[i]\n    y_pred = model.predict(X_val_sample)\n    if name == \"XGB Classifier\":\n        confmat = confusion_matrix(y_true=y_val_encoded, y_pred=y_pred)\n    else:\n        confmat = confusion_matrix(y_true=y_val_sample, y_pred=y_pred)\n\n    index = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\n    columns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n    conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\", ax=ax)\n    ax.set_title(f\"Confusion Matrix - {name}\")\n\n# Hide the remaining subplots\nfor j in range(len(models), len(axs)):\n    axs[j].axis('off')\n\nplt.tight_layout()\nplt.savefig(\"All_Models_Conf_Matrix_Plot.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\n\nModel Training Observations\n\nEvery model trained at different speeds, I believe this majorly was due to different complexity of models\nBy observing the performance metrics and curves on validation data, I think the best models are XGB classifier and Random Forest Classifier\nThis can even be observed by the confusion matrices\nWe will now train Random Forest Classifier and XGB Classifier with the best params on full dataset and save the pickle files"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html#best-model-training-on-whole-datasets",
    "href": "docs/projects/project-4-us-accidents-model-runs.html#best-model-training-on-whole-datasets",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "Best Model Training On Whole Datasets",
    "text": "Best Model Training On Whole Datasets\n\nRandom Forest Classifier\n\nprint(\"Random Forest Classifier Scores After Training on Whole Dataset:\")\nbest_rfc = RandomForestClassifier(**best_params['rfc'],random_state = 42)\nbest_rfc.fit(X_train, y_train)\nprint(\"Train Score: \", best_rfc.score(X_train, y_train))\nprint(\"Validation Score: \", best_rfc.score(X_val, y_val))\n\nRandom Forest Classifier Scores After Training on Whole Dataset:\nTrain Score:  0.9965762117919476\nValidation Score:  0.8146111936602278\n\n\n\nRandom Forest Classifier Conf. Matrix on Validation Set\n\ny_pred = best_rfc.predict(X_val)\nconfmat = confusion_matrix(y_true=y_val, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Random Forest Best\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for Random Forest Classifier with best params\n\ny_pred = best_rfc.predict(X_val)\n\naccuracy[\"Random Forest Best\"] = accuracy_score(y_val, y_pred)\nf1[\"Random Forest Best\"] = f1_score(y_val, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, best_rfc.predict(X_train)))\nprint(classification_report(y_val, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00     40486\n           2       1.00      0.99      1.00     40372\n           3       0.99      1.00      1.00     40370\n           4       1.00      1.00      1.00     40289\n\n    accuracy                           1.00    161517\n   macro avg       1.00      1.00      1.00    161517\nweighted avg       1.00      1.00      1.00    161517\n\n              precision    recall  f1-score   support\n\n           1       0.93      0.96      0.94     10079\n           2       0.77      0.66      0.71     10070\n           3       0.78      0.80      0.79     10087\n           4       0.77      0.84      0.80     10144\n\n    accuracy                           0.81     40380\n   macro avg       0.81      0.81      0.81     40380\nweighted avg       0.81      0.81      0.81     40380\n\n\n\n\n\nRandom Forest Classifier Precision vs Recall Curve on Validation Set\n\nY = label_binarize(y_val, classes=[1, 2, 3, 4])\n\ny_score = rfc.predict_proba(X_val)\n\nprecision[\"Random Forest Best\"], recall[\"Random Forest Best\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Random Forest Best\"], tpr[\"Random Forest Best\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Random Forest Best\"], precision[\"Random Forest Best\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Random Forest Best\")\nplt.show()\n\n\n\n\n\n\nRandom Forest Classifier ROC Curve on Validation Set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"Random Forest Best\"], tpr[\"Random Forest Best\"], where=\"post\")\n\nplt.title(\"ROC curve - Random Forest Best\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n\nimportances.iloc[:,0] = best_rfc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()\n\n\n\n\n\n\n\nXGBoost Classifier\n\nThis algo requires the target labels to start from 0 so we transform the target | Original Label | Encoded Label | | ‚Äî‚Äî‚Äî‚Äî‚Äì | ‚Äî‚Äî‚Äî‚Äî- | | 1 | 0 | | 2 | 1 | | 3 | 2 | | 4 | 3 |\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create an instance of LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the target variable\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.transform(y_val)\n\n\nRunning the best params XGB Model\n\nprint(\"XGB Classifier Scores After Training on Whole Dataset:\")\n# \nbest_xgb = XGBClassifier(**best_params['xgb'],random_state=42)\nbest_xgb.fit(X_train, y_train_encoded)\nprint(\"Train Score: \", best_xgb.score(X_train, y_train_encoded))\nprint(\"Validation Score: \", best_xgb.score(X_val, y_val_encoded))\n\nXGB Classifier Scores After Training on Whole Dataset:\nTrain Score:  0.8237894463121529\nValidation Score:  0.8105002476473502\n\n\n\n\nXGB Conf. Matrix on Validation Set\n\ny_pred = xgb.predict(X_val)\nconfmat = confusion_matrix(y_true=y_val_encoded, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - XGB Best\")\nplt.show()\n\n\n\n\n\n\nMetrics Scores and Metric Visualizations for XGB with best params\n\ny_pred = xgb.predict(X_val)\n\naccuracy[\"XGB_best\"] = accuracy_score(y_val_encoded, y_pred)\nf1[\"XGB_best\"] = f1_score(y_val_encoded, y_pred, average=\"macro\")\n\nprint(classification_report(y_train_encoded, best_xgb.predict(X_train)))\nprint(classification_report(y_val_encoded, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.96      0.95     40486\n           1       0.79      0.68      0.73     40372\n           2       0.79      0.82      0.81     40370\n           3       0.78      0.83      0.81     40289\n\n    accuracy                           0.82    161517\n   macro avg       0.82      0.82      0.82    161517\nweighted avg       0.82      0.82      0.82    161517\n\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.95     10079\n           1       0.80      0.68      0.74     10070\n           2       0.80      0.83      0.81     10087\n           3       0.80      0.85      0.82     10144\n\n    accuracy                           0.83     40380\n   macro avg       0.83      0.83      0.83     40380\nweighted avg       0.83      0.83      0.83     40380\n\n\n\n\n\nXGB Precision vs Recall Curve on Validation Set\n\nY = label_binarize(y_val_encoded, classes=[0, 1, 2, 3])\n\ny_score = best_xgb.predict_proba(X_val)\n\nprecision[\"XGB_best\"], recall[\"XGB_best\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"XGB_best\"], tpr[\"XGB_best\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"XGB_best\"], precision[\"XGB_best\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - XGB_best\")\nplt.show()\n\n\n\n\n\n\nXGB ROC Curve on Validation Set\n\nplt.figure(figsize=(18, 10))\nplt.step(fpr[\"XGB_best\"], tpr[\"XGB_best\"], where=\"post\")\n\nplt.title(\"ROC curve - XGB_best\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()\n\n\n\n\n\n\n\nComparing Results between XGB n Random Forest Regressor after training on whole dataset\n\nplt.figure(figsize=(20, 8))\nkeys = [\"XGB_best\",\"Random Forest Best\"]\nfor key in keys:\n    plt.step(fpr[key], tpr[key], where=\"post\", label=key)\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"ROC curve\")\nplt.legend()\nplt.savefig(\"Best_Model_Comparision_ROC_Curve_plot.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(20, 8))\nkeys = [\"XGB_best\",\"Random Forest Best\"]\nfor key in keys:\n    plt.step(recall[key], precision[key], where=\"post\", label=key)\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR curve\")\nplt.legend()\nplt.savefig(\"Best_Model_Comparision_PR_Curve_plot.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(18, 12))\nbest_models = {\"XGB_best\": best_xgb, \"Random Forest Best\": best_rfc}\naxs = axs.ravel()\nfor i, (name, model) in enumerate(best_models.items()):\n    ax = axs[i]\n    y_pred = model.predict(X_val)\n    if name == \"XGB_best\":\n        confmat = confusion_matrix(y_true=y_val_encoded, y_pred=y_pred)\n    else:\n        confmat = confusion_matrix(y_true=y_val, y_pred=y_pred)\n\n    index = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\n    columns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n    conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\", ax=ax)\n    ax.set_title(f\"Confusion Matrix - {name}\")\n\n# Hide the remaining subplots\nfor j in range(len(models), len(axs)):\n    axs[j].axis('off')\n\nplt.tight_layout()\nplt.savefig(\"Best Models Conf_Matrix_Plot.png\",bbox_inches = 'tight', dpi = 300)\nplt.show()"
  },
  {
    "objectID": "docs/projects/project-4-us-accidents-model-runs.html#conclusion-of-model-training",
    "href": "docs/projects/project-4-us-accidents-model-runs.html#conclusion-of-model-training",
    "title": "Model Training on US-Accidents Processed Dataset",
    "section": "Conclusion of Model Training",
    "text": "Conclusion of Model Training\n\nRandom Forest classifier performs the best on US-Accidents Severity Level Predictions\nOverall every model performs well on Severity Levels 1,3,4 on Validation sets, however the accuracy and recall decreases on Severity Level 2\n\nThe reasons for this can mainly be while balancing the data the sample that we chose for 2 Severity is not a good representative\nThis is where one can retrain the models by choosing various random samples, or try including more data of Level 2 Severity and then retrain the models\n\nFurther every model can be trained using cuml GPU implementations on whole dataset, and then do more dense comparisions between models to select the best one\nThank you for reading the work üòä"
  },
  {
    "objectID": "docs/projects/RL3_DeepQ-Learning.html",
    "href": "docs/projects/RL3_DeepQ-Learning.html",
    "title": "RL Unit 2: Introduction to Q Learning",
    "section": "",
    "text": "Back in previous Q-learning unit:\n\nWe implemented a Q-learning algorithm from scratch and trained it on Taxi-v3 and FrozenLake-v1 env‚Äôs\nWe got excellent results with this simple algorithm, but these environments were relatively simple because the state space was discrete and small\nHowever, we need to work on a bit complex problems as well, such as Atari games which has \\(10^9\\) to \\(10^{11}\\) states\nIn such huge state space, producing and updating a Q-table can become ineffective\nThus we will use Deep Q-Learning, uses Neural Network that takes a state and approximates Q-values for each action based on that state\nIn this unit, we will train an agent to play Space Invaders and other Atari environments using RL-Zoo, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n\n\n\n\n\nQ-Learning is an algorithm we use to train our Q-function, an action value function that determines the value of being at a particular state and taking a specific action at that state.\nQ here stands for quality of that action at that state, internally Q-function, is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Q-table serves as the memory of our Q-function\nHowever, Q-learning is a tabular method, this is fine for small state space, but if state space becomes large Q-learning is not scalable to such problems.\nFor ex. Atari environments have an observation space with a shape of (210,160,3)* containing values of 0 to 255 this gives us N = \\(256^{210x160x3}\\) possible observations (for comparision we have approx \\(10^{80}\\) atoms in the observable universe)\nSo overall we can say that we will have a Q-table of N by A, and that‚Äôs again huge\n\n\n\nThus we can see that the state space is gigantic, due to this, creating and updating a Q-table for that environment would not be efficient.\nIn this case, the best idea is to approximate Q-values using a parameterized Q-function \\(Q_{\\theta}(s,a)\\)\nWe will use a neural network that approximates Q-values for a given state, for each possible action at that state\n\n\n\n\n\n\nThis is the architecture of our Deep Q-Learning network:\n\n\n\nAs input, we take a stack of 4 frames passed through the network as a state and output a vector of Q-values for each possible action at that state.\nThen, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take\nWhen the Neural Network is initialized, the Q-value estimation is terrible, but during training, our Deep Q-Network agent will associate a situation with the appropriate action and learn to play the game well.\n\n\n\n\nWe need to preprocess the input, it‚Äôs an essential step since we would like to reduce the complexity of our state to reduce the computation time needed for training\nTo achieve this, we reduce the state space to 84x84 and grayscale it. We can do this since the colors in Atari environments, don‚Äôt add important information. This is a big improvement since we reduce our three color channels (RGB) to 1.\nWe can also crop a part of this scren in some games if it doesn‚Äôt add any crucial information. Then we stack 4 frames together.\nThis stacking is necessary since it helps us to handle the problem of temporal limitation. Basically having a single frame doesn‚Äôt give us any idea about motion, however if we stack more frames we capture temporal information.\n\n\n\nThese stacked frames are processed by 3 convolutional layers. These layers allow us to capture and exploit spatial relationships in images. But also, because the frames are stacked together, we can exploit some temporal properties across those frames. Finally, we have a couple of fully connected layers that output a Q-value for each possible action at that state.\n\n\n\nSo we can basically see that Deep Q-learning given a state, uses a neural network to approximate, the different Q-values for each possible action at that state.\n\n\n\n\n\n\nNow we know Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state (value-function estimation)\nThe main difference between Q-Learning and Deep Q-Learning is that during training phase, instead of updating the Q-value of a state-action pair directly as we have done with Q-Learning, in Deep Q-Learning, we create a loss function that compares our Q-value prediction and the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better So we need to have Q-targets and Q-value predictions\n\n\n\nThe Deep Q-Learning training algorithm has 2 phases:\n\nSampling: we perform actions and store the observed experience tuples in a replay memory\nTraining: Select a small batch of tuples randomly and learn from this batch using a gradient descent update step\n\n\n\n\nThis is not the only difference compared with Q-Learning.\nDeep Q-Learning training might suffer from instability, mainly because of combining a non-linear Q-value function (Neural Network) and bootstrapping (when we update targets with existing estimates and not an actual complete return)\nTo help us stabilize the training, we implement 3 different solutions:\n\nExperience Replay to make more efficient use of experiences\nFixed Q-Target to stabilize the training\nDouble Deep Q-Learning, to handle the problem of the overestimation of Q-values\n\nLet‚Äôs go through them!\n\n\n\n\nWhy to create a replay memory?\n\nExperience Replay in Deep Q-Learning has 2 functions:\n\n1. Make more efficient use of the experiences during the training. Usually, in online re-inforcement learning, the agent interacts with the environment, gets experience (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient\n\nExperience replay helps by using the experiences of the training more efficiently. We use a replay buffer that saves experience samples that we can reuse during the training\nThis allows the agent to learn from the same experiences multiple times.\n\n2. Avoid forgetting previous experiences and reduce the correlation between experiences.\n\nThe problem we get if we give sequential samples of experiences to our neural network is that it tends to forget the previous experiences as it gets new experiences. For instance, if the agent is in the first level and then in the second, which is different, it can forget how to behave and play in the first level.\nThe solution is to create a Replay Buffer that stores experience tuples while interacting with the environment and then sample a small batch of tuples. This prevents the network from only learning about what it has done immediately before\n\n\nExperience replay also has other benefits. By randomly sampling the experiences, we remove correlation in the observation sequences and avoid action values from Oscilating or Diverging catastrophically\nIn Deep Q-Learning psuedocode, we initialize a replay memory buffer D with capacity N (N is a hyperparameter that you can define). We then store experiences in the memory and sample a batch of experiences to feed the Deep Q-Network during the training phase.\n\n\n\n\n\n\n\nWhen we want to calculate the TD error (aka the loss), we calculate the difference between the TD target (Q-Target) and the current Q-value (estimation of Q)\nBut we don‚Äôt have any idea of the real TD target. We need to estimate it. Using the Bellman equation, we saw that the TD target is just the reward of taking that action at that state plus the discounted highest Q-value for the next state.\nHowever, the problem is that we are using the same parameters (weights) for estimating the TD target and the Q-value. Consequently, there is a significant correlation between the TD target and the parameters we are changing. Therefore, at every step of training, both our Q-values and the target values shift. We‚Äôre getting closer to our target, but the target is also moving. It‚Äôs like chasing a moving target! This can lead to significant oscillation in training.\nFor ex.\n\nIt‚Äôs like if you were a cowboy (the Q estimation) and you wanted to catch a cow (the Q-target). Your goal is to get closer (reduce the error).\nAt each time step, you‚Äôre trying to approach the cow, which also moves at each time step (because you move the same parameters)\nThis leads to a bizzare path of chasing (a significant oscillating in training)\nInstead, what we see in the psuedo-code is that we:\n\nUse a separate network with fixed parameters for estimating the TD Target\nCopy the parameters from our Deep Q-Network every C steps to update the target network\n\n\n\n\n\n\n\n\n\nDouble DQN‚Äôs, or Double Q-Learning neural networks, were introduced by Hado van Hasselt. This method handles the problem of the overestimation of Q-values\nTo understand this problem, remember how we can calculate the TD Target: \nWe face a simple problem by calculating the TD target: How are we sure that the best action for the next state is the action with the highest Q-value?\nWe know that the accuracy of Q-values depend on what action we tried and what neighboring states we explored\nConsequently, we don‚Äôt have enough information about the best action to take at the beginning of the training. Therefore, taking the maximum Q-value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly given a higher Q value than the optimal best action, the learning will be complicated\nThe solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q-value generation. We:\n\nUse our DQN network to select the best action to take for the next state (the action with the highest Q-value)\nUse our Target network to calculate the target Q-value of taking that action at the next state\n\nTherefore, Double DQN helps us reduce the overestimation of Q-values and, as a consequence, helps us train faster and with stable learning.\nSince these 3 improvements in Deep Q-Learning, many more have been added, such as Prioritized Experience Replay and Dueling Deep Q-Learning.\n\n\n\n\n\nTabular Method: Type of problem in which the state and action spaces are small enough to approximate value functions to be represented as arrays and tables. Q-learning is an example of tabular method since a table is used to represent the value for different state-action pairs.\nDeep Q-Learning: Method that trains a neural network to approximate, given a state, the different Q-values for each possible action at that state. It is used to solve problems when observational space is too big to apply a tabular Q-Learning approach.\nTemporal Limitation is a difficulty presented when the environment state is represented by frames. A frame by itself does not provide temporal information. In order to obtain temporal information, we need to stack a number of frames together.\nPhases of Deep Q-Learning:\n\nSampling: Actions are performed, and observed experience tuples are stored in a replay memory.\nTraining: Batches of tuples are selected randomly and the neural network updates its weights using gradient descent.\n\nSolutions to stabilize Deep Q-Learning:\n\nExperience Replay: A replay memory is created to save experiences samples that can be reused during training. This allows the agent to learn from the same experiences multiple times. Also, it helps the agent avoid forgetting previous experiences as it gets new ones.\nRandom sampling from replay buffer allows to remove correlation in the observation sequences and prevents action values from oscillating or diverging catastrophically.\nFixed Q-Target: In order to calculate the Q-Target we need to estimate the discounted optimal Q-value of the next state by using Bellman equation. The problem is that the same network weights are used to calculate the Q-Target and the Q-value. This means that everytime we are modifying the Q-value, the Q-Target also moves with it. To avoid this issue, a separate network with fixed parameters is used for estimating the Temporal Difference Target. The target network is updated by copying parameters from our Deep Q-Network after certain C steps.\nDouble DQN: Method to handle overestimation of Q-Values. This solution uses two networks to decouple the action selection from the target Value generation:\n\nDQN Network to select the best action to take for the next state (the action with the highest Q-Value)\nTarget Network to calculate the target Q-Value of taking that action at the next state. This approach reduces the Q-Values overestimation, it helps to train faster and have more stable learning.\n\n\n\n\n\nCode\na = {1:'A',2:'B',3:'C'}\na.update({1:'D'})\n\n\n\n\nCode\na\n\n\n{1: 'D', 2: 'B', 3: 'C'}"
  },
  {
    "objectID": "docs/projects/RL3_DeepQ-Learning.html#chapter-3-deep-q-learning",
    "href": "docs/projects/RL3_DeepQ-Learning.html#chapter-3-deep-q-learning",
    "title": "RL Unit 2: Introduction to Q Learning",
    "section": "",
    "text": "Back in previous Q-learning unit:\n\nWe implemented a Q-learning algorithm from scratch and trained it on Taxi-v3 and FrozenLake-v1 env‚Äôs\nWe got excellent results with this simple algorithm, but these environments were relatively simple because the state space was discrete and small\nHowever, we need to work on a bit complex problems as well, such as Atari games which has \\(10^9\\) to \\(10^{11}\\) states\nIn such huge state space, producing and updating a Q-table can become ineffective\nThus we will use Deep Q-Learning, uses Neural Network that takes a state and approximates Q-values for each action based on that state\nIn this unit, we will train an agent to play Space Invaders and other Atari environments using RL-Zoo, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n\n\n\n\n\nQ-Learning is an algorithm we use to train our Q-function, an action value function that determines the value of being at a particular state and taking a specific action at that state.\nQ here stands for quality of that action at that state, internally Q-function, is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Q-table serves as the memory of our Q-function\nHowever, Q-learning is a tabular method, this is fine for small state space, but if state space becomes large Q-learning is not scalable to such problems.\nFor ex. Atari environments have an observation space with a shape of (210,160,3)* containing values of 0 to 255 this gives us N = \\(256^{210x160x3}\\) possible observations (for comparision we have approx \\(10^{80}\\) atoms in the observable universe)\nSo overall we can say that we will have a Q-table of N by A, and that‚Äôs again huge\n\n\n\nThus we can see that the state space is gigantic, due to this, creating and updating a Q-table for that environment would not be efficient.\nIn this case, the best idea is to approximate Q-values using a parameterized Q-function \\(Q_{\\theta}(s,a)\\)\nWe will use a neural network that approximates Q-values for a given state, for each possible action at that state\n\n\n\n\n\n\nThis is the architecture of our Deep Q-Learning network:\n\n\n\nAs input, we take a stack of 4 frames passed through the network as a state and output a vector of Q-values for each possible action at that state.\nThen, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take\nWhen the Neural Network is initialized, the Q-value estimation is terrible, but during training, our Deep Q-Network agent will associate a situation with the appropriate action and learn to play the game well.\n\n\n\n\nWe need to preprocess the input, it‚Äôs an essential step since we would like to reduce the complexity of our state to reduce the computation time needed for training\nTo achieve this, we reduce the state space to 84x84 and grayscale it. We can do this since the colors in Atari environments, don‚Äôt add important information. This is a big improvement since we reduce our three color channels (RGB) to 1.\nWe can also crop a part of this scren in some games if it doesn‚Äôt add any crucial information. Then we stack 4 frames together.\nThis stacking is necessary since it helps us to handle the problem of temporal limitation. Basically having a single frame doesn‚Äôt give us any idea about motion, however if we stack more frames we capture temporal information.\n\n\n\nThese stacked frames are processed by 3 convolutional layers. These layers allow us to capture and exploit spatial relationships in images. But also, because the frames are stacked together, we can exploit some temporal properties across those frames. Finally, we have a couple of fully connected layers that output a Q-value for each possible action at that state.\n\n\n\nSo we can basically see that Deep Q-learning given a state, uses a neural network to approximate, the different Q-values for each possible action at that state.\n\n\n\n\n\n\nNow we know Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state (value-function estimation)\nThe main difference between Q-Learning and Deep Q-Learning is that during training phase, instead of updating the Q-value of a state-action pair directly as we have done with Q-Learning, in Deep Q-Learning, we create a loss function that compares our Q-value prediction and the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better So we need to have Q-targets and Q-value predictions\n\n\n\nThe Deep Q-Learning training algorithm has 2 phases:\n\nSampling: we perform actions and store the observed experience tuples in a replay memory\nTraining: Select a small batch of tuples randomly and learn from this batch using a gradient descent update step\n\n\n\n\nThis is not the only difference compared with Q-Learning.\nDeep Q-Learning training might suffer from instability, mainly because of combining a non-linear Q-value function (Neural Network) and bootstrapping (when we update targets with existing estimates and not an actual complete return)\nTo help us stabilize the training, we implement 3 different solutions:\n\nExperience Replay to make more efficient use of experiences\nFixed Q-Target to stabilize the training\nDouble Deep Q-Learning, to handle the problem of the overestimation of Q-values\n\nLet‚Äôs go through them!\n\n\n\n\nWhy to create a replay memory?\n\nExperience Replay in Deep Q-Learning has 2 functions:\n\n1. Make more efficient use of the experiences during the training. Usually, in online re-inforcement learning, the agent interacts with the environment, gets experience (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient\n\nExperience replay helps by using the experiences of the training more efficiently. We use a replay buffer that saves experience samples that we can reuse during the training\nThis allows the agent to learn from the same experiences multiple times.\n\n2. Avoid forgetting previous experiences and reduce the correlation between experiences.\n\nThe problem we get if we give sequential samples of experiences to our neural network is that it tends to forget the previous experiences as it gets new experiences. For instance, if the agent is in the first level and then in the second, which is different, it can forget how to behave and play in the first level.\nThe solution is to create a Replay Buffer that stores experience tuples while interacting with the environment and then sample a small batch of tuples. This prevents the network from only learning about what it has done immediately before\n\n\nExperience replay also has other benefits. By randomly sampling the experiences, we remove correlation in the observation sequences and avoid action values from Oscilating or Diverging catastrophically\nIn Deep Q-Learning psuedocode, we initialize a replay memory buffer D with capacity N (N is a hyperparameter that you can define). We then store experiences in the memory and sample a batch of experiences to feed the Deep Q-Network during the training phase.\n\n\n\n\n\n\n\nWhen we want to calculate the TD error (aka the loss), we calculate the difference between the TD target (Q-Target) and the current Q-value (estimation of Q)\nBut we don‚Äôt have any idea of the real TD target. We need to estimate it. Using the Bellman equation, we saw that the TD target is just the reward of taking that action at that state plus the discounted highest Q-value for the next state.\nHowever, the problem is that we are using the same parameters (weights) for estimating the TD target and the Q-value. Consequently, there is a significant correlation between the TD target and the parameters we are changing. Therefore, at every step of training, both our Q-values and the target values shift. We‚Äôre getting closer to our target, but the target is also moving. It‚Äôs like chasing a moving target! This can lead to significant oscillation in training.\nFor ex.\n\nIt‚Äôs like if you were a cowboy (the Q estimation) and you wanted to catch a cow (the Q-target). Your goal is to get closer (reduce the error).\nAt each time step, you‚Äôre trying to approach the cow, which also moves at each time step (because you move the same parameters)\nThis leads to a bizzare path of chasing (a significant oscillating in training)\nInstead, what we see in the psuedo-code is that we:\n\nUse a separate network with fixed parameters for estimating the TD Target\nCopy the parameters from our Deep Q-Network every C steps to update the target network\n\n\n\n\n\n\n\n\n\nDouble DQN‚Äôs, or Double Q-Learning neural networks, were introduced by Hado van Hasselt. This method handles the problem of the overestimation of Q-values\nTo understand this problem, remember how we can calculate the TD Target: \nWe face a simple problem by calculating the TD target: How are we sure that the best action for the next state is the action with the highest Q-value?\nWe know that the accuracy of Q-values depend on what action we tried and what neighboring states we explored\nConsequently, we don‚Äôt have enough information about the best action to take at the beginning of the training. Therefore, taking the maximum Q-value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly given a higher Q value than the optimal best action, the learning will be complicated\nThe solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q-value generation. We:\n\nUse our DQN network to select the best action to take for the next state (the action with the highest Q-value)\nUse our Target network to calculate the target Q-value of taking that action at the next state\n\nTherefore, Double DQN helps us reduce the overestimation of Q-values and, as a consequence, helps us train faster and with stable learning.\nSince these 3 improvements in Deep Q-Learning, many more have been added, such as Prioritized Experience Replay and Dueling Deep Q-Learning.\n\n\n\n\n\nTabular Method: Type of problem in which the state and action spaces are small enough to approximate value functions to be represented as arrays and tables. Q-learning is an example of tabular method since a table is used to represent the value for different state-action pairs.\nDeep Q-Learning: Method that trains a neural network to approximate, given a state, the different Q-values for each possible action at that state. It is used to solve problems when observational space is too big to apply a tabular Q-Learning approach.\nTemporal Limitation is a difficulty presented when the environment state is represented by frames. A frame by itself does not provide temporal information. In order to obtain temporal information, we need to stack a number of frames together.\nPhases of Deep Q-Learning:\n\nSampling: Actions are performed, and observed experience tuples are stored in a replay memory.\nTraining: Batches of tuples are selected randomly and the neural network updates its weights using gradient descent.\n\nSolutions to stabilize Deep Q-Learning:\n\nExperience Replay: A replay memory is created to save experiences samples that can be reused during training. This allows the agent to learn from the same experiences multiple times. Also, it helps the agent avoid forgetting previous experiences as it gets new ones.\nRandom sampling from replay buffer allows to remove correlation in the observation sequences and prevents action values from oscillating or diverging catastrophically.\nFixed Q-Target: In order to calculate the Q-Target we need to estimate the discounted optimal Q-value of the next state by using Bellman equation. The problem is that the same network weights are used to calculate the Q-Target and the Q-value. This means that everytime we are modifying the Q-value, the Q-Target also moves with it. To avoid this issue, a separate network with fixed parameters is used for estimating the Temporal Difference Target. The target network is updated by copying parameters from our Deep Q-Network after certain C steps.\nDouble DQN: Method to handle overestimation of Q-Values. This solution uses two networks to decouple the action selection from the target Value generation:\n\nDQN Network to select the best action to take for the next state (the action with the highest Q-Value)\nTarget Network to calculate the target Q-Value of taking that action at the next state. This approach reduces the Q-Values overestimation, it helps to train faster and have more stable learning.\n\n\n\n\n\nCode\na = {1:'A',2:'B',3:'C'}\na.update({1:'D'})\n\n\n\n\nCode\na\n\n\n{1: 'D', 2: 'B', 3: 'C'}"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html",
    "href": "docs/projects/Youtube_API_EDA.html",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "With millions of users and billions of views, YouTube has become a major platform for spirituality content creators to share their knowledge and insights with a global audience. However, understanding what makes a video successful on YouTube can be a challenge, as the platform‚Äôs algorithm is complex and constantly evolving. Aspiring spirituality content creators can benefit from analyzing successful channels in their niche and identifying trends in their topics and presentation styles. In this project, we will explore the statistics of 9 popular spirituality channels on YouTube to gain insights on their audience, content, and engagement metrics.\n\n\n\nWithin this project, I would like to explore the following:\n\nGetting to know Youtube API and how to obtain video data.\nAnalyzing video data and verify different common ‚Äúmyths‚Äù about what makes a video do well on Youtube, for example:\n\nDoes the number of likes and comments matter for a video to get more views?\nDoes the video duration matter for views and interaction (likes/ comments)?\nDoes title length matter for views?\nHow many tags do good performing videos have? What are the common tags among these videos?\nAcross all the creators I take into consideration, how often do they upload new videos? On which days in the week?\n\nExplore the trending topics using NLP techniques\n\nWhich popular topics are being covered in the videos (e.g.¬†using wordcloud for video titles)?\n\nWhich questions are being asked in the comment sections in the videos\n\n\n\n\n\nObtain video meta data via the API app, from top 10 youtube niche channels.\nPreprocess data and engineer aditional features for analysis\nExploratory data analysis\nConclusions\n\n\n\n\n\nCreated my own dataset usign the Google API version 3.0\nThe channels are included as per my liking and self-thoughts about spirituality.\nAlso I have chosen channels based on their subscriber counts.\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport os\nimport time\nimport numpy as np\nfrom dateutil import parser\nimport isodate\nimport datetime\n\n# Data visualization libraries\nimport matplotlib\n#matplotlib.use('TkAgg') #default backend 'module://matplotlib_inline.backend_inline'\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nsns.set(style = 'darkgrid', color_codes=True)\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150}) #fig = plt.figure(dpi=200,figsize = (16,20)) similar\n#matplotlib.use(\"TkAgg\")\n%matplotlib inline\n# #plt.rcParams['font.family'] = 'Lohit-Devanagari'\n# #plt.rcParams[\"font.path\"] = \"/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf\"\n# english_font = fm.FontProperties(family = 'Arial', size = 14)\n# #mangal_font = fm.FontProperties(fname = \"~/downloads/fonts/mangal.ttf\",size = 14)\n# #%matplotlib inline\n\n#NLP Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom wordcloud import WordCloud\n# To install wordcloud use: python -m pip install -e git+https://github.com/amueller/word_cloud#egg=wordcloud\n# Google API\nfrom googleapiclient.discovery import build\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/yuvi_dh/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/yuvi_dh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n\n\n\nCreated a project on Google Developer Console\nRequested an Authorization Credential API Key\nEnabled Youtube API for the project work to send API requests to Youtube API services.\nGot the channel ID‚Äôs from my favorite channels which I would like to get stats on.\nFinally created the functions for getting the channel stats.\n\n\n\nCode\napi_key_1 = os.environ.get('yt_1')\n#api_key_2 = os.environ.get('yt_2')\n#api_key_3 = os.environ.get('yt_3')\n#api_key = api_key_1\n#print(api_key)\n\n\n\n\nCode\nimport os\napi_key_1 = os.environ.get('yt_1')\napi_key_2 = os.environ.get('yt_2')\napi_key_3 = os.environ.get('yt_3')\napi_key = api_key_1\n#print(api_key)\n\n# channel_ids = ['UCCy2rLnGdwoJcSAtixNdsPQ', # The Sanskrit Channel\n#               'UCzszIh4jH06kYp7k_DxhH5A',  # Chinmaya Channel\n#               'UCtDAJiFT4sy42oNPA8zo0sw',  # Star Bharat\n#               'UCdsQsaeI8pQZtgrMmbjGUug',  # Swaminarayan Aksharpith\n#               'UCqFg6QnwgtVHo1iFgpxrx-A',  # Mayapur TV\n#               'UCutvkeF3tVgItCX31QhJ2Dw',  # Nova Spiritual India\n#               'UCypj9Vvizo4cCERfDFIG3zw',  # Shemaroo Bhakti Darshan\n#               'UCxoQaZS8YdKkyfBwGZay-Xg',  # Hyper Quest\n#               'UC8HRYUBXTHv4mJ67Y5FitSg']  # Rajshri Soul\nchannel_ids = [\n    \"UCgeicB5AuF3MyyUto0-M5Lw\",  # Actualized.org\n    \"UCOnnmKlDZltHAqJLz-XIpGA\",  # Universe Inside You\n    \"UC48MclMZIY_EaOQwatzCpvw\",  # Aaron Doughty\n    \"UCg3F5jxUrSvJQICen48cX4w\",  # Mindvalley\n    \"UCEcMWs6GudljuLw0-Umf97A\",  # Spirit Science\n    \"UCFVqzO9_qHVckKqNC95o9tw\",  # Gaia    \n    \"UC7IcJI8PUf5Z3zKxnZvTBog\",  # The School of Life\n    \"UCz22l7kbce-uFJAoaZqxD1A\",  # Gaur Gopal Das\n    \"UCFJZQtrh5Ksncayy2FaoNbQ\",  # Vishuddha Das\n    \"UCkJEpR7JmS36tajD34Gp4VA\",  # Psych2Go\n]\nyoutube = build('youtube', 'v3',developerKey=api_key)\n\n\n\n\n\n\nCode\ndef get_channel_stats(channel_ids,yt=youtube):\n    '''\n    Get Channel statistics: title subscriber count, view count, video count, upload playlist\n    \n    Params:\n    youtube: the build object from googleapiclient.discovery\n    channel_ids: list of channel IDs\n    \n    Returns:\n    Dataframe containing the channel statistics for all channels in the provided list\n    \n    '''\n    all_data = []\n    request = youtube.channels().list(\n        part = 'snippet,contentDetails,statistics,brandingSettings',\n        id=','.join(channel_ids))\n    response = request.execute()\n    \n    for i in range(len(response['items'])):\n        data = dict(channelName = response['items'][i]['snippet']['title'],\n                    #countryName = response['items'][i]['snippet'][\"country\"],\n                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n                    views = response['items'][i]['statistics']['viewCount'],\n                    totalVideos = response['items'][i]['statistics']['videoCount'],\n                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'],\n                   publishedAt = isodate.parse_datetime(response['items'][i]['snippet']['publishedAt']))\n                    \n        all_data.append(data)\n    return pd.DataFrame(all_data)\n\n\n\n\nCode\ndef get_video_ids(playlist_id, max_results=1500,yt = youtube):\n    \"\"\"\n    Get list of video IDs of all videos in the given playlist, up to a maximum of 1500 videos\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    playlist_id: playlist ID of the channel\n    max_results: maximum number of videos to retrieve (default: 1500)\n    \n    Returns:\n    List of video IDs of all videos in the playlist, up to the maximum number of videos specified\n    \n    \"\"\"\n    \n    request = youtube.playlistItems().list(\n                part='contentDetails',\n                playlistId = playlist_id,\n                maxResults = min(max_results, 50))\n    response = request.execute()\n    \n    video_ids = []\n    num_videos = 0\n    \n    for i in range(len(response['items'])):\n        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n        num_videos += 1\n        if num_videos &gt;= max_results:\n            break\n        \n    next_page_token = response.get('nextPageToken')\n    more_pages = True\n    \n    while more_pages and num_videos &lt; max_results:\n        if next_page_token is None:\n            more_pages = False\n        else:\n            request = youtube.playlistItems().list(\n                        part='contentDetails',\n                        playlistId = playlist_id,\n                        maxResults = min(max_results - num_videos, 50),\n                        pageToken = next_page_token)\n            response = request.execute()\n    \n            for i in range(len(response['items'])):\n                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n                num_videos += 1\n                if num_videos &gt;= max_results:\n                    break\n            \n            next_page_token = response.get('nextPageToken')\n        \n    return video_ids\n\n\n\n\nCode\ndef get_video_details(video_ids,yt = youtube):\n    \"\"\"\n    Get video statistics of all videos with given IDs\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with statistics of videos, i.e.:\n        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n        'duration', 'definition', 'caption'\n    \"\"\"\n        \n    all_video_info = []\n    \n    for i in range(0, len(video_ids), 50):\n        request = youtube.videos().list(\n            part=\"snippet,contentDetails,statistics\",\n            id=','.join(video_ids[i:i+50])\n        )\n        response = request.execute() \n\n        for video in response['items']:\n            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n                             'contentDetails': ['duration', 'definition', 'caption']\n                            }\n            video_info = {}\n            video_info['video_id'] = video['id']\n\n            for k in stats_to_keep.keys():\n                for v in stats_to_keep[k]:\n                    try:\n                        video_info[v] = video[k][v]\n                    except:\n                        video_info[v] = None\n\n            all_video_info.append(video_info)\n            \n    return pd.DataFrame(all_video_info)\n\n\n\n\nCode\ndef get_comments_in_videos(video_ids, yt = youtube):\n    \"\"\"\n    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with video IDs and associated top level comment in text.\n    \n    \"\"\"\n    all_comments = []\n    \n    for video_id in video_ids:\n        try:   \n            request = youtube.commentThreads().list(\n                part=\"snippet,replies\",\n                videoId=video_id\n            )\n            response = request.execute()\n        \n            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n\n            all_comments.append(comments_in_video_info)\n            \n        except: \n            # When error occurs - most likely because comments are disabled on a video\n            print('Could not get comments for video ' + video_id)\n        \n    return pd.DataFrame(all_comments) \n\n\n\n\n\nUsing the get_channel_stats function defined below, now we are going to obtain the channel statistics for the above channels in scope\n\n\nCode\nchannel_data = get_channel_stats(channel_ids)\nchannel_data.to_csv(\"./files/spiritual_channel_data_original.csv\")\n\n\n\n\nCode\n#channel_data\n\n\n\n\nCode\n# Creation of copy so that I save a dummy df and also a csv, to not keep reusing youtube credits i.e. 10k per day.\nl_channel_data = pd.read_csv(\"./files/spiritual_channel_data_original.csv\",index_col=0)\nl_channel_data\n\n\n\n\n\n\n\n\n\nchannelName\nsubscribers\nviews\ntotalVideos\nplaylistId\npublishedAt\n\n\n\n\n0\nAaron Doughty\n1450000\n141121587\n1793\nUU48MclMZIY_EaOQwatzCpvw\n2014-07-10 04:24:58+00:00\n\n\n1\nThe School of Life\n8380000\n820210489\n902\nUU7IcJI8PUf5Z3zKxnZvTBog\n2010-05-18 16:46:57+00:00\n\n\n2\nPsych2Go\n10800000\n1477376571\n2380\nUUkJEpR7JmS36tajD34Gp4VA\n2014-10-05 06:27:31+00:00\n\n\n3\nGaia\n1520000\n127649328\n645\nUUFVqzO9_qHVckKqNC95o9tw\n2008-08-06 15:26:41+00:00\n\n\n4\nUniverse Inside You\n1790000\n118060252\n126\nUUOnnmKlDZltHAqJLz-XIpGA\n2017-03-30 10:55:39+00:00\n\n\n5\nMindvalley\n2260000\n468381830\n1662\nUUg3F5jxUrSvJQICen48cX4w\n2014-04-23 08:21:13+00:00\n\n\n6\nGaur Gopal Das\n4860000\n288878180\n356\nUUz22l7kbce-uFJAoaZqxD1A\n2016-04-12 18:16:24+00:00\n\n\n7\nVishuddha Das\n596000\n40838427\n381\nUUFJZQtrh5Ksncayy2FaoNbQ\n2014-09-24 14:04:59+00:00\n\n\n8\nActualized.org\n1100000\n107549192\n528\nUUgeicB5AuF3MyyUto0-M5Lw\n2012-01-23 20:08:34+00:00\n\n\n9\nSpirit Science\n1300000\n79311991\n306\nUUEcMWs6GudljuLw0-Umf97A\n2011-12-29 05:49:29+00:00\n\n\n\n\n\n\n\n\n\nCode\n# Copy used for further manipulation and original of l_channel_data can be used to load this chdd any time.\nchdd = l_channel_data.copy()\nchdd\n\n\n\n\n\n\n\n\n\nchannelName\nsubscribers\nviews\ntotalVideos\nplaylistId\npublishedAt\n\n\n\n\n0\nAaron Doughty\n1450000\n141121587\n1793\nUU48MclMZIY_EaOQwatzCpvw\n2014-07-10 04:24:58+00:00\n\n\n1\nThe School of Life\n8380000\n820210489\n902\nUU7IcJI8PUf5Z3zKxnZvTBog\n2010-05-18 16:46:57+00:00\n\n\n2\nPsych2Go\n10800000\n1477376571\n2380\nUUkJEpR7JmS36tajD34Gp4VA\n2014-10-05 06:27:31+00:00\n\n\n3\nGaia\n1520000\n127649328\n645\nUUFVqzO9_qHVckKqNC95o9tw\n2008-08-06 15:26:41+00:00\n\n\n4\nUniverse Inside You\n1790000\n118060252\n126\nUUOnnmKlDZltHAqJLz-XIpGA\n2017-03-30 10:55:39+00:00\n\n\n5\nMindvalley\n2260000\n468381830\n1662\nUUg3F5jxUrSvJQICen48cX4w\n2014-04-23 08:21:13+00:00\n\n\n6\nGaur Gopal Das\n4860000\n288878180\n356\nUUz22l7kbce-uFJAoaZqxD1A\n2016-04-12 18:16:24+00:00\n\n\n7\nVishuddha Das\n596000\n40838427\n381\nUUFJZQtrh5Ksncayy2FaoNbQ\n2014-09-24 14:04:59+00:00\n\n\n8\nActualized.org\n1100000\n107549192\n528\nUUgeicB5AuF3MyyUto0-M5Lw\n2012-01-23 20:08:34+00:00\n\n\n9\nSpirit Science\n1300000\n79311991\n306\nUUEcMWs6GudljuLw0-Umf97A\n2011-12-29 05:49:29+00:00\n\n\n\n\n\n\n\n\n\nCode\n# Setting Numeric n Categorical columns\nnumeric_cols = ['subscribers','views','totalVideos']\nchdd[numeric_cols] = chdd[numeric_cols].apply(pd.to_numeric,errors = 'coerce')\n\n# Convert publishedAt column to datetime\nchdd['publishedAt'] =(pd.to_datetime(chdd['publishedAt']))\n\n# Extract year, month, and time into separate columns\nchdd['publishingYear'] = chdd['publishedAt'].dt.year\nchdd['publishingMonth'] = chdd['publishedAt'].dt.month\nchdd['publishingTime'] = chdd['publishedAt'].dt.time\n\n# Get month name\nchdd['publishingMonthName'] = chdd['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nchdd.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\n# chdd['channelName'] = chdd['channelName'].replace('STAR ‡§≠‡§æ‡§∞‡§§','Star Bharat')\n# chdd \n# Was for other spiritual channels, but isn't necessary now.\n\n\n\n\nCode\nchdd.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 10 entries, 0 to 9\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   channelName          10 non-null     object\n 1   subscribers          10 non-null     int64 \n 2   views                10 non-null     int64 \n 3   totalVideos          10 non-null     int64 \n 4   playlistId           10 non-null     object\n 5   publishingYear       10 non-null     int64 \n 6   publishingMonth      10 non-null     int64 \n 7   publishingTime       10 non-null     object\n 8   publishingMonthName  10 non-null     object\ndtypes: int64(5), object(4)\nmemory usage: 800.0+ bytes\n\n\n\n\n\n\n\nCode\nmatplotlib.get_backend()\n\n\n'module://matplotlib_inline.backend_inline'\n\n\n\n\nCode\n#matplotlib.use??\n#sns.barplot??\n\n\n\n\nCode\n# Fixing colors for each channel\n#palette = sns.color_palette('pastel6', n_colors=10)\ncolors = plt.cm.tab10.colors[:10]\n#colors = sns.color_palette('Set1', 10)\nchannel_colors = {}\nchdd.sort_values('subscribers',ascending=False,inplace=True)\nfor i, channel in enumerate(chdd['channelName']):\n    channel_colors[channel] = colors[i]\n\n\n\n\nCode\nax = sns.barplot(x='channelName', y='subscribers', data=chdd.sort_values('subscribers', ascending=False), palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n#%matplotlib inline\nax = sns.barplot(x='channelName', y='views', data=chdd.sort_values('views', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, some channels have more subscribers but less views and vice versa. For example, GGD channel has significantly more subscribers than Mind Valley channel, but less views in total.\nPsych2Go and The School of Life hold onto their ranks in both views and subscriber count\n\n\n\n\n\nIn the next step, we will obtain the video statistics for all the channels. In total, we obtained 8700 videos as seen in below.\n\n\nCode\n# # Create a dataframe with video statistics and comments from all channels\n# video_df = pd.DataFrame()\n# comments_df = pd.DataFrame()\n\n# for c in channel_data['channelName'].unique():\n#     print(\"Getting video information from channel: \" + c)\n#     playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n#     video_ids = get_video_ids(playlist_id,max_results=2000,yt = youtube)\n    \n#     # get video data\n#     video_data = get_video_details(video_ids,yt = youtube)\n#     # get comment data\n#     comments_data = get_comments_in_videos(video_ids,yt = youtube)\n\n#     # append video data together and comment data toghether\n#     video_df = video_df.append(video_data, ignore_index=True)\n#     comments_df = comments_df.append(comments_data, ignore_index=True)\n\n\n\n\nCode\n# video_df.to_csv(\"./files/spirituality_video_df_original.csv\")\n# comments_df.to_csv(\"./files/spirituality_comments_df_original.csv\")\n\n\n\n\nCode\nl_video_df = pd.read_csv(\"./files/spirituality_video_df_original.csv\",index_col=0)\nl_comments_df = pd.read_csv(\"./files/spirituality_comments_df_original.csv\",index_col=0)\n\n\n\n\nCode\nviddf = l_video_df.copy()\ncomdf = l_comments_df.copy()\n\n\n\n\nCode\n# Create publish day (in the week) column\nviddf['publishedAt'] =  viddf['publishedAt'].apply(lambda x: parser.parse(x)) \nviddf['pushblishDayName'] = viddf['publishedAt'].apply(lambda x: x.strftime(\"%A\"))\n\n# Convert publishedAt column to datetime\nviddf['publishedAt'] =(pd.to_datetime(viddf['publishedAt']))\n\n\n# Extract year, month, and time into separate columns\nviddf['publishingYear'] = viddf['publishedAt'].dt.year\nviddf['publishingMonth'] = viddf['publishedAt'].dt.month\nviddf['publishingTime'] = viddf['publishedAt'].dt.time\n\n# Get month name\nviddf['publishingMonthName'] = viddf['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nviddf.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\nviddf\n\n\n\n\n\n\n\n\n\nvideo_id\nchannelTitle\ntitle\ndescription\ntags\nviewCount\nlikeCount\nfavouriteCount\ncommentCount\nduration\ndefinition\ncaption\npushblishDayName\npublishingYear\npublishingMonth\npublishingTime\npublishingMonthName\n\n\n\n\n0\nQ8dOR0bN-Mw\nGaia\nMan Able to Project Thoughts Through Crystals\nLearn how crystals can hold a powerful place i...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n7218.0\n1309.0\nNaN\n31.0\nPT1M\nhd\nFalse\nSaturday\n2023\n4\n15:00:28\nApril\n\n\n1\nJa1m4mHjZJY\nGaia\nFULL EPISODE: Channeling - A Bridge to the Beyond\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n18104.0\n1394.0\nNaN\n113.0\nPT26M53S\nhd\nFalse\nFriday\n2023\n3\n16:00:07\nMarch\n\n\n2\nqixrU_pwvD0\nGaia\nThis Man Taught Princess Diana to Express Hers...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n9462.0\n640.0\nNaN\n51.0\nPT4M47S\nhd\nFalse\nWednesday\n2023\n3\n16:00:40\nMarch\n\n\n3\ndeZsy9GYn8w\nGaia\nMysterious, Ancient Satellite Is Monitoring Earth\nFive of the world‚Äôs leading experts unravel my...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n73915.0\n10072.0\nNaN\n338.0\nPT1M\nhd\nFalse\nTuesday\n2023\n3\n15:00:21\nMarch\n\n\n4\nKbfpd8zp3mk\nGaia\nHow Shamanic Dancing Leads to Altered States o...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n14585.0\n932.0\nNaN\n54.0\nPT3M56S\nhd\nFalse\nMonday\n2023\n3\n16:00:21\nMarch\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8695\nZS3sOfs5jBY\nActualized.org\nGet Coached\nhttp://www.actualized.org/coaching\\n\\nResults ...\n['life coaching', 'Coaching (Profession)']\n10102.0\n323.0\nNaN\n31.0\nPT16M23S\nhd\nFalse\nSaturday\n2013\n5\n11:48:19\nMay\n\n\n8696\nmlIYVsuIofs\nActualized.org\nBe Different to Be Successful\nHow doing things differently in your life is n...\nNaN\n27343.0\n929.0\nNaN\n41.0\nPT26M5S\nhd\nFalse\nFriday\n2013\n4\n10:24:43\nApril\n\n\n8697\n8cbtMhHpLC8\nActualized.org\nWhy Life Coaching Works\nAn explanation of how coaching works and why i...\nNaN\n20081.0\n647.0\nNaN\n42.0\nPT21M\nhd\nFalse\nFriday\n2013\n4\n08:53:35\nApril\n\n\n8698\nC1QYF5WYzCo\nActualized.org\nHow to Invest In Yourself\nHow a long-term investment mindset in yourself...\nNaN\n61685.0\n1805.0\nNaN\n193.0\nPT19M13S\nhd\nTrue\nFriday\n2013\n4\n07:29:27\nApril\n\n\n8699\n_874QVgwvEk\nActualized.org\nMastery Part 1\nTest video for self-development blog. This vid...\n['mastery', 'self-help', 'self-development', '...\n5496.0\n217.0\nNaN\n63.0\nPT16M12S\nhd\nFalse\nWednesday\n2012\n8\n08:24:31\nAugust\n\n\n\n\n8700 rows √ó 17 columns\n\n\n\nLet‚Äôs take a look at the comment_df as well. We only get 8674 comments in total due to the fact that we limited to 10 first comments on the video to avoid exceeding the Youtube API quota limit.\n\n\nCode\ncomdf\n\n\n\n\n\n\n\n\n\nvideo_id\ncomments\n\n\n\n\n0\nQ8dOR0bN-Mw\n['Intentionally concentrating the mind setting...\n\n\n1\nJa1m4mHjZJY\n['8,000+ Films, Shows & Classes on Gaia. Start...\n\n\n2\nqixrU_pwvD0\n['8,000+ Films, Shows & Classes on Gaia. Start...\n\n\n3\ndeZsy9GYn8w\n['Watch more of the Awakening Conference with ...\n\n\n4\nKbfpd8zp3mk\n['8,000+ Films, Shows & Classes on Gaia. Start...\n\n\n...\n...\n...\n\n\n8669\nZS3sOfs5jBY\n['Thanks', 'A legend is born..', 'Currently go...\n\n\n8670\nmlIYVsuIofs\n['Great video üí´', 'Inspiring', 'Thanks', 'Than...\n\n\n8671\n8cbtMhHpLC8\n['Thanks.', '0:00 a legend was born', 'Magic.'...\n\n\n8672\nC1QYF5WYzCo\n['Thanks', \"If you discovered this/him you've ...\n\n\n8673\n_874QVgwvEk\n['Proud of you man. You helped me a lot', 'Who...\n\n\n\n\n8674 rows √ó 2 columns\n\n\n\n\n\n\n\nTo be able to make use of the data for analysis, we need to perform a few pre-processing steps. Firstly, I would like reformat some columns, especially the date and time columns such as ‚ÄúpushlishedAt‚Äù and ‚Äúduration‚Äù. In addition, I also think it is necessary to enrich the data with some new features that might be useful for understanding the videos‚Äô characteristics. Also I removed the favorite count column as it‚Äôs completely blank, rest other columns having null values weren‚Äôt modified for simplicity.\n\n\n\n\nCode\nviddf.isnull().sum(axis = 0)\n\n\nvideo_id                  0\nchannelTitle              0\ntitle                     0\ndescription               3\ntags                    554\nviewCount                 5\nlikeCount                 6\nfavouriteCount         8700\ncommentCount              8\nduration                  0\ndefinition                0\ncaption                   0\npushblishDayName          0\npublishingYear            0\npublishingMonth           0\npublishingTime            0\npublishingMonthName       0\ndtype: int64\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nax = sns.heatmap(viddf.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nviddf.publishingYear.sort_values().value_counts()\n# Videos are from 2011 to 2023\n#- During the covid time the channels became more active it seems over all.\n\n\n2022    1667\n2021    1355\n2020    1236\n2018    1027\n2019     950\n2017     863\n2016     516\n2023     392\n2015     355\n2014     248\n2013      45\n2012      44\n2011       2\nName: publishingYear, dtype: int64\n\n\n\n\n\n\n\nCode\ncols = ['viewCount', 'likeCount','commentCount']\nviddf[cols] = viddf[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n\n\n\n\nI want to enrich the data for further analyses, for example: - convert video duration to seconds instead of the current default string format - calculate number of tags for each video - calculate comments and likes per 1000 view ratio - calculate title character length\n\n\nCode\n# convert duration to seconds\nviddf['durationSecs'] = viddf['duration'].apply(lambda x: isodate.parse_duration(x))\nviddf['durationSecs'] = viddf['durationSecs'].astype('timedelta64[s]')\n\n\n\n\nCode\n# Add number of tags\nviddf['tagsstr'] = viddf.tags.apply(lambda x: 0 if x is None else str((x))) #tags were not in proper format so converting them to str\nviddf['tagsCount'] = viddf.tagsstr.apply(lambda x: 0 if (x == 0 or x =='nan') else len(eval(x)))\n\n\n\n\nCode\n# Comments and likes per 1000 view ratio\nviddf['likeRatio'] = viddf['likeCount']/ viddf['viewCount'] * 1000\nviddf['commentRatio'] = viddf['commentCount']/ viddf['viewCount'] * 1000\n\n\n\n\nCode\n# Title character length\nviddf['titleLength'] = viddf['title'].apply(lambda x: len(x))\n\n\n\n\nCode\n# Dropping the favourite Count as all of it is empty\nviddf.drop(['favouriteCount'],axis = 1, inplace=True)\n\n\n\n\nCode\n#Observing df before proceeding further\nviddf \n\n\n\n\n\n\n\n\n\nvideo_id\nchannelTitle\ntitle\ndescription\ntags\nviewCount\nlikeCount\ncommentCount\nduration\ndefinition\n...\npublishingYear\npublishingMonth\npublishingTime\npublishingMonthName\ndurationSecs\ntagsstr\ntagsCount\nlikeRatio\ncommentRatio\ntitleLength\n\n\n\n\n0\nQ8dOR0bN-Mw\nGaia\nMan Able to Project Thoughts Through Crystals\nLearn how crystals can hold a powerful place i...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n7218.0\n1309.0\n31.0\nPT1M\nhd\n...\n2023\n4\n15:00:28\nApril\n60.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n181.352175\n4.294819\n45\n\n\n1\nJa1m4mHjZJY\nGaia\nFULL EPISODE: Channeling - A Bridge to the Beyond\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n18104.0\n1394.0\n113.0\nPT26M53S\nhd\n...\n2023\n3\n16:00:07\nMarch\n1613.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n76.999558\n6.241715\n49\n\n\n2\nqixrU_pwvD0\nGaia\nThis Man Taught Princess Diana to Express Hers...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n9462.0\n640.0\n51.0\nPT4M47S\nhd\n...\n2023\n3\n16:00:40\nMarch\n287.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n67.638977\n5.389981\n63\n\n\n3\ndeZsy9GYn8w\nGaia\nMysterious, Ancient Satellite Is Monitoring Earth\nFive of the world‚Äôs leading experts unravel my...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n73915.0\n10072.0\n338.0\nPT1M\nhd\n...\n2023\n3\n15:00:21\nMarch\n60.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n136.264628\n4.572820\n49\n\n\n4\nKbfpd8zp3mk\nGaia\nHow Shamanic Dancing Leads to Altered States o...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n14585.0\n932.0\n54.0\nPT3M56S\nhd\n...\n2023\n3\n16:00:21\nMarch\n236.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n63.901268\n3.702434\n61\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8695\nZS3sOfs5jBY\nActualized.org\nGet Coached\nhttp://www.actualized.org/coaching\\n\\nResults ...\n['life coaching', 'Coaching (Profession)']\n10102.0\n323.0\n31.0\nPT16M23S\nhd\n...\n2013\n5\n11:48:19\nMay\n983.0\n['life coaching', 'Coaching (Profession)']\n2\n31.973867\n3.068699\n11\n\n\n8696\nmlIYVsuIofs\nActualized.org\nBe Different to Be Successful\nHow doing things differently in your life is n...\nNaN\n27343.0\n929.0\n41.0\nPT26M5S\nhd\n...\n2013\n4\n10:24:43\nApril\n1565.0\nnan\n0\n33.975789\n1.499470\n29\n\n\n8697\n8cbtMhHpLC8\nActualized.org\nWhy Life Coaching Works\nAn explanation of how coaching works and why i...\nNaN\n20081.0\n647.0\n42.0\nPT21M\nhd\n...\n2013\n4\n08:53:35\nApril\n1260.0\nnan\n0\n32.219511\n2.091529\n23\n\n\n8698\nC1QYF5WYzCo\nActualized.org\nHow to Invest In Yourself\nHow a long-term investment mindset in yourself...\nNaN\n61685.0\n1805.0\n193.0\nPT19M13S\nhd\n...\n2013\n4\n07:29:27\nApril\n1153.0\nnan\n0\n29.261571\n3.128800\n25\n\n\n8699\n_874QVgwvEk\nActualized.org\nMastery Part 1\nTest video for self-development blog. This vid...\n['mastery', 'self-help', 'self-development', '...\n5496.0\n217.0\n63.0\nPT16M12S\nhd\n...\n2012\n8\n08:24:31\nAugust\n972.0\n['mastery', 'self-help', 'self-development', '...\n5\n39.483261\n11.462882\n14\n\n\n\n\n8700 rows √ó 22 columns\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.set(rc={'figure.figsize':(10,12),'figure.dpi':100})\nax = sns.barplot(x='channelName', y='totalVideos', data=chdd.sort_values('totalVideos', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# group by channel and year, count the videos\ncount_data = viddf.groupby(['channelTitle', 'publishingYear'])['video_id'].count().reset_index(name=\"count\")\n\n# plot using seaborn\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150,})\nax = sns.barplot(data=count_data, x='publishingYear', y='count', hue='channelTitle', palette=channel_colors)\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., ncol=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWith the video statistics for all channel, now we can see how the views are distributed per channel. - Boxplot provides us with the following insights: - Mindvalley and Psychgo have too many videos that went viral over time. - The School of life and Universe inside you seem to have more broad range of video views over time as their IQR seems more via the box length. - I think the views and subscriber count are correlated since the channel having more subscribers tend to have more views.\n\nViolinplot confirms us that few channels seem to have quite variation among views of the videos, like Aaron Daughty and Vishuddha Das videos have recieved more varied views for the videos.\n\n\n\n\n\n\nCode\nax = sns.violinplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\n#ax.set_ylim(ymin = -1e3, ymax = 1e5)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nax = sns.boxplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\nax.set_ylim(ymin = -1e3, ymax = 2.4e6)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFirstly, I would like to check if comments and likes do correlate with how many views a video would get. In the plots below, it can be observed that the number of views and number of comments/ likes strongly correlated with each other. The number of likes seems to suggest stronger correlation than the number of comments. However, this is expected as the more people watching a video, the more likely this video will get comments and likes. To correct for this factor, we will plot these relationships again using the comments per 1000 view and likes per 1000 view ratios.\n\n\n\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentCount\", y = \"viewCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"likeCount\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\nNow we will take a look at the correlation if we look at the comment ratio and like ratio instead of the absolute number. It seems that more views is leading to more comments and more likes as well, but after a certain point I think, with views viewers, don‚Äôt write comments that much.\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentRatio\", y = \"viewCount\", ax=ax[0])\n#ax[0].set_ylim(0,9e6)\n#ax[1].set_ylim(0,9e6)\nsns.scatterplot(data = viddf, x = \"likeRatio\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in the histogram below, most videos are between 1600 to 1800 seconds, which is about 20 to 30 minutes. Here I have to limit the duration to 10,000 because of some really long videos (potentially streaming videos).\n\n\n\n\n\nCode\nax = sns.histplot(data=viddf[viddf['durationSecs'] &lt; 10000], x=\"durationSecs\", bins=30, color=\"#9368b7\")\nplt.show()\n\n\n\n\n\nNow we plot the duration against comment count and like count. It can be seen that actually shorter videos tend to get more likes and comments than very long videos.\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nfig, ax =plt.subplots(1,2)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"commentCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"likeCount\", ax=ax[1])\n#ax[0].set_ylim(0,1e5)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n#ax[1].set_ylim(0,1e5)\nax[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThere is no clear relationship between title length and views as seen the scatterplot below, but most-viewed videos tend to have average title length of 35-60 characters\n\n\nCode\nax = sns.scatterplot(data = viddf, x = \"titleLength\", y = \"viewCount\")\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_ylim(0,1e7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAs I‚Äôm interested to see what the creators are making videos about and which terms most frequently appear in their video titles, I will create a wordcloud for the most common words. We first need to remove the stopwords such as ‚Äúyou‚Äù, ‚ÄúI‚Äù, ‚Äúthe‚Äù, etc. which do note contribute a lot to the meaning of the title. It can be seen that the main words posted in title are Life, Attraction, Love, Meditation, People and Thing.\n\n\nCode\nstop_words = set(stopwords.words('english'))\nviddf['title_no_stopwords'] = viddf['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in viddf['title_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(30, 20))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n\nwordcloud = WordCloud(width = 1920, height = 780, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()\n\n\n\n\n\n\n\nCode\n# for i in sorted(wordcloud.words_.items(), key = lambda x: x[1],reverse = False):\n#     if i[0] == 'sign gaur':\n#         print(i)\n\n\n\n\n\nIt seems that most videos have between 10 and 45 tags. The relationship between number of tags and view count is not clearly seen, but too few tags or too many tags do seem to correlate with fewer views.\n\n\nCode\nplot = sns.scatterplot(data = viddf, x = \"tagsCount\", y = \"viewCount\")\nplot.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIt‚Äôs interesting to see that more videos are uploaded on Mondays, Wednesdays and Fridays. It seems the pattern is alternative in uploading the videos. This might be because of maintaining a consistency on channel, like when the user can more expect the videos, on a consistent basis.\n\n\nCode\nday_df = pd.DataFrame(viddf['pushblishDayName'].value_counts())\nweekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nday_df = day_df.reindex(weekdays)\nax = day_df.reset_index().plot.bar(x='index', y='pushblishDayName', rot=0)\nplt.xlabel(\"Days\")\nplt.ylabel(\"Counts\")\nplt.legend(labels = [\"Counts\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLike the video titles, the video comments also revolve around Love, Life, Sign, Thing, Attraction words\n\n\nCode\nstop_words = set(stopwords.words('english'))\ncomdf['comments_no_stopwords'] = comdf['comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in comdf['comments_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nwordcloud = WordCloud(width = 1980, height = 720, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#aims-objectives-steps-background",
    "href": "docs/projects/Youtube_API_EDA.html#aims-objectives-steps-background",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "With millions of users and billions of views, YouTube has become a major platform for spirituality content creators to share their knowledge and insights with a global audience. However, understanding what makes a video successful on YouTube can be a challenge, as the platform‚Äôs algorithm is complex and constantly evolving. Aspiring spirituality content creators can benefit from analyzing successful channels in their niche and identifying trends in their topics and presentation styles. In this project, we will explore the statistics of 9 popular spirituality channels on YouTube to gain insights on their audience, content, and engagement metrics.\n\n\n\nWithin this project, I would like to explore the following:\n\nGetting to know Youtube API and how to obtain video data.\nAnalyzing video data and verify different common ‚Äúmyths‚Äù about what makes a video do well on Youtube, for example:\n\nDoes the number of likes and comments matter for a video to get more views?\nDoes the video duration matter for views and interaction (likes/ comments)?\nDoes title length matter for views?\nHow many tags do good performing videos have? What are the common tags among these videos?\nAcross all the creators I take into consideration, how often do they upload new videos? On which days in the week?\n\nExplore the trending topics using NLP techniques\n\nWhich popular topics are being covered in the videos (e.g.¬†using wordcloud for video titles)?\n\nWhich questions are being asked in the comment sections in the videos\n\n\n\n\n\nObtain video meta data via the API app, from top 10 youtube niche channels.\nPreprocess data and engineer aditional features for analysis\nExploratory data analysis\nConclusions\n\n\n\n\n\nCreated my own dataset usign the Google API version 3.0\nThe channels are included as per my liking and self-thoughts about spirituality.\nAlso I have chosen channels based on their subscriber counts."
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#library-imports",
    "href": "docs/projects/Youtube_API_EDA.html#library-imports",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport os\nimport time\nimport numpy as np\nfrom dateutil import parser\nimport isodate\nimport datetime\n\n# Data visualization libraries\nimport matplotlib\n#matplotlib.use('TkAgg') #default backend 'module://matplotlib_inline.backend_inline'\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nsns.set(style = 'darkgrid', color_codes=True)\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150}) #fig = plt.figure(dpi=200,figsize = (16,20)) similar\n#matplotlib.use(\"TkAgg\")\n%matplotlib inline\n# #plt.rcParams['font.family'] = 'Lohit-Devanagari'\n# #plt.rcParams[\"font.path\"] = \"/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf\"\n# english_font = fm.FontProperties(family = 'Arial', size = 14)\n# #mangal_font = fm.FontProperties(fname = \"~/downloads/fonts/mangal.ttf\",size = 14)\n# #%matplotlib inline\n\n#NLP Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom wordcloud import WordCloud\n# To install wordcloud use: python -m pip install -e git+https://github.com/amueller/word_cloud#egg=wordcloud\n# Google API\nfrom googleapiclient.discovery import build\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/yuvi_dh/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/yuvi_dh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#data-creation-with-youtube-api",
    "href": "docs/projects/Youtube_API_EDA.html#data-creation-with-youtube-api",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "Created a project on Google Developer Console\nRequested an Authorization Credential API Key\nEnabled Youtube API for the project work to send API requests to Youtube API services.\nGot the channel ID‚Äôs from my favorite channels which I would like to get stats on.\nFinally created the functions for getting the channel stats.\n\n\n\nCode\napi_key_1 = os.environ.get('yt_1')\n#api_key_2 = os.environ.get('yt_2')\n#api_key_3 = os.environ.get('yt_3')\n#api_key = api_key_1\n#print(api_key)\n\n\n\n\nCode\nimport os\napi_key_1 = os.environ.get('yt_1')\napi_key_2 = os.environ.get('yt_2')\napi_key_3 = os.environ.get('yt_3')\napi_key = api_key_1\n#print(api_key)\n\n# channel_ids = ['UCCy2rLnGdwoJcSAtixNdsPQ', # The Sanskrit Channel\n#               'UCzszIh4jH06kYp7k_DxhH5A',  # Chinmaya Channel\n#               'UCtDAJiFT4sy42oNPA8zo0sw',  # Star Bharat\n#               'UCdsQsaeI8pQZtgrMmbjGUug',  # Swaminarayan Aksharpith\n#               'UCqFg6QnwgtVHo1iFgpxrx-A',  # Mayapur TV\n#               'UCutvkeF3tVgItCX31QhJ2Dw',  # Nova Spiritual India\n#               'UCypj9Vvizo4cCERfDFIG3zw',  # Shemaroo Bhakti Darshan\n#               'UCxoQaZS8YdKkyfBwGZay-Xg',  # Hyper Quest\n#               'UC8HRYUBXTHv4mJ67Y5FitSg']  # Rajshri Soul\nchannel_ids = [\n    \"UCgeicB5AuF3MyyUto0-M5Lw\",  # Actualized.org\n    \"UCOnnmKlDZltHAqJLz-XIpGA\",  # Universe Inside You\n    \"UC48MclMZIY_EaOQwatzCpvw\",  # Aaron Doughty\n    \"UCg3F5jxUrSvJQICen48cX4w\",  # Mindvalley\n    \"UCEcMWs6GudljuLw0-Umf97A\",  # Spirit Science\n    \"UCFVqzO9_qHVckKqNC95o9tw\",  # Gaia    \n    \"UC7IcJI8PUf5Z3zKxnZvTBog\",  # The School of Life\n    \"UCz22l7kbce-uFJAoaZqxD1A\",  # Gaur Gopal Das\n    \"UCFJZQtrh5Ksncayy2FaoNbQ\",  # Vishuddha Das\n    \"UCkJEpR7JmS36tajD34Gp4VA\",  # Psych2Go\n]\nyoutube = build('youtube', 'v3',developerKey=api_key)\n\n\n\n\n\n\nCode\ndef get_channel_stats(channel_ids,yt=youtube):\n    '''\n    Get Channel statistics: title subscriber count, view count, video count, upload playlist\n    \n    Params:\n    youtube: the build object from googleapiclient.discovery\n    channel_ids: list of channel IDs\n    \n    Returns:\n    Dataframe containing the channel statistics for all channels in the provided list\n    \n    '''\n    all_data = []\n    request = youtube.channels().list(\n        part = 'snippet,contentDetails,statistics,brandingSettings',\n        id=','.join(channel_ids))\n    response = request.execute()\n    \n    for i in range(len(response['items'])):\n        data = dict(channelName = response['items'][i]['snippet']['title'],\n                    #countryName = response['items'][i]['snippet'][\"country\"],\n                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n                    views = response['items'][i]['statistics']['viewCount'],\n                    totalVideos = response['items'][i]['statistics']['videoCount'],\n                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'],\n                   publishedAt = isodate.parse_datetime(response['items'][i]['snippet']['publishedAt']))\n                    \n        all_data.append(data)\n    return pd.DataFrame(all_data)\n\n\n\n\nCode\ndef get_video_ids(playlist_id, max_results=1500,yt = youtube):\n    \"\"\"\n    Get list of video IDs of all videos in the given playlist, up to a maximum of 1500 videos\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    playlist_id: playlist ID of the channel\n    max_results: maximum number of videos to retrieve (default: 1500)\n    \n    Returns:\n    List of video IDs of all videos in the playlist, up to the maximum number of videos specified\n    \n    \"\"\"\n    \n    request = youtube.playlistItems().list(\n                part='contentDetails',\n                playlistId = playlist_id,\n                maxResults = min(max_results, 50))\n    response = request.execute()\n    \n    video_ids = []\n    num_videos = 0\n    \n    for i in range(len(response['items'])):\n        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n        num_videos += 1\n        if num_videos &gt;= max_results:\n            break\n        \n    next_page_token = response.get('nextPageToken')\n    more_pages = True\n    \n    while more_pages and num_videos &lt; max_results:\n        if next_page_token is None:\n            more_pages = False\n        else:\n            request = youtube.playlistItems().list(\n                        part='contentDetails',\n                        playlistId = playlist_id,\n                        maxResults = min(max_results - num_videos, 50),\n                        pageToken = next_page_token)\n            response = request.execute()\n    \n            for i in range(len(response['items'])):\n                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n                num_videos += 1\n                if num_videos &gt;= max_results:\n                    break\n            \n            next_page_token = response.get('nextPageToken')\n        \n    return video_ids\n\n\n\n\nCode\ndef get_video_details(video_ids,yt = youtube):\n    \"\"\"\n    Get video statistics of all videos with given IDs\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with statistics of videos, i.e.:\n        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n        'duration', 'definition', 'caption'\n    \"\"\"\n        \n    all_video_info = []\n    \n    for i in range(0, len(video_ids), 50):\n        request = youtube.videos().list(\n            part=\"snippet,contentDetails,statistics\",\n            id=','.join(video_ids[i:i+50])\n        )\n        response = request.execute() \n\n        for video in response['items']:\n            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n                             'contentDetails': ['duration', 'definition', 'caption']\n                            }\n            video_info = {}\n            video_info['video_id'] = video['id']\n\n            for k in stats_to_keep.keys():\n                for v in stats_to_keep[k]:\n                    try:\n                        video_info[v] = video[k][v]\n                    except:\n                        video_info[v] = None\n\n            all_video_info.append(video_info)\n            \n    return pd.DataFrame(all_video_info)\n\n\n\n\nCode\ndef get_comments_in_videos(video_ids, yt = youtube):\n    \"\"\"\n    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with video IDs and associated top level comment in text.\n    \n    \"\"\"\n    all_comments = []\n    \n    for video_id in video_ids:\n        try:   \n            request = youtube.commentThreads().list(\n                part=\"snippet,replies\",\n                videoId=video_id\n            )\n            response = request.execute()\n        \n            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n\n            all_comments.append(comments_in_video_info)\n            \n        except: \n            # When error occurs - most likely because comments are disabled on a video\n            print('Could not get comments for video ' + video_id)\n        \n    return pd.DataFrame(all_comments) \n\n\n\n\n\nUsing the get_channel_stats function defined below, now we are going to obtain the channel statistics for the above channels in scope\n\n\nCode\nchannel_data = get_channel_stats(channel_ids)\nchannel_data.to_csv(\"./files/spiritual_channel_data_original.csv\")\n\n\n\n\nCode\n#channel_data\n\n\n\n\nCode\n# Creation of copy so that I save a dummy df and also a csv, to not keep reusing youtube credits i.e. 10k per day.\nl_channel_data = pd.read_csv(\"./files/spiritual_channel_data_original.csv\",index_col=0)\nl_channel_data\n\n\n\n\n\n\n\n\n\nchannelName\nsubscribers\nviews\ntotalVideos\nplaylistId\npublishedAt\n\n\n\n\n0\nAaron Doughty\n1450000\n141121587\n1793\nUU48MclMZIY_EaOQwatzCpvw\n2014-07-10 04:24:58+00:00\n\n\n1\nThe School of Life\n8380000\n820210489\n902\nUU7IcJI8PUf5Z3zKxnZvTBog\n2010-05-18 16:46:57+00:00\n\n\n2\nPsych2Go\n10800000\n1477376571\n2380\nUUkJEpR7JmS36tajD34Gp4VA\n2014-10-05 06:27:31+00:00\n\n\n3\nGaia\n1520000\n127649328\n645\nUUFVqzO9_qHVckKqNC95o9tw\n2008-08-06 15:26:41+00:00\n\n\n4\nUniverse Inside You\n1790000\n118060252\n126\nUUOnnmKlDZltHAqJLz-XIpGA\n2017-03-30 10:55:39+00:00\n\n\n5\nMindvalley\n2260000\n468381830\n1662\nUUg3F5jxUrSvJQICen48cX4w\n2014-04-23 08:21:13+00:00\n\n\n6\nGaur Gopal Das\n4860000\n288878180\n356\nUUz22l7kbce-uFJAoaZqxD1A\n2016-04-12 18:16:24+00:00\n\n\n7\nVishuddha Das\n596000\n40838427\n381\nUUFJZQtrh5Ksncayy2FaoNbQ\n2014-09-24 14:04:59+00:00\n\n\n8\nActualized.org\n1100000\n107549192\n528\nUUgeicB5AuF3MyyUto0-M5Lw\n2012-01-23 20:08:34+00:00\n\n\n9\nSpirit Science\n1300000\n79311991\n306\nUUEcMWs6GudljuLw0-Umf97A\n2011-12-29 05:49:29+00:00\n\n\n\n\n\n\n\n\n\nCode\n# Copy used for further manipulation and original of l_channel_data can be used to load this chdd any time.\nchdd = l_channel_data.copy()\nchdd\n\n\n\n\n\n\n\n\n\nchannelName\nsubscribers\nviews\ntotalVideos\nplaylistId\npublishedAt\n\n\n\n\n0\nAaron Doughty\n1450000\n141121587\n1793\nUU48MclMZIY_EaOQwatzCpvw\n2014-07-10 04:24:58+00:00\n\n\n1\nThe School of Life\n8380000\n820210489\n902\nUU7IcJI8PUf5Z3zKxnZvTBog\n2010-05-18 16:46:57+00:00\n\n\n2\nPsych2Go\n10800000\n1477376571\n2380\nUUkJEpR7JmS36tajD34Gp4VA\n2014-10-05 06:27:31+00:00\n\n\n3\nGaia\n1520000\n127649328\n645\nUUFVqzO9_qHVckKqNC95o9tw\n2008-08-06 15:26:41+00:00\n\n\n4\nUniverse Inside You\n1790000\n118060252\n126\nUUOnnmKlDZltHAqJLz-XIpGA\n2017-03-30 10:55:39+00:00\n\n\n5\nMindvalley\n2260000\n468381830\n1662\nUUg3F5jxUrSvJQICen48cX4w\n2014-04-23 08:21:13+00:00\n\n\n6\nGaur Gopal Das\n4860000\n288878180\n356\nUUz22l7kbce-uFJAoaZqxD1A\n2016-04-12 18:16:24+00:00\n\n\n7\nVishuddha Das\n596000\n40838427\n381\nUUFJZQtrh5Ksncayy2FaoNbQ\n2014-09-24 14:04:59+00:00\n\n\n8\nActualized.org\n1100000\n107549192\n528\nUUgeicB5AuF3MyyUto0-M5Lw\n2012-01-23 20:08:34+00:00\n\n\n9\nSpirit Science\n1300000\n79311991\n306\nUUEcMWs6GudljuLw0-Umf97A\n2011-12-29 05:49:29+00:00\n\n\n\n\n\n\n\n\n\nCode\n# Setting Numeric n Categorical columns\nnumeric_cols = ['subscribers','views','totalVideos']\nchdd[numeric_cols] = chdd[numeric_cols].apply(pd.to_numeric,errors = 'coerce')\n\n# Convert publishedAt column to datetime\nchdd['publishedAt'] =(pd.to_datetime(chdd['publishedAt']))\n\n# Extract year, month, and time into separate columns\nchdd['publishingYear'] = chdd['publishedAt'].dt.year\nchdd['publishingMonth'] = chdd['publishedAt'].dt.month\nchdd['publishingTime'] = chdd['publishedAt'].dt.time\n\n# Get month name\nchdd['publishingMonthName'] = chdd['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nchdd.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\n# chdd['channelName'] = chdd['channelName'].replace('STAR ‡§≠‡§æ‡§∞‡§§','Star Bharat')\n# chdd \n# Was for other spiritual channels, but isn't necessary now.\n\n\n\n\nCode\nchdd.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 10 entries, 0 to 9\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   channelName          10 non-null     object\n 1   subscribers          10 non-null     int64 \n 2   views                10 non-null     int64 \n 3   totalVideos          10 non-null     int64 \n 4   playlistId           10 non-null     object\n 5   publishingYear       10 non-null     int64 \n 6   publishingMonth      10 non-null     int64 \n 7   publishingTime       10 non-null     object\n 8   publishingMonthName  10 non-null     object\ndtypes: int64(5), object(4)\nmemory usage: 800.0+ bytes\n\n\n\n\n\n\n\nCode\nmatplotlib.get_backend()\n\n\n'module://matplotlib_inline.backend_inline'\n\n\n\n\nCode\n#matplotlib.use??\n#sns.barplot??\n\n\n\n\nCode\n# Fixing colors for each channel\n#palette = sns.color_palette('pastel6', n_colors=10)\ncolors = plt.cm.tab10.colors[:10]\n#colors = sns.color_palette('Set1', 10)\nchannel_colors = {}\nchdd.sort_values('subscribers',ascending=False,inplace=True)\nfor i, channel in enumerate(chdd['channelName']):\n    channel_colors[channel] = colors[i]\n\n\n\n\nCode\nax = sns.barplot(x='channelName', y='subscribers', data=chdd.sort_values('subscribers', ascending=False), palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n#%matplotlib inline\nax = sns.barplot(x='channelName', y='views', data=chdd.sort_values('views', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, some channels have more subscribers but less views and vice versa. For example, GGD channel has significantly more subscribers than Mind Valley channel, but less views in total.\nPsych2Go and The School of Life hold onto their ranks in both views and subscriber count\n\n\n\n\n\nIn the next step, we will obtain the video statistics for all the channels. In total, we obtained 8700 videos as seen in below.\n\n\nCode\n# # Create a dataframe with video statistics and comments from all channels\n# video_df = pd.DataFrame()\n# comments_df = pd.DataFrame()\n\n# for c in channel_data['channelName'].unique():\n#     print(\"Getting video information from channel: \" + c)\n#     playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n#     video_ids = get_video_ids(playlist_id,max_results=2000,yt = youtube)\n    \n#     # get video data\n#     video_data = get_video_details(video_ids,yt = youtube)\n#     # get comment data\n#     comments_data = get_comments_in_videos(video_ids,yt = youtube)\n\n#     # append video data together and comment data toghether\n#     video_df = video_df.append(video_data, ignore_index=True)\n#     comments_df = comments_df.append(comments_data, ignore_index=True)\n\n\n\n\nCode\n# video_df.to_csv(\"./files/spirituality_video_df_original.csv\")\n# comments_df.to_csv(\"./files/spirituality_comments_df_original.csv\")\n\n\n\n\nCode\nl_video_df = pd.read_csv(\"./files/spirituality_video_df_original.csv\",index_col=0)\nl_comments_df = pd.read_csv(\"./files/spirituality_comments_df_original.csv\",index_col=0)\n\n\n\n\nCode\nviddf = l_video_df.copy()\ncomdf = l_comments_df.copy()\n\n\n\n\nCode\n# Create publish day (in the week) column\nviddf['publishedAt'] =  viddf['publishedAt'].apply(lambda x: parser.parse(x)) \nviddf['pushblishDayName'] = viddf['publishedAt'].apply(lambda x: x.strftime(\"%A\"))\n\n# Convert publishedAt column to datetime\nviddf['publishedAt'] =(pd.to_datetime(viddf['publishedAt']))\n\n\n# Extract year, month, and time into separate columns\nviddf['publishingYear'] = viddf['publishedAt'].dt.year\nviddf['publishingMonth'] = viddf['publishedAt'].dt.month\nviddf['publishingTime'] = viddf['publishedAt'].dt.time\n\n# Get month name\nviddf['publishingMonthName'] = viddf['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nviddf.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\nviddf\n\n\n\n\n\n\n\n\n\nvideo_id\nchannelTitle\ntitle\ndescription\ntags\nviewCount\nlikeCount\nfavouriteCount\ncommentCount\nduration\ndefinition\ncaption\npushblishDayName\npublishingYear\npublishingMonth\npublishingTime\npublishingMonthName\n\n\n\n\n0\nQ8dOR0bN-Mw\nGaia\nMan Able to Project Thoughts Through Crystals\nLearn how crystals can hold a powerful place i...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n7218.0\n1309.0\nNaN\n31.0\nPT1M\nhd\nFalse\nSaturday\n2023\n4\n15:00:28\nApril\n\n\n1\nJa1m4mHjZJY\nGaia\nFULL EPISODE: Channeling - A Bridge to the Beyond\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n18104.0\n1394.0\nNaN\n113.0\nPT26M53S\nhd\nFalse\nFriday\n2023\n3\n16:00:07\nMarch\n\n\n2\nqixrU_pwvD0\nGaia\nThis Man Taught Princess Diana to Express Hers...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n9462.0\n640.0\nNaN\n51.0\nPT4M47S\nhd\nFalse\nWednesday\n2023\n3\n16:00:40\nMarch\n\n\n3\ndeZsy9GYn8w\nGaia\nMysterious, Ancient Satellite Is Monitoring Earth\nFive of the world‚Äôs leading experts unravel my...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n73915.0\n10072.0\nNaN\n338.0\nPT1M\nhd\nFalse\nTuesday\n2023\n3\n15:00:21\nMarch\n\n\n4\nKbfpd8zp3mk\nGaia\nHow Shamanic Dancing Leads to Altered States o...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n14585.0\n932.0\nNaN\n54.0\nPT3M56S\nhd\nFalse\nMonday\n2023\n3\n16:00:21\nMarch\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8695\nZS3sOfs5jBY\nActualized.org\nGet Coached\nhttp://www.actualized.org/coaching\\n\\nResults ...\n['life coaching', 'Coaching (Profession)']\n10102.0\n323.0\nNaN\n31.0\nPT16M23S\nhd\nFalse\nSaturday\n2013\n5\n11:48:19\nMay\n\n\n8696\nmlIYVsuIofs\nActualized.org\nBe Different to Be Successful\nHow doing things differently in your life is n...\nNaN\n27343.0\n929.0\nNaN\n41.0\nPT26M5S\nhd\nFalse\nFriday\n2013\n4\n10:24:43\nApril\n\n\n8697\n8cbtMhHpLC8\nActualized.org\nWhy Life Coaching Works\nAn explanation of how coaching works and why i...\nNaN\n20081.0\n647.0\nNaN\n42.0\nPT21M\nhd\nFalse\nFriday\n2013\n4\n08:53:35\nApril\n\n\n8698\nC1QYF5WYzCo\nActualized.org\nHow to Invest In Yourself\nHow a long-term investment mindset in yourself...\nNaN\n61685.0\n1805.0\nNaN\n193.0\nPT19M13S\nhd\nTrue\nFriday\n2013\n4\n07:29:27\nApril\n\n\n8699\n_874QVgwvEk\nActualized.org\nMastery Part 1\nTest video for self-development blog. This vid...\n['mastery', 'self-help', 'self-development', '...\n5496.0\n217.0\nNaN\n63.0\nPT16M12S\nhd\nFalse\nWednesday\n2012\n8\n08:24:31\nAugust\n\n\n\n\n8700 rows √ó 17 columns\n\n\n\nLet‚Äôs take a look at the comment_df as well. We only get 8674 comments in total due to the fact that we limited to 10 first comments on the video to avoid exceeding the Youtube API quota limit.\n\n\nCode\ncomdf\n\n\n\n\n\n\n\n\n\nvideo_id\ncomments\n\n\n\n\n0\nQ8dOR0bN-Mw\n['Intentionally concentrating the mind setting...\n\n\n1\nJa1m4mHjZJY\n['8,000+ Films, Shows & Classes on Gaia. Start...\n\n\n2\nqixrU_pwvD0\n['8,000+ Films, Shows & Classes on Gaia. Start...\n\n\n3\ndeZsy9GYn8w\n['Watch more of the Awakening Conference with ...\n\n\n4\nKbfpd8zp3mk\n['8,000+ Films, Shows & Classes on Gaia. Start...\n\n\n...\n...\n...\n\n\n8669\nZS3sOfs5jBY\n['Thanks', 'A legend is born..', 'Currently go...\n\n\n8670\nmlIYVsuIofs\n['Great video üí´', 'Inspiring', 'Thanks', 'Than...\n\n\n8671\n8cbtMhHpLC8\n['Thanks.', '0:00 a legend was born', 'Magic.'...\n\n\n8672\nC1QYF5WYzCo\n['Thanks', \"If you discovered this/him you've ...\n\n\n8673\n_874QVgwvEk\n['Proud of you man. You helped me a lot', 'Who...\n\n\n\n\n8674 rows √ó 2 columns"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#preprocessing-feature-engineering",
    "href": "docs/projects/Youtube_API_EDA.html#preprocessing-feature-engineering",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "To be able to make use of the data for analysis, we need to perform a few pre-processing steps. Firstly, I would like reformat some columns, especially the date and time columns such as ‚ÄúpushlishedAt‚Äù and ‚Äúduration‚Äù. In addition, I also think it is necessary to enrich the data with some new features that might be useful for understanding the videos‚Äô characteristics. Also I removed the favorite count column as it‚Äôs completely blank, rest other columns having null values weren‚Äôt modified for simplicity.\n\n\n\n\nCode\nviddf.isnull().sum(axis = 0)\n\n\nvideo_id                  0\nchannelTitle              0\ntitle                     0\ndescription               3\ntags                    554\nviewCount                 5\nlikeCount                 6\nfavouriteCount         8700\ncommentCount              8\nduration                  0\ndefinition                0\ncaption                   0\npushblishDayName          0\npublishingYear            0\npublishingMonth           0\npublishingTime            0\npublishingMonthName       0\ndtype: int64\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nax = sns.heatmap(viddf.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nviddf.publishingYear.sort_values().value_counts()\n# Videos are from 2011 to 2023\n#- During the covid time the channels became more active it seems over all.\n\n\n2022    1667\n2021    1355\n2020    1236\n2018    1027\n2019     950\n2017     863\n2016     516\n2023     392\n2015     355\n2014     248\n2013      45\n2012      44\n2011       2\nName: publishingYear, dtype: int64\n\n\n\n\n\n\n\nCode\ncols = ['viewCount', 'likeCount','commentCount']\nviddf[cols] = viddf[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n\n\n\n\nI want to enrich the data for further analyses, for example: - convert video duration to seconds instead of the current default string format - calculate number of tags for each video - calculate comments and likes per 1000 view ratio - calculate title character length\n\n\nCode\n# convert duration to seconds\nviddf['durationSecs'] = viddf['duration'].apply(lambda x: isodate.parse_duration(x))\nviddf['durationSecs'] = viddf['durationSecs'].astype('timedelta64[s]')\n\n\n\n\nCode\n# Add number of tags\nviddf['tagsstr'] = viddf.tags.apply(lambda x: 0 if x is None else str((x))) #tags were not in proper format so converting them to str\nviddf['tagsCount'] = viddf.tagsstr.apply(lambda x: 0 if (x == 0 or x =='nan') else len(eval(x)))\n\n\n\n\nCode\n# Comments and likes per 1000 view ratio\nviddf['likeRatio'] = viddf['likeCount']/ viddf['viewCount'] * 1000\nviddf['commentRatio'] = viddf['commentCount']/ viddf['viewCount'] * 1000\n\n\n\n\nCode\n# Title character length\nviddf['titleLength'] = viddf['title'].apply(lambda x: len(x))\n\n\n\n\nCode\n# Dropping the favourite Count as all of it is empty\nviddf.drop(['favouriteCount'],axis = 1, inplace=True)\n\n\n\n\nCode\n#Observing df before proceeding further\nviddf \n\n\n\n\n\n\n\n\n\nvideo_id\nchannelTitle\ntitle\ndescription\ntags\nviewCount\nlikeCount\ncommentCount\nduration\ndefinition\n...\npublishingYear\npublishingMonth\npublishingTime\npublishingMonthName\ndurationSecs\ntagsstr\ntagsCount\nlikeRatio\ncommentRatio\ntitleLength\n\n\n\n\n0\nQ8dOR0bN-Mw\nGaia\nMan Able to Project Thoughts Through Crystals\nLearn how crystals can hold a powerful place i...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n7218.0\n1309.0\n31.0\nPT1M\nhd\n...\n2023\n4\n15:00:28\nApril\n60.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n181.352175\n4.294819\n45\n\n\n1\nJa1m4mHjZJY\nGaia\nFULL EPISODE: Channeling - A Bridge to the Beyond\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n18104.0\n1394.0\n113.0\nPT26M53S\nhd\n...\n2023\n3\n16:00:07\nMarch\n1613.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n76.999558\n6.241715\n49\n\n\n2\nqixrU_pwvD0\nGaia\nThis Man Taught Princess Diana to Express Hers...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n9462.0\n640.0\n51.0\nPT4M47S\nhd\n...\n2023\n3\n16:00:40\nMarch\n287.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n67.638977\n5.389981\n63\n\n\n3\ndeZsy9GYn8w\nGaia\nMysterious, Ancient Satellite Is Monitoring Earth\nFive of the world‚Äôs leading experts unravel my...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n73915.0\n10072.0\n338.0\nPT1M\nhd\n...\n2023\n3\n15:00:21\nMarch\n60.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n136.264628\n4.572820\n49\n\n\n4\nKbfpd8zp3mk\nGaia\nHow Shamanic Dancing Leads to Altered States o...\n8,000+ Films, Shows & Classes on Gaia. Start Y...\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n14585.0\n932.0\n54.0\nPT3M56S\nhd\n...\n2023\n3\n16:00:21\nMarch\n236.0\n['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n26\n63.901268\n3.702434\n61\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8695\nZS3sOfs5jBY\nActualized.org\nGet Coached\nhttp://www.actualized.org/coaching\\n\\nResults ...\n['life coaching', 'Coaching (Profession)']\n10102.0\n323.0\n31.0\nPT16M23S\nhd\n...\n2013\n5\n11:48:19\nMay\n983.0\n['life coaching', 'Coaching (Profession)']\n2\n31.973867\n3.068699\n11\n\n\n8696\nmlIYVsuIofs\nActualized.org\nBe Different to Be Successful\nHow doing things differently in your life is n...\nNaN\n27343.0\n929.0\n41.0\nPT26M5S\nhd\n...\n2013\n4\n10:24:43\nApril\n1565.0\nnan\n0\n33.975789\n1.499470\n29\n\n\n8697\n8cbtMhHpLC8\nActualized.org\nWhy Life Coaching Works\nAn explanation of how coaching works and why i...\nNaN\n20081.0\n647.0\n42.0\nPT21M\nhd\n...\n2013\n4\n08:53:35\nApril\n1260.0\nnan\n0\n32.219511\n2.091529\n23\n\n\n8698\nC1QYF5WYzCo\nActualized.org\nHow to Invest In Yourself\nHow a long-term investment mindset in yourself...\nNaN\n61685.0\n1805.0\n193.0\nPT19M13S\nhd\n...\n2013\n4\n07:29:27\nApril\n1153.0\nnan\n0\n29.261571\n3.128800\n25\n\n\n8699\n_874QVgwvEk\nActualized.org\nMastery Part 1\nTest video for self-development blog. This vid...\n['mastery', 'self-help', 'self-development', '...\n5496.0\n217.0\n63.0\nPT16M12S\nhd\n...\n2012\n8\n08:24:31\nAugust\n972.0\n['mastery', 'self-help', 'self-development', '...\n5\n39.483261\n11.462882\n14\n\n\n\n\n8700 rows √ó 22 columns"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#exploratory-analysis",
    "href": "docs/projects/Youtube_API_EDA.html#exploratory-analysis",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "Code\nsns.set(rc={'figure.figsize':(10,12),'figure.dpi':100})\nax = sns.barplot(x='channelName', y='totalVideos', data=chdd.sort_values('totalVideos', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# group by channel and year, count the videos\ncount_data = viddf.groupby(['channelTitle', 'publishingYear'])['video_id'].count().reset_index(name=\"count\")\n\n# plot using seaborn\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150,})\nax = sns.barplot(data=count_data, x='publishingYear', y='count', hue='channelTitle', palette=channel_colors)\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., ncol=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWith the video statistics for all channel, now we can see how the views are distributed per channel. - Boxplot provides us with the following insights: - Mindvalley and Psychgo have too many videos that went viral over time. - The School of life and Universe inside you seem to have more broad range of video views over time as their IQR seems more via the box length. - I think the views and subscriber count are correlated since the channel having more subscribers tend to have more views.\n\nViolinplot confirms us that few channels seem to have quite variation among views of the videos, like Aaron Daughty and Vishuddha Das videos have recieved more varied views for the videos.\n\n\n\n\n\n\nCode\nax = sns.violinplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\n#ax.set_ylim(ymin = -1e3, ymax = 1e5)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nax = sns.boxplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\nax.set_ylim(ymin = -1e3, ymax = 2.4e6)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFirstly, I would like to check if comments and likes do correlate with how many views a video would get. In the plots below, it can be observed that the number of views and number of comments/ likes strongly correlated with each other. The number of likes seems to suggest stronger correlation than the number of comments. However, this is expected as the more people watching a video, the more likely this video will get comments and likes. To correct for this factor, we will plot these relationships again using the comments per 1000 view and likes per 1000 view ratios.\n\n\n\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentCount\", y = \"viewCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"likeCount\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\nNow we will take a look at the correlation if we look at the comment ratio and like ratio instead of the absolute number. It seems that more views is leading to more comments and more likes as well, but after a certain point I think, with views viewers, don‚Äôt write comments that much.\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentRatio\", y = \"viewCount\", ax=ax[0])\n#ax[0].set_ylim(0,9e6)\n#ax[1].set_ylim(0,9e6)\nsns.scatterplot(data = viddf, x = \"likeRatio\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in the histogram below, most videos are between 1600 to 1800 seconds, which is about 20 to 30 minutes. Here I have to limit the duration to 10,000 because of some really long videos (potentially streaming videos).\n\n\n\n\n\nCode\nax = sns.histplot(data=viddf[viddf['durationSecs'] &lt; 10000], x=\"durationSecs\", bins=30, color=\"#9368b7\")\nplt.show()\n\n\n\n\n\nNow we plot the duration against comment count and like count. It can be seen that actually shorter videos tend to get more likes and comments than very long videos.\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nfig, ax =plt.subplots(1,2)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"commentCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"likeCount\", ax=ax[1])\n#ax[0].set_ylim(0,1e5)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n#ax[1].set_ylim(0,1e5)\nax[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThere is no clear relationship between title length and views as seen the scatterplot below, but most-viewed videos tend to have average title length of 35-60 characters\n\n\nCode\nax = sns.scatterplot(data = viddf, x = \"titleLength\", y = \"viewCount\")\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_ylim(0,1e7)\nplt.show()"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#wordcloud-for-words-in-title",
    "href": "docs/projects/Youtube_API_EDA.html#wordcloud-for-words-in-title",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "As I‚Äôm interested to see what the creators are making videos about and which terms most frequently appear in their video titles, I will create a wordcloud for the most common words. We first need to remove the stopwords such as ‚Äúyou‚Äù, ‚ÄúI‚Äù, ‚Äúthe‚Äù, etc. which do note contribute a lot to the meaning of the title. It can be seen that the main words posted in title are Life, Attraction, Love, Meditation, People and Thing.\n\n\nCode\nstop_words = set(stopwords.words('english'))\nviddf['title_no_stopwords'] = viddf['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in viddf['title_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(30, 20))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n\nwordcloud = WordCloud(width = 1920, height = 780, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()\n\n\n\n\n\n\n\nCode\n# for i in sorted(wordcloud.words_.items(), key = lambda x: x[1],reverse = False):\n#     if i[0] == 'sign gaur':\n#         print(i)\n\n\n\n\n\nIt seems that most videos have between 10 and 45 tags. The relationship between number of tags and view count is not clearly seen, but too few tags or too many tags do seem to correlate with fewer views.\n\n\nCode\nplot = sns.scatterplot(data = viddf, x = \"tagsCount\", y = \"viewCount\")\nplot.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIt‚Äôs interesting to see that more videos are uploaded on Mondays, Wednesdays and Fridays. It seems the pattern is alternative in uploading the videos. This might be because of maintaining a consistency on channel, like when the user can more expect the videos, on a consistent basis.\n\n\nCode\nday_df = pd.DataFrame(viddf['pushblishDayName'].value_counts())\nweekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nday_df = day_df.reindex(weekdays)\nax = day_df.reset_index().plot.bar(x='index', y='pushblishDayName', rot=0)\nplt.xlabel(\"Days\")\nplt.ylabel(\"Counts\")\nplt.legend(labels = [\"Counts\"])\nplt.show()"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html#wordcloud-for-video-comments",
    "href": "docs/projects/Youtube_API_EDA.html#wordcloud-for-video-comments",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "Like the video titles, the video comments also revolve around Love, Life, Sign, Thing, Attraction words\n\n\nCode\nstop_words = set(stopwords.words('english'))\ncomdf['comments_no_stopwords'] = comdf['comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in comdf['comments_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nwordcloud = WordCloud(width = 1980, height = 720, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()"
  },
  {
    "objectID": "docs/projects/RL ML Agents.html",
    "href": "docs/projects/RL ML Agents.html",
    "title": "RL Unity ML Agents",
    "section": "",
    "text": "Given that a number of users of the ML-Agents Toolkit might not have a formal machine learning background, this page provides an overview to facilitate the understanding of the ML-Agents Toolkit. However, we will not attempt to provide a thorough treatment of machine learning as there are fantastic resources online.\nMachine learning, a branch of artificial intelligence, focuses on learning patterns from data. The three main classes of machine learning algorithms include: unsupervised learning, supervised learning and reinforcement learning. Each class of algorithm learns from a different type of data. The following paragraphs provide an overview for each of these classes of machine learning, as well as introductory examples.\n\n\nThe goal of unsupervised learning is to group or cluster similar items in a data set. For example, consider the players of a game. We may want to group the players depending on how engaged they are with the game. This would enable us to target different groups (e.g.¬†for highly-engaged players we might invite them to be beta testers for new features, while for unengaged players we might email them helpful tutorials). Say that we wish to split our players into two groups. We would first define basic attributes of the players, such as the number of hours played, total money spent on in-app purchases and number of levels completed. We can then feed this data set (three attributes for every player) to an unsupervised learning algorithm where we specify the number of groups to be two. The algorithm would then split the data set of players into two groups where the players within each group would be similar to each other. Given the attributes we used to describe each player, in this case, the output would be a split of all the players into two groups, where one group would semantically represent the engaged players and the second group would semantically represent the unengaged players.\nWith unsupervised learning, we did not provide specific examples of which players are considered engaged and which are considered unengaged. We just defined the appropriate attributes and relied on the algorithm to uncover the two groups on its own. This type of data set is typically called an unlabeled data set as it is lacking these direct labels. Consequently, unsupervised learning can be helpful in situations where these labels can be expensive or hard to produce. In the next paragraph, we overview supervised learning algorithms which accept input labels in addition to attributes.\n\n\n\nIn supervised learning, we do not want to just group similar items but directly learn a mapping from each item to the group (or class) that it belongs to. Returning to our earlier example of clustering players, let‚Äôs say we now wish to predict which of our players are about to churn (that is stop playing the game for the next 30 days). We can look into our historical records and create a data set that contains attributes of our players in addition to a label indicating whether they have churned or not. Note that the player attributes we use for this churn prediction task may be different from the ones we used for our earlier clustering task. We can then feed this data set (attributes and label for each player) into a supervised learning algorithm which would learn a mapping from the player attributes to a label indicating whether that player will churn or not. The intuition is that the supervised learning algorithm will learn which values of these attributes typically correspond to players who have churned and not churned (for example, it may learn that players who spend very little and play for very short periods will most likely churn). Now given this learned model, we can provide it the attributes of a new player (one that recently started playing the game) and it would output a predicted label for that player. This prediction is the algorithms expectation of whether the player will churn or not. We can now use these predictions to target the players who are expected to churn and entice them to continue playing the game.\nAs you may have noticed, for both supervised and unsupervised learning, there are two tasks that need to be performed: attribute selection and model selection. Attribute selection (also called feature selection) pertains to selecting how we wish to represent the entity of interest, in this case, the player. Model selection, on the other hand, pertains to selecting the algorithm (and its parameters) that perform the task well. Both of these tasks are active areas of machine learning research and, in practice, require several iterations to achieve good performance.\nWe now switch to reinforcement learning, the third class of machine learning algorithms, and arguably the one most relevant for the ML-Agents Toolkit.\n\n\n\nReinforcement learning can be viewed as a form of learning for sequential decision making that is commonly associated with controlling robots (but is, in fact, much more general). Consider an autonomous firefighting robot that is tasked with navigating into an area, finding the fire and neutralizing it. At any given moment, the robot perceives the environment through its sensors (e.g.¬†camera, heat, touch), processes this information and produces an action (e.g.¬†move to the left, rotate the water hose, turn on the water). In other words, it is continuously making decisions about how to interact in this environment given its view of the world (i.e.¬†sensors input) and objective (i.e.¬†neutralizing the fire). Teaching a robot to be a successful firefighting machine is precisely what reinforcement learning is designed to do.\nMore specifically, the goal of reinforcement learning is to learn a policy, which is essentially a mapping from observations to actions. An observation is what the robot can measure from its environment (in this case, all its sensory inputs) and an action, in its most raw form, is a change to the configuration of the robot (e.g.¬†position of its base, position of its water hose and whether the hose is on or off).\nThe last remaining piece of the reinforcement learning task is the reward signal. The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it with rewards (positive and negative) indicating how well it is doing on completing the task. Note that the robot does not know how to put out fires before it is trained. It learns the objective because it receives a large positive reward when it puts out the fire and a small negative reward for every passing second. The fact that rewards are sparse (i.e.¬†may not be provided at every step, but only when a robot arrives at a success or failure situation), is a defining characteristic of reinforcement learning and precisely why learning good policies can be difficult (and/or time-consuming) for complex environments.\n\nLearning a policy usually requires many trials and iterative policy updates. More specifically, the robot is placed in several fire situations and over time learns an optimal policy which allows it to put out fires more effectively. Obviously, we cannot expect to train a robot repeatedly in the real world, particularly when fires are involved. This is precisely why the use of Unity as a simulator serves as the perfect training grounds for learning such behaviors. While our discussion of reinforcement learning has centered around robots, there are strong parallels between robots and characters in a game. In fact, in many ways, one can view a non-playable character (NPC) as a virtual robot, with its own observations about the environment, its own set of actions and a specific objective. Thus it is natural to explore how we can train behaviors within Unity using reinforcement learning. This is precisely what the ML-Agents Toolkit offers. The video linked below includes a reinforcement learning demo showcasing training character behaviors using the ML-Agents Toolkit.\n\n  \n\nSimilar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective, while model selection is defining the form of the policy (mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices.\n\n\n\nOne common aspect of all three branches of machine learning is that they all involve a training phase and an inference phase. While the details of the training and inference phases are different for each of the three, at a high-level, the training phase involves building a model using the provided data, while the inference phase involves applying this model to new, previously unseen, data. More specifically:\n\nFor our unsupervised learning example, the training phase learns the optimal two clusters based on the data describing existing players, while the inference phase assigns a new player to one of these two clusters.\nFor our supervised learning example, the training phase learns the mapping from player attributes to player label (whether they churned or not), and the inference phase predicts whether a new player will churn or not based on that learned mapping.\nFor our reinforcement learning example, the training phase learns the optimal policy through guided trials, and in the inference phase, the agent observes and takes actions in the wild using its learned policy.\n\nTo briefly summarize: all three classes of algorithms involve training and inference phases in addition to attribute and model selections. What ultimately separates them is the type of data available to learn from. In unsupervised learning our data set was a collection of attributes, in supervised learning our data set was a collection of attribute-label pairs, and, lastly, in reinforcement learning our data set was a collection of observation-action-reward tuples.\n\n\n\nDeep learning is a family of algorithms that can be used to address any of the problems introduced above. More specifically, they can be used to solve both attribute and model selection tasks. Deep learning has gained popularity in recent years due to its outstanding performance on several challenging machine learning tasks. One example is AlphaGo, a computer Go program, that leverages deep learning, that was able to beat Lee Sedol (a Go world champion).\nA key characteristic of deep learning algorithms is their ability to learn very complex functions from large amounts of training data. This makes them a natural choice for reinforcement learning tasks when a large amount of data can be generated, say through the use of a simulator or engine such as Unity. By generating hundreds of thousands of simulations of the environment within Unity, we can learn policies for very complex environments (a complex environment is one where the number of observations an agent perceives and the number of actions they can take are large). Many of the algorithms we provide in ML-Agents use some form of deep learning, built on top of the open-source library, PyTorch."
  },
  {
    "objectID": "docs/projects/RL ML Agents.html#unsupervised-learning",
    "href": "docs/projects/RL ML Agents.html#unsupervised-learning",
    "title": "RL Unity ML Agents",
    "section": "",
    "text": "The goal of unsupervised learning is to group or cluster similar items in a data set. For example, consider the players of a game. We may want to group the players depending on how engaged they are with the game. This would enable us to target different groups (e.g.¬†for highly-engaged players we might invite them to be beta testers for new features, while for unengaged players we might email them helpful tutorials). Say that we wish to split our players into two groups. We would first define basic attributes of the players, such as the number of hours played, total money spent on in-app purchases and number of levels completed. We can then feed this data set (three attributes for every player) to an unsupervised learning algorithm where we specify the number of groups to be two. The algorithm would then split the data set of players into two groups where the players within each group would be similar to each other. Given the attributes we used to describe each player, in this case, the output would be a split of all the players into two groups, where one group would semantically represent the engaged players and the second group would semantically represent the unengaged players.\nWith unsupervised learning, we did not provide specific examples of which players are considered engaged and which are considered unengaged. We just defined the appropriate attributes and relied on the algorithm to uncover the two groups on its own. This type of data set is typically called an unlabeled data set as it is lacking these direct labels. Consequently, unsupervised learning can be helpful in situations where these labels can be expensive or hard to produce. In the next paragraph, we overview supervised learning algorithms which accept input labels in addition to attributes."
  },
  {
    "objectID": "docs/projects/RL ML Agents.html#supervised-learning",
    "href": "docs/projects/RL ML Agents.html#supervised-learning",
    "title": "RL Unity ML Agents",
    "section": "",
    "text": "In supervised learning, we do not want to just group similar items but directly learn a mapping from each item to the group (or class) that it belongs to. Returning to our earlier example of clustering players, let‚Äôs say we now wish to predict which of our players are about to churn (that is stop playing the game for the next 30 days). We can look into our historical records and create a data set that contains attributes of our players in addition to a label indicating whether they have churned or not. Note that the player attributes we use for this churn prediction task may be different from the ones we used for our earlier clustering task. We can then feed this data set (attributes and label for each player) into a supervised learning algorithm which would learn a mapping from the player attributes to a label indicating whether that player will churn or not. The intuition is that the supervised learning algorithm will learn which values of these attributes typically correspond to players who have churned and not churned (for example, it may learn that players who spend very little and play for very short periods will most likely churn). Now given this learned model, we can provide it the attributes of a new player (one that recently started playing the game) and it would output a predicted label for that player. This prediction is the algorithms expectation of whether the player will churn or not. We can now use these predictions to target the players who are expected to churn and entice them to continue playing the game.\nAs you may have noticed, for both supervised and unsupervised learning, there are two tasks that need to be performed: attribute selection and model selection. Attribute selection (also called feature selection) pertains to selecting how we wish to represent the entity of interest, in this case, the player. Model selection, on the other hand, pertains to selecting the algorithm (and its parameters) that perform the task well. Both of these tasks are active areas of machine learning research and, in practice, require several iterations to achieve good performance.\nWe now switch to reinforcement learning, the third class of machine learning algorithms, and arguably the one most relevant for the ML-Agents Toolkit."
  },
  {
    "objectID": "docs/projects/RL ML Agents.html#reinforcement-learning",
    "href": "docs/projects/RL ML Agents.html#reinforcement-learning",
    "title": "RL Unity ML Agents",
    "section": "",
    "text": "Reinforcement learning can be viewed as a form of learning for sequential decision making that is commonly associated with controlling robots (but is, in fact, much more general). Consider an autonomous firefighting robot that is tasked with navigating into an area, finding the fire and neutralizing it. At any given moment, the robot perceives the environment through its sensors (e.g.¬†camera, heat, touch), processes this information and produces an action (e.g.¬†move to the left, rotate the water hose, turn on the water). In other words, it is continuously making decisions about how to interact in this environment given its view of the world (i.e.¬†sensors input) and objective (i.e.¬†neutralizing the fire). Teaching a robot to be a successful firefighting machine is precisely what reinforcement learning is designed to do.\nMore specifically, the goal of reinforcement learning is to learn a policy, which is essentially a mapping from observations to actions. An observation is what the robot can measure from its environment (in this case, all its sensory inputs) and an action, in its most raw form, is a change to the configuration of the robot (e.g.¬†position of its base, position of its water hose and whether the hose is on or off).\nThe last remaining piece of the reinforcement learning task is the reward signal. The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it with rewards (positive and negative) indicating how well it is doing on completing the task. Note that the robot does not know how to put out fires before it is trained. It learns the objective because it receives a large positive reward when it puts out the fire and a small negative reward for every passing second. The fact that rewards are sparse (i.e.¬†may not be provided at every step, but only when a robot arrives at a success or failure situation), is a defining characteristic of reinforcement learning and precisely why learning good policies can be difficult (and/or time-consuming) for complex environments.\n\nLearning a policy usually requires many trials and iterative policy updates. More specifically, the robot is placed in several fire situations and over time learns an optimal policy which allows it to put out fires more effectively. Obviously, we cannot expect to train a robot repeatedly in the real world, particularly when fires are involved. This is precisely why the use of Unity as a simulator serves as the perfect training grounds for learning such behaviors. While our discussion of reinforcement learning has centered around robots, there are strong parallels between robots and characters in a game. In fact, in many ways, one can view a non-playable character (NPC) as a virtual robot, with its own observations about the environment, its own set of actions and a specific objective. Thus it is natural to explore how we can train behaviors within Unity using reinforcement learning. This is precisely what the ML-Agents Toolkit offers. The video linked below includes a reinforcement learning demo showcasing training character behaviors using the ML-Agents Toolkit.\n\n  \n\nSimilar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective, while model selection is defining the form of the policy (mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices."
  },
  {
    "objectID": "docs/projects/RL ML Agents.html#training-and-inference",
    "href": "docs/projects/RL ML Agents.html#training-and-inference",
    "title": "RL Unity ML Agents",
    "section": "",
    "text": "One common aspect of all three branches of machine learning is that they all involve a training phase and an inference phase. While the details of the training and inference phases are different for each of the three, at a high-level, the training phase involves building a model using the provided data, while the inference phase involves applying this model to new, previously unseen, data. More specifically:\n\nFor our unsupervised learning example, the training phase learns the optimal two clusters based on the data describing existing players, while the inference phase assigns a new player to one of these two clusters.\nFor our supervised learning example, the training phase learns the mapping from player attributes to player label (whether they churned or not), and the inference phase predicts whether a new player will churn or not based on that learned mapping.\nFor our reinforcement learning example, the training phase learns the optimal policy through guided trials, and in the inference phase, the agent observes and takes actions in the wild using its learned policy.\n\nTo briefly summarize: all three classes of algorithms involve training and inference phases in addition to attribute and model selections. What ultimately separates them is the type of data available to learn from. In unsupervised learning our data set was a collection of attributes, in supervised learning our data set was a collection of attribute-label pairs, and, lastly, in reinforcement learning our data set was a collection of observation-action-reward tuples."
  },
  {
    "objectID": "docs/projects/RL ML Agents.html#deep-learning",
    "href": "docs/projects/RL ML Agents.html#deep-learning",
    "title": "RL Unity ML Agents",
    "section": "",
    "text": "Deep learning is a family of algorithms that can be used to address any of the problems introduced above. More specifically, they can be used to solve both attribute and model selection tasks. Deep learning has gained popularity in recent years due to its outstanding performance on several challenging machine learning tasks. One example is AlphaGo, a computer Go program, that leverages deep learning, that was able to beat Lee Sedol (a Go world champion).\nA key characteristic of deep learning algorithms is their ability to learn very complex functions from large amounts of training data. This makes them a natural choice for reinforcement learning tasks when a large amount of data can be generated, say through the use of a simulator or engine such as Unity. By generating hundreds of thousands of simulations of the environment within Unity, we can learn policies for very complex environments (a complex environment is one where the number of observations an agent perceives and the number of actions they can take are large). Many of the algorithms we provide in ML-Agents use some form of deep learning, built on top of the open-source library, PyTorch."
  },
  {
    "objectID": "docs/projects/RL1_Introduction.html",
    "href": "docs/projects/RL1_Introduction.html",
    "title": "Introduction to RL HF",
    "section": "",
    "text": "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback\nRL Process: Imagine an agent learning to play a platform game:\n\nOur agent receives state \\(S_0\\) from the environment - we receive the first frame of our game\nBased on that state \\(S_0\\), the Agent takes action \\(A_0\\) - our agent will move to the right\nThe environment goes to a new state \\(S_1\\) - new frame\nThe environment gives some reward \\(R_1\\) to the agent - we‚Äôre not dead (Positive Reward +1)\n\nThis RL loop outputs a sequence of state, action, reward, and next state: \\(S_0, A_0, R_1, S_1\\)\nThe agent‚Äôs goal is to maximize it‚Äôs cumulative reward, callled the expected return.\n\n\n\n\nRL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\nThat‚Äôs why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.\n\n\n\n\n\nMarkov property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before.\n\n\n\n\n\nObservations/States are the information our agent gets from he environment. In the case of a video game, it can be a frame, in case of a trading agent, it can be the value of a certain stock.\nThere is a differentiation to make between observation and state, however:\n\nState s: is a complete description of the state of the world.\n\nIn a chess game, we have access to the whole board information, so we receive a state from the environment. In other words, the environment is fully observed.\n\nObservation o: is a partial description of the state.\n\nIn Super Mario Bros, we are in a partially observed environment. We receive an observation since we only see a part of the level.\n\n\n\n\n\n\n\nAction space is the set of all possible actions in an environment.\n\nThe actions can come from a discrete or continuous space:\n\nDiscrete space: the number of possible actions is finite. Ex. In Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.\nContinous space: A Self Driving Car agent has an infinite number of possible actions since it can turn left 20¬∞, 21,1¬∞, 21,2¬∞, honk, turn right 20¬∞.\n\nTaking this information into consideration is crucial because it will have importance when choosing the RL algorithm in the future.\n\n\n\n\n\n\nThe rewared is fundamental in RL because it‚Äôs the only feedback for the agent. Because of this our agent knows if the action taken was good or not.\n\nThe cumulative reward at each time step t, equals the sum of all rewards in the sequence.\n\n\n\nHowever, in reality, we can‚Äôt just add them like that. The rewards that come sooner (at the beginning of the game) are more likely to happen since they are more predictable than the long-term future reward.\n\n\nLet‚Äôs say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (that can move too). The mouse‚Äôs goal is to eat the maximum amount of cheese before being eaten by the cat.\nAs we can see in the diagram, it‚Äôs more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).\nConsequently, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we‚Äôre not really sure we‚Äôll be able to eat it.\nTo discount the rewards, we proceed like this:\n\n\nWe define a discount rate called gamma. It must be between 0 and 1. Most of the time between 0.95 and 0.99.\n\n\nThe larger the gamma, the smaller the discount. This means our agent cares more about the long-term reward.\nOn the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n\n\nThen, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.\n\n\n\nOur discounted expected cumulative reward is:\n\n\n\n\n\nA task is an instance of a Reinforcement learning problem. We can have 2 types of tasks: episodic and continuing.\nEpisodic task\n\nIn this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States.\n\nFor instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ends when you‚Äôre killed or you reached the end of the level.\n\n\nContinuing task:\n\nThese are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.\n\nFor instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it.\n\n\n\n\n\n\n\nExploration is exploring the environment by trying random actions in order to find more information about the environment\nExploitation is exploiting known information to maximize the reward\nRemember the goal of our RL agent is to maximize the expected cumulative reward. However, one can fall in the trap of exploiting the known rewards all the time.\n\nExample - In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000). - However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation). - But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).\n\nIf it‚Äôs still confusing, think of a real problem: the choice of picking a restaurant:\n\nExploitation: You go to the same one that you know is good every day and take the risk to miss another better restaurant.\nExploration: Try restaurants you never went to before, with the risk of having a bad experience but the probable opportunity of a fantastic experience.\n\nThis is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment.\n\nTherefore, we must define a rule that helps to handle this trade-off. We‚Äôll see the different ways to handle it in the future units.\n\n\n\n\n\n\nAfter taking a look at the RL framework\n\nRL process which consists of:\n\nObservations or States Space\nAction Space\nRewards and it‚Äôs discounting\nTasks, i.e.¬†an instance of reinforcement learning. Episodic or Continuing tasks\n\nReward Hypothesis: Every goal can be described as a maximization of the expected return.\nMarkov Property\nExploration vs Exploitation.\n\nWe now have to see how this whole RL framework can be used to solve the RL problems\n\nIn other words, how do we build an RL agent that can select the actions that maximize it‚Äôs expected cumulative rewards?\n\nPolicy \\(\\pi\\)\n\nThe brain of our agent defining the behaviour of our agent\nDescribes which action to take in which state\nThis is what we want to learn to solve the RL problem via the framework.\n2 Approaches to find the policy: Policy Based Methods and Value Based Methods\n\nPolicy based methods:\n\nLearn the policy function directly.\n\nThis function defines a mapping from each state to the best corresponding action or a probability distribution over the set of all the possible actions at that state\nThere are 2 types of policy,\n\nDeterministic policy: Given a state this policy returns the same action\nStochastic policy: Outputs a probability distribution over all the actions in a given state.\n\n\n\n\n\n\n\nValue based methods:\n\nIndirect way of learning policy\nLearn a value function, which maps a state to the expected value of being at that state.\n\nValue of that state is the expected discounted return the agent can get if it starts in that state, and then acts according to our policy.\nAct according to our policy just means that our policy is going to the state with the highest value.\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems ‚Äî hence the name ‚Äúdeep‚Äù.\nIn the next unit, we‚Äôll learn about two value-based algorithms: Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning.\n\nIn Q-learning approach, we use a traditional algorithm to create a Q table that helps us find what action to take for each state.\nIn Deep Q-learning approach, we will use a Neural Network (to approximate the Q value).\n\n\n\n\n\n\n\nReinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.\nThe goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.\nThe RL process is a loop that outputs a sequence of state, action, reward and next state.\nTo calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.\nTo solve an RL problem, you want to find an optimal policy. The policy is the ‚Äúbrain‚Äù of your agent, which will tell us what action to take given a state. The optimal policy is the one which gives you the actions that maximize the expected return.\nThere are two ways to find your optimal policy:\n\nBy training your policy directly: policy-based methods.\nBy training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n\nFinally, we speak about Deep RL because we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name ‚Äúdeep‚Äù.\n\n\n\n\n\nMarkov Property\n\nIt implies that the action taken by our agent is conditional solely on the present state and independent of the past states and actions.\n\nObservations/State\n\nState: Complete description of the state of the world.\nObservation: Partial description of the state of the environment/world.\n\nActions\n\nDiscrete Actions: Finite number of actions, such as left, right, up, and down.\nContinuous Actions: Infinite possibility of actions; for example, in the case of self-driving cars, the driving scenario has an infinite possibility of actions occurring.\n\nRewards and Discounting\n\nRewards: Fundamental factor in RL. Tells the agent whether the action taken is good/bad.\nRL algorithms are focused on maximizing the cumulative reward.\n\nReward Hypothesis: RL problems can be formulated as a maximisation of (cumulative) return.\n\nDiscounting is performed because rewards obtained at the start are more likely to happen as they are more predictable than long-term rewards.\n\nTasks\n\nEpisodic: Has a starting point and an ending point.\nContinuous: Has a starting point but no ending point.\n\nExploration v/s Exploitation Trade-Off\n\nExploration: It‚Äôs all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.\nExploitation: It‚Äôs about exploiting what we know about the environment to gain maximum rewards.\nExploration-Exploitation Trade-Off: It balances how much we want to explore the environment and how much we want to exploit what we know about the environment.\n\nPolicy\n\nPolicy: It is called the agent‚Äôs brain. It tells us what action to take, given the state.\nOptimal Policy: Policy that maximizes the expected return when an agent acts according to it. It is learned through training.\nPolicy-based Methods:\n\nAn approach to solving RL problems.\nIn this method, the Policy is learned directly.\nWill map each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.\n\nValue-based Methods:\n\nAnother approach to solving RL problems.\nHere, instead of training a policy, we train a value function that maps each state to the expected value of being in that state."
  },
  {
    "objectID": "docs/projects/RL1_Introduction.html#chapter-1-introduction-to-deep-reinforcement-learning",
    "href": "docs/projects/RL1_Introduction.html#chapter-1-introduction-to-deep-reinforcement-learning",
    "title": "Introduction to RL HF",
    "section": "",
    "text": "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback\nRL Process: Imagine an agent learning to play a platform game:\n\nOur agent receives state \\(S_0\\) from the environment - we receive the first frame of our game\nBased on that state \\(S_0\\), the Agent takes action \\(A_0\\) - our agent will move to the right\nThe environment goes to a new state \\(S_1\\) - new frame\nThe environment gives some reward \\(R_1\\) to the agent - we‚Äôre not dead (Positive Reward +1)\n\nThis RL loop outputs a sequence of state, action, reward, and next state: \\(S_0, A_0, R_1, S_1\\)\nThe agent‚Äôs goal is to maximize it‚Äôs cumulative reward, callled the expected return.\n\n\n\n\nRL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\nThat‚Äôs why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.\n\n\n\n\n\nMarkov property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before.\n\n\n\n\n\nObservations/States are the information our agent gets from he environment. In the case of a video game, it can be a frame, in case of a trading agent, it can be the value of a certain stock.\nThere is a differentiation to make between observation and state, however:\n\nState s: is a complete description of the state of the world.\n\nIn a chess game, we have access to the whole board information, so we receive a state from the environment. In other words, the environment is fully observed.\n\nObservation o: is a partial description of the state.\n\nIn Super Mario Bros, we are in a partially observed environment. We receive an observation since we only see a part of the level.\n\n\n\n\n\n\n\nAction space is the set of all possible actions in an environment.\n\nThe actions can come from a discrete or continuous space:\n\nDiscrete space: the number of possible actions is finite. Ex. In Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.\nContinous space: A Self Driving Car agent has an infinite number of possible actions since it can turn left 20¬∞, 21,1¬∞, 21,2¬∞, honk, turn right 20¬∞.\n\nTaking this information into consideration is crucial because it will have importance when choosing the RL algorithm in the future.\n\n\n\n\n\n\nThe rewared is fundamental in RL because it‚Äôs the only feedback for the agent. Because of this our agent knows if the action taken was good or not.\n\nThe cumulative reward at each time step t, equals the sum of all rewards in the sequence.\n\n\n\nHowever, in reality, we can‚Äôt just add them like that. The rewards that come sooner (at the beginning of the game) are more likely to happen since they are more predictable than the long-term future reward.\n\n\nLet‚Äôs say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (that can move too). The mouse‚Äôs goal is to eat the maximum amount of cheese before being eaten by the cat.\nAs we can see in the diagram, it‚Äôs more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).\nConsequently, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we‚Äôre not really sure we‚Äôll be able to eat it.\nTo discount the rewards, we proceed like this:\n\n\nWe define a discount rate called gamma. It must be between 0 and 1. Most of the time between 0.95 and 0.99.\n\n\nThe larger the gamma, the smaller the discount. This means our agent cares more about the long-term reward.\nOn the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n\n\nThen, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.\n\n\n\nOur discounted expected cumulative reward is:\n\n\n\n\n\nA task is an instance of a Reinforcement learning problem. We can have 2 types of tasks: episodic and continuing.\nEpisodic task\n\nIn this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States.\n\nFor instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ends when you‚Äôre killed or you reached the end of the level.\n\n\nContinuing task:\n\nThese are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.\n\nFor instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it.\n\n\n\n\n\n\n\nExploration is exploring the environment by trying random actions in order to find more information about the environment\nExploitation is exploiting known information to maximize the reward\nRemember the goal of our RL agent is to maximize the expected cumulative reward. However, one can fall in the trap of exploiting the known rewards all the time.\n\nExample - In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000). - However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation). - But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).\n\nIf it‚Äôs still confusing, think of a real problem: the choice of picking a restaurant:\n\nExploitation: You go to the same one that you know is good every day and take the risk to miss another better restaurant.\nExploration: Try restaurants you never went to before, with the risk of having a bad experience but the probable opportunity of a fantastic experience.\n\nThis is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment.\n\nTherefore, we must define a rule that helps to handle this trade-off. We‚Äôll see the different ways to handle it in the future units.\n\n\n\n\n\n\nAfter taking a look at the RL framework\n\nRL process which consists of:\n\nObservations or States Space\nAction Space\nRewards and it‚Äôs discounting\nTasks, i.e.¬†an instance of reinforcement learning. Episodic or Continuing tasks\n\nReward Hypothesis: Every goal can be described as a maximization of the expected return.\nMarkov Property\nExploration vs Exploitation.\n\nWe now have to see how this whole RL framework can be used to solve the RL problems\n\nIn other words, how do we build an RL agent that can select the actions that maximize it‚Äôs expected cumulative rewards?\n\nPolicy \\(\\pi\\)\n\nThe brain of our agent defining the behaviour of our agent\nDescribes which action to take in which state\nThis is what we want to learn to solve the RL problem via the framework.\n2 Approaches to find the policy: Policy Based Methods and Value Based Methods\n\nPolicy based methods:\n\nLearn the policy function directly.\n\nThis function defines a mapping from each state to the best corresponding action or a probability distribution over the set of all the possible actions at that state\nThere are 2 types of policy,\n\nDeterministic policy: Given a state this policy returns the same action\nStochastic policy: Outputs a probability distribution over all the actions in a given state.\n\n\n\n\n\n\n\nValue based methods:\n\nIndirect way of learning policy\nLearn a value function, which maps a state to the expected value of being at that state.\n\nValue of that state is the expected discounted return the agent can get if it starts in that state, and then acts according to our policy.\nAct according to our policy just means that our policy is going to the state with the highest value.\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems ‚Äî hence the name ‚Äúdeep‚Äù.\nIn the next unit, we‚Äôll learn about two value-based algorithms: Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning.\n\nIn Q-learning approach, we use a traditional algorithm to create a Q table that helps us find what action to take for each state.\nIn Deep Q-learning approach, we will use a Neural Network (to approximate the Q value).\n\n\n\n\n\n\n\nReinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.\nThe goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.\nThe RL process is a loop that outputs a sequence of state, action, reward and next state.\nTo calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.\nTo solve an RL problem, you want to find an optimal policy. The policy is the ‚Äúbrain‚Äù of your agent, which will tell us what action to take given a state. The optimal policy is the one which gives you the actions that maximize the expected return.\nThere are two ways to find your optimal policy:\n\nBy training your policy directly: policy-based methods.\nBy training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n\nFinally, we speak about Deep RL because we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name ‚Äúdeep‚Äù.\n\n\n\n\n\nMarkov Property\n\nIt implies that the action taken by our agent is conditional solely on the present state and independent of the past states and actions.\n\nObservations/State\n\nState: Complete description of the state of the world.\nObservation: Partial description of the state of the environment/world.\n\nActions\n\nDiscrete Actions: Finite number of actions, such as left, right, up, and down.\nContinuous Actions: Infinite possibility of actions; for example, in the case of self-driving cars, the driving scenario has an infinite possibility of actions occurring.\n\nRewards and Discounting\n\nRewards: Fundamental factor in RL. Tells the agent whether the action taken is good/bad.\nRL algorithms are focused on maximizing the cumulative reward.\n\nReward Hypothesis: RL problems can be formulated as a maximisation of (cumulative) return.\n\nDiscounting is performed because rewards obtained at the start are more likely to happen as they are more predictable than long-term rewards.\n\nTasks\n\nEpisodic: Has a starting point and an ending point.\nContinuous: Has a starting point but no ending point.\n\nExploration v/s Exploitation Trade-Off\n\nExploration: It‚Äôs all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.\nExploitation: It‚Äôs about exploiting what we know about the environment to gain maximum rewards.\nExploration-Exploitation Trade-Off: It balances how much we want to explore the environment and how much we want to exploit what we know about the environment.\n\nPolicy\n\nPolicy: It is called the agent‚Äôs brain. It tells us what action to take, given the state.\nOptimal Policy: Policy that maximizes the expected return when an agent acts according to it. It is learned through training.\nPolicy-based Methods:\n\nAn approach to solving RL problems.\nIn this method, the Policy is learned directly.\nWill map each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.\n\nValue-based Methods:\n\nAnother approach to solving RL problems.\nHere, instead of training a policy, we train a value function that maps each state to the expected value of being in that state."
  },
  {
    "objectID": "docs/notes/Docker_Notes.html",
    "href": "docs/notes/Docker_Notes.html",
    "title": "Docker Tutorial Part 4",
    "section": "",
    "text": "Once we have worked with the single container apps, being a data scientist, we need to deal with various processes, at the same time to interact with our data. For example we can have redis to cache the process, we can have MySql, MongoDb and much more. So where to run these other processes.\nIn general the philosophy of docker is that each container should do one thing and do it well. So the few reasons because of which we will run any different process in a seperate container are:\n\nScaling of API‚Äôs and front-ends differently than databases in our dummy example, but we can apply it to any process.\nSeperate containers allow one to version and update versions of different processes in isolation.\nUsing a seperate container is a good habit, so that builds up the practice to use managed services in production, like for our example using a managed service for database in production, in this case we will not ship our database engine with the app.\nRunning multiple processes together adds complexity to startup and shutdown of the processes, but this is simplified by using multiple containers.\n\nSo with these reasons, it‚Äôs better to go with seperate containers for different processes for the application.\n\n\n\n\n\nContainers by default run in isolation and don‚Äôt know anythign about other processes or containers on the same machine.\nThus to have multiple containers for different interlinked processes, like in our case, our frontend process communicating with the db at the backend, we need some sort of mechanism to make one container talk to the other.\nHere networking comes into play, if there are 2 containers on the same network they can talk to each other.\nThere are 2 ways to do this communication possible.\n\nAssign the network when starting the container\nConnect an already running container to a network.\n\n\n\n\n\nFor MySQL we assign a network while starting the container.\n\n\n\nCode\n# Creation of a network\ndocker network create todo-app\n\n\n\nStarting a MySQL container and attaching it to the network, here we are usign few env variables that the DB will use to initialize the db.\n\n\n\nCode\ndocker run -d \\\n     --network todo-app --network-alias mysql \\\n     -v todo-mysql-data:/var/lib/mysql \\\n     -e MYSQL_ROOT_PASSWORD=secret \\\n     -e MYSQL_DATABASE=todos \\\n     mysql:8.0\n\n\nHere the volume named as todo-mysql-data is mounted to /var/lib/mysql, and here it runs without creating a new volume via docker volume create command.\n\nTo confirm the database is up and running, we connect to db and verify that it connects.\n\n\n\nCode\ndocker exec -it &lt;mysql-container-id&gt; mysql -u root -p\n\n\n\nFor the password promp type secret. In the MySQL shell, list the databases and verify you see the todos databse.\n\n\n\nCode\nmysql&gt; SHOW DATABASES;\n\n\n\nThe op will be something like this:\n\n +--------------------+\n | Database           |\n +--------------------+\n | information_schema |\n | mysql              |\n | performance_schema |\n | sys                |\n | todos              |\n +--------------------+\n 5 rows in set (0.00 sec)\n\nExit the MySQL shell to return to the shell on your machine.\n\n\n\nCode\nmysql&gt; exit\n\n\nWith all this setup we have a todos database ready for use on the network we created earlier.\n\n\n\n\nThis step is simply used to see it‚Äôs IP address of MySQL container.\nNow that MySQL is running we can use it, but now we need another container to connect to MySQL‚Äôs container, on the same network.\n\nThe question is how will the other container find the MySQL container‚Ä¶ the solution is each container has it‚Äôs own IP address.\nTo better understand container networking, you‚Äôre going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues.\n\nStart a new container using the nicolaka/netshoot image, and make sure, it is connected to the same network of todo.\n\n\n\nCode\ndocker run -it --network todo-app nicolaka/netshoot\n\n\n\nInside the container, we‚Äôre going to use the dig command, which is a useful DNS too. We‚Äôre going to look up the IP address for the hostname mysql.\n\ndig mysql\n\nThe output of dig will be as follows\n\n; &lt;&lt;&gt;&gt; DiG 9.18.8 &lt;&lt;&gt;&gt; mysql\n ;; global options: +cmd\n ;; Got answer:\n ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 32162\n ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n ;; QUESTION SECTION:\n ;mysql.                IN  A\n\n ;; ANSWER SECTION:\n mysql.         600 IN  A   172.23.0.2\n\n ;; Query time: 0 msec\n ;; SERVER: 127.0.0.11#53(127.0.0.11)\n ;; WHEN: Tue Oct 01 23:47:24 UTC 2019\n ;; MSG SIZE  rcvd: 44\n\nIn the ‚ÄúANSWER SECTION‚Äù, we see an A record for mysql that resolevs to 172.23.0.2 (your ip address may be different). While mysql isn‚Äôt normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias. Remember, the --network-alias used earlier with the creation of SQL container.\n\nThis means that our app only needs to connect to a host named mysql and it‚Äôll talk to the database.\n\n\n\n\n\n\nNow once we know the ip address of MySQL container, we will simply connect our todo app‚Äôs container to it.\nThe todo app supports a few MySQL env variables setup.\n\nMYSQL_HOST - the hostname for the running MySQL server\nMYSQL_USER - the username to use for the connection\nMYSQL_PASSWORD - the password to use for the connection\nMYSQL_DB - the database to use once connected\n\nWhile using env vars to set connection settings is generally accepted for development, it‚Äôs highly discouraged when running applications in production. Diogo Monica, a former lead of security at Docker, wrote a fantastic blog post explaining why.\n\nA more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You‚Äôll see many apps (including the MySQL image and the todo app) also support env vars with a _FILE suffix to point to a file containing the variable.\nAs an example, setting the MYSQL_PASSWORD_FILE var will cause the app to use the contents of the referenced file as the connection password. Docker doesn‚Äôt do anything to support these env vars. Your app will need to know to look for the variable and get the file contents.\n\nWe can now start the dev-ready container.\n\nSpecify each of the environment variables above, as well as connect the container to the app network from the getting-started/app directory.\n\n\n\n\nCode\ndocker run -dp 3000:3000 \\\n   -w /app -v \"$(pwd):/app\" \\\n   --network todo-app \\\n   -e MYSQL_HOST=mysql \\\n   -e MYSQL_USER=root \\\n   -e MYSQL_PASSWORD=secret \\\n   -e MYSQL_DB=todos \\\n   node:18-alpine \\\n   sh -c \"yarn install && yarn run dev\"\n\n\n\nIf you look at the logs for the container (docker logs -f ), you should see a message similar to the following, which indicates it‚Äôs using the mysql database.\n\n\n\nCode\nnodemon src/index.js\n [nodemon] 2.0.20\n [nodemon] to restart at any time, enter `rs`\n [nodemon] watching dir(s): *.*\n [nodemon] starting `node src/index.js`\n Connected to mysql db at host mysql\n Listening on port 3000\n\n\n\nOpen the app in your browser and add a few items to your todo list.\nConnect to the mysql database and prove that the items are being written to the database. Remember, the password is secret.\n\n\n\nCode\ndocker exec -it &lt;mysql-container-id&gt; mysql -p todos\n\n\n\nAnd in the mysql shell, run the following which will generate the output the items from the todo list:\n\n\n\nCode\nmysql&gt; select * from todo_items;\n\n\n +--------------------------------------+--------------------+-----------+\n | id                                   | name               | completed |\n +--------------------------------------+--------------------+-----------+\n | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! |         0 |\n | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome!        |         0 |\n +--------------------------------------+--------------------+-----------+\n\n\n\n\n\nAt this point we have an application that stores it‚Äôs data in an external database running in a separate container.\n\nWe learned a little bit about container networking and service discovery using DNS.\n\nBut there‚Äôs a good chance you are starting to feel a little overwhelmed with everything you need to do start up this application. You have to create a network, start containers, specify all of the env variables, expose ports and more!. That‚Äôs a lot to remember and it‚Äôs actually making things harder to pass aloong to someone else.\nIn the following section, we‚Äôll see Docker Compose. With Docker compose, we can share the application stacks in a much easier way and let others spin them up with a single, simple command."
  },
  {
    "objectID": "docs/notes/Docker_Notes.html#docker-multicontainer-apps",
    "href": "docs/notes/Docker_Notes.html#docker-multicontainer-apps",
    "title": "Docker Tutorial Part 4",
    "section": "",
    "text": "Once we have worked with the single container apps, being a data scientist, we need to deal with various processes, at the same time to interact with our data. For example we can have redis to cache the process, we can have MySql, MongoDb and much more. So where to run these other processes.\nIn general the philosophy of docker is that each container should do one thing and do it well. So the few reasons because of which we will run any different process in a seperate container are:\n\nScaling of API‚Äôs and front-ends differently than databases in our dummy example, but we can apply it to any process.\nSeperate containers allow one to version and update versions of different processes in isolation.\nUsing a seperate container is a good habit, so that builds up the practice to use managed services in production, like for our example using a managed service for database in production, in this case we will not ship our database engine with the app.\nRunning multiple processes together adds complexity to startup and shutdown of the processes, but this is simplified by using multiple containers.\n\nSo with these reasons, it‚Äôs better to go with seperate containers for different processes for the application.\n\n\n\n\n\nContainers by default run in isolation and don‚Äôt know anythign about other processes or containers on the same machine.\nThus to have multiple containers for different interlinked processes, like in our case, our frontend process communicating with the db at the backend, we need some sort of mechanism to make one container talk to the other.\nHere networking comes into play, if there are 2 containers on the same network they can talk to each other.\nThere are 2 ways to do this communication possible.\n\nAssign the network when starting the container\nConnect an already running container to a network.\n\n\n\n\n\nFor MySQL we assign a network while starting the container.\n\n\n\nCode\n# Creation of a network\ndocker network create todo-app\n\n\n\nStarting a MySQL container and attaching it to the network, here we are usign few env variables that the DB will use to initialize the db.\n\n\n\nCode\ndocker run -d \\\n     --network todo-app --network-alias mysql \\\n     -v todo-mysql-data:/var/lib/mysql \\\n     -e MYSQL_ROOT_PASSWORD=secret \\\n     -e MYSQL_DATABASE=todos \\\n     mysql:8.0\n\n\nHere the volume named as todo-mysql-data is mounted to /var/lib/mysql, and here it runs without creating a new volume via docker volume create command.\n\nTo confirm the database is up and running, we connect to db and verify that it connects.\n\n\n\nCode\ndocker exec -it &lt;mysql-container-id&gt; mysql -u root -p\n\n\n\nFor the password promp type secret. In the MySQL shell, list the databases and verify you see the todos databse.\n\n\n\nCode\nmysql&gt; SHOW DATABASES;\n\n\n\nThe op will be something like this:\n\n +--------------------+\n | Database           |\n +--------------------+\n | information_schema |\n | mysql              |\n | performance_schema |\n | sys                |\n | todos              |\n +--------------------+\n 5 rows in set (0.00 sec)\n\nExit the MySQL shell to return to the shell on your machine.\n\n\n\nCode\nmysql&gt; exit\n\n\nWith all this setup we have a todos database ready for use on the network we created earlier.\n\n\n\n\nThis step is simply used to see it‚Äôs IP address of MySQL container.\nNow that MySQL is running we can use it, but now we need another container to connect to MySQL‚Äôs container, on the same network.\n\nThe question is how will the other container find the MySQL container‚Ä¶ the solution is each container has it‚Äôs own IP address.\nTo better understand container networking, you‚Äôre going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues.\n\nStart a new container using the nicolaka/netshoot image, and make sure, it is connected to the same network of todo.\n\n\n\nCode\ndocker run -it --network todo-app nicolaka/netshoot\n\n\n\nInside the container, we‚Äôre going to use the dig command, which is a useful DNS too. We‚Äôre going to look up the IP address for the hostname mysql.\n\ndig mysql\n\nThe output of dig will be as follows\n\n; &lt;&lt;&gt;&gt; DiG 9.18.8 &lt;&lt;&gt;&gt; mysql\n ;; global options: +cmd\n ;; Got answer:\n ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 32162\n ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n ;; QUESTION SECTION:\n ;mysql.                IN  A\n\n ;; ANSWER SECTION:\n mysql.         600 IN  A   172.23.0.2\n\n ;; Query time: 0 msec\n ;; SERVER: 127.0.0.11#53(127.0.0.11)\n ;; WHEN: Tue Oct 01 23:47:24 UTC 2019\n ;; MSG SIZE  rcvd: 44\n\nIn the ‚ÄúANSWER SECTION‚Äù, we see an A record for mysql that resolevs to 172.23.0.2 (your ip address may be different). While mysql isn‚Äôt normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias. Remember, the --network-alias used earlier with the creation of SQL container.\n\nThis means that our app only needs to connect to a host named mysql and it‚Äôll talk to the database.\n\n\n\n\n\n\nNow once we know the ip address of MySQL container, we will simply connect our todo app‚Äôs container to it.\nThe todo app supports a few MySQL env variables setup.\n\nMYSQL_HOST - the hostname for the running MySQL server\nMYSQL_USER - the username to use for the connection\nMYSQL_PASSWORD - the password to use for the connection\nMYSQL_DB - the database to use once connected\n\nWhile using env vars to set connection settings is generally accepted for development, it‚Äôs highly discouraged when running applications in production. Diogo Monica, a former lead of security at Docker, wrote a fantastic blog post explaining why.\n\nA more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You‚Äôll see many apps (including the MySQL image and the todo app) also support env vars with a _FILE suffix to point to a file containing the variable.\nAs an example, setting the MYSQL_PASSWORD_FILE var will cause the app to use the contents of the referenced file as the connection password. Docker doesn‚Äôt do anything to support these env vars. Your app will need to know to look for the variable and get the file contents.\n\nWe can now start the dev-ready container.\n\nSpecify each of the environment variables above, as well as connect the container to the app network from the getting-started/app directory.\n\n\n\n\nCode\ndocker run -dp 3000:3000 \\\n   -w /app -v \"$(pwd):/app\" \\\n   --network todo-app \\\n   -e MYSQL_HOST=mysql \\\n   -e MYSQL_USER=root \\\n   -e MYSQL_PASSWORD=secret \\\n   -e MYSQL_DB=todos \\\n   node:18-alpine \\\n   sh -c \"yarn install && yarn run dev\"\n\n\n\nIf you look at the logs for the container (docker logs -f ), you should see a message similar to the following, which indicates it‚Äôs using the mysql database.\n\n\n\nCode\nnodemon src/index.js\n [nodemon] 2.0.20\n [nodemon] to restart at any time, enter `rs`\n [nodemon] watching dir(s): *.*\n [nodemon] starting `node src/index.js`\n Connected to mysql db at host mysql\n Listening on port 3000\n\n\n\nOpen the app in your browser and add a few items to your todo list.\nConnect to the mysql database and prove that the items are being written to the database. Remember, the password is secret.\n\n\n\nCode\ndocker exec -it &lt;mysql-container-id&gt; mysql -p todos\n\n\n\nAnd in the mysql shell, run the following which will generate the output the items from the todo list:\n\n\n\nCode\nmysql&gt; select * from todo_items;\n\n\n +--------------------------------------+--------------------+-----------+\n | id                                   | name               | completed |\n +--------------------------------------+--------------------+-----------+\n | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! |         0 |\n | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome!        |         0 |\n +--------------------------------------+--------------------+-----------+\n\n\n\n\n\nAt this point we have an application that stores it‚Äôs data in an external database running in a separate container.\n\nWe learned a little bit about container networking and service discovery using DNS.\n\nBut there‚Äôs a good chance you are starting to feel a little overwhelmed with everything you need to do start up this application. You have to create a network, start containers, specify all of the env variables, expose ports and more!. That‚Äôs a lot to remember and it‚Äôs actually making things harder to pass aloong to someone else.\nIn the following section, we‚Äôll see Docker Compose. With Docker compose, we can share the application stacks in a much easier way and let others spin them up with a single, simple command."
  },
  {
    "objectID": "docs/notes/Docker_Notes.html#using-docker-compose",
    "href": "docs/notes/Docker_Notes.html#using-docker-compose",
    "title": "Docker Tutorial Part 4",
    "section": "Using Docker Compose",
    "text": "Using Docker Compose\n\nDocker compose is a tool that was developed to help define and share multi-container applications. With compose, we can create a YAML file to define the services and with a single command, which can spin everything up or tear it all down.\nThe big advantage of compose is one can define the application stack in a file, keep it at the root of the project repo, and easily enable someone else to contribute to one‚Äôs project. Someone would only need to clone the repo and start the compose app.\n\n\nInstalling Docker Compose\n\nIf you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well.\n\n\n\nCode\n$ docker compose version #check the installed version.\n\n\n\n\nCreation of Compose file\n\nAt the root of the /getting-started/app folder, create a file named docker-compose.yml\nIn the compose file, we‚Äôll start off by defining the list of services (or containers) we want to run as part of our application.\nNow we‚Äôll start migrating a service at a time into the compose file.\n\n\nDefine the app service\n\nTo remember, this was the command we were usign to define our app container.\n\n\n\nCode\ndocker run -dp 3000:3000 \\\n  -w /app -v \"$(pwd):/app\" \\\n  --network todo-app \\\n  -e MYSQL_HOST=mysql \\\n  -e MYSQL_USER=root \\\n  -e MYSQL_PASSWORD=secret \\\n  -e MYSQL_DB=todos \\\n  node:18-alpine \\\n  sh -c \"yarn install && yarn run dev\"\n\n\n\nFirst, let‚Äôs define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service.\n\n\n\nCode\nservices:\n  app:\n    image: node:18-alpine\n\n\n\nTypically, you will see the command close to the image definition, although there is no requirement on ordering. So, let‚Äôs go ahead and move that into our file.\n\n\n\nCode\nservices:\n  app:\n    image: node:18-alpine\n    command: sh -c \"yarn install && yarn run dev\"\n\n\n\nLet‚Äôs migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well.\n\n\n\nCode\nservices:\n  app:\n    image: node:18-alpine\n    command: sh -c \"yarn install && yarn run dev\"\n    ports:\n      - 3000:3000\n\n\n\nNext, we‚Äôll migrate both the working directory (-w /app) and the volume mapping (-v ‚Äú$(pwd):/app‚Äù) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory.\n\n\n\nCode\nservices:\n  app:\n    image: node:18-alpine\n    command: sh -c \"yarn install && yarn run dev\"\n    ports:\n      - 3000:3000\n    working_dir: /app\n    volumes:\n      - ./:/app\n\n\n\nFinally, we need to migrate the environment variable definitions using the environment key.\n\n\n\nCode\nservices:\n  app:\n    image: node:18-alpine\n    command: sh -c \"yarn install && yarn run dev\"\n    ports:\n      - 3000:3000\n    working_dir: /app\n    volumes:\n      - ./:/app\n    environment:\n      MYSQL_HOST: mysql\n      MYSQL_USER: root\n      MYSQL_PASSWORD: secret\n      MYSQL_DB: todos\n\n\n\n\nDefine the app service\n\nThe command we used for the MySQL container earlier was:\n\n\n\nCode\ndocker run -d \\\n  --network todo-app --network-alias mysql \\\n  -v todo-mysql-data:/var/lib/mysql \\\n  -e MYSQL_ROOT_PASSWORD=secret \\\n  -e MYSQL_DATABASE=todos \\\n  mysql:8.0\n\n\n\nIn our services section of docker-compose.yml file we add a new service and name it mysql so it automatically gets the network alias. We‚Äôll go ahead and specify the image to use as well.\n\n\n\nCode\nservices:\n  app:\n    # The app service definition\n  mysql:\n    image: mysql:8.0\n\n\n\nNext we‚Äôll define the volume mapping. When we ran the container with docker run, the named volume was created automatically. However, that doesn‚Äôt happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. there are many options available though.\n\n\n\nCode\nservices:\n  app:\n    # The app service definition\n  mysql:\n    image: mysql:8.0\n    volumes:\n      - todo-mysql-data:/var/lib/mysql\n\nvolumes:\n  todo-mysql-data:\n\n\n\nFinally, we only need to specify the environment variables.\n\n\n\nCode\nservices:\n  app:\n    # The app service definition\n  mysql:\n    image: mysql:8.0\n    volumes:\n      - todo-mysql-data:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: secret\n      MYSQL_DATABASE: todos\n\nvolumes:\n  todo-mysql-data:\n\n\n\nAt this point the docker-compose.yml should look like this:\n\n\n\nCode\nservices:\n  app:\n    image: node:18-alpine\n    command: sh -c \"yarn install && yarn run dev\"\n    ports:\n      - 3000:3000\n    working_dir: /app\n    volumes:\n      - ./:/app\n    environment:\n      MYSQL_HOST: mysql\n      MYSQL_USER: root\n      MYSQL_PASSWORD: secret\n      MYSQL_DB: todos\n\n  mysql:\n    image: mysql:8.0\n    volumes:\n      - todo-mysql-data:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: secret\n      MYSQL_DATABASE: todos\n\nvolumes:\n  todo-mysql-data:\n\n\n\n\n\nRunning the application stack\n\nNow that we have our docker-compose.yml file, we can start it up!\n\nMake sure no other copies of the app/db are running first (docker ps and docker rm -f &lt;container-ids&gt;)\nStartup the application using the docker compose up command. We‚Äôll add the -d flag to run everything in the background. You;ll notic that the volume was created as well a network! By default, Docker compose automatically creates a network specifically for the application stack (which is why we didn‚Äôt define one in the compose file, like we did for connecting App & MySQL container previously.)\n\n\n\n\nCode\ndocker compose up -d\n\n\n\nLet‚Äôs look at the logs using the docker compose logs -f command. You‚Äôll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag ‚Äúfollows‚Äù the log, so will give you live output as it‚Äôs generated.\n\nIf you have run the command already, you‚Äôll see output that looks like this:\nmysql_1  | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections.\n mysql_1  | Version: '8.0.31'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)\n app_1    | Connected to mysql db at host mysql\n app_1    | Listening on port 3000\n\nThe service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker compose logs -f app).\nAt this point, you should be able to open your app and see it running. And hey! We‚Äôre down to a single command!\n\n\nTear it all down\n\nWhen you‚Äôre ready to tear it all down, simply run docker compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed.\nRemoving Volumes\n\nBy default, named volumes in your compose file are NOT removed when running docker compose down. If you want to remove the volumes, you will need to add the ‚Äìvolumes flag.\nThe Docker Dashboard does not remove volumes when you delete the app stack.\n\nOnce torn down, you can switch to another project, run docker compose up and be ready to contribute to that project! It really doesn‚Äôt get much simpler than that!\n\nIn this section, you learned about Docker Compose and how it helps you dramatically simplify the defining and sharing of multi-service applications. You created a Compose file by translating the commands you were using into the appropriate compose format. With this we can end the tutorial, but yup don‚Äôt forget to keep up with the dockers and docking from the official guides."
  },
  {
    "objectID": "Self_Instructions.html",
    "href": "Self_Instructions.html",
    "title": "References",
    "section": "",
    "text": "Read Later\n\nGithub_Actions\nGithub_Pages\n\n\n\nPublishing Content\n\nCreate a gh-pages branch\n\ngit checkout ‚Äìorphan gh-pages #Creation of new branch\ngit reset ‚Äìhard # make sure you‚Äôve committed changes before running this! # This makes the gh-pages branch as the current one.\ngit commit ‚Äìallow-empty -m ‚ÄúInitialising gh-pages branch‚Äù\ngit push  gh-pages #Remember for current branch the repo_name is my_repo, but by default the name is origin.\nFor publishing to github pages follow the site:https://quarto.org/docs/publishing/github-pages.html\n\nOnce this is done do gh-configuration on github repo.\n\ngithub-pages_config\n\nPublish first to Quarto Pub\n\nAdd the following content to _publish.yml in the main directory\nRun the command quarto publish with Quarto-Pub and follow the authorization steps.\n\nPublish to gh-pages\n\nCreate a publish.yml file in .github/workflows directory with the following content\nAdd _site and .quarto to .gitignore file to ignore those folders staging.\nTo render codes, setup a virtual env and update the publish.yml.\nRef: Executing Codes, Quarto setting up virtual_envs\n\n\n\nimgs\n\n  \n\n\nYAML Codes\n\n- source: project\n    quarto-pub:\n        - id: 1a303cb3-ce18-42f5-85e8-873316e2d3d8\n        url: 'https://quartopub.com/sites/yuvraj-dhepe/data-science-blog'\n\non:\n  workflow_dispatch:\n  push:\n    branches: master\n\nname: Quarto Publish on Github Pages\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n        with:\n          node-version: 16.x\n        # Update Node.js version to 16\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2 #using quarto-development github pages to use actions for gh-pages posting. \n        with:\n          # to install LaTeX to build PDF book\n          tinytex: true\n\n      - name: Render and Publish #Publishing to github pages and rendering.\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions so no need to add it again in secrets"
  }
]