[
  {
    "objectID": "Self_Instructions.html",
    "href": "Self_Instructions.html",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "",
    "text": "References\n\nRead Later\n\nGithub_Actions\nGithub_Pages\n\n\n\n\nPublishing Content\n\nCreate a gh-pages branch\n\ngit checkout ‚Äìorphan gh-pages #Creation of new branch\ngit reset ‚Äìhard # make sure you‚Äôve committed changes before running this! # This makes the gh-pages branch as the current one.\ngit commit ‚Äìallow-empty -m ‚ÄúInitialising gh-pages branch‚Äù\ngit push  gh-pages #Remember for current branch the repo_name is my_repo, but by default the name is origin.\nFor publishing to github pages follow the site:https://quarto.org/docs/publishing/github-pages.html\n\nOnce this is done do gh-configuration on github repo.\n\ngithub-pages_config\n\nPublish first to Quarto Pub\n\nAdd the following content to _publish.yml in the main directory\nRun the command quarto publish with Quarto-Pub and follow the authorization steps.\n\nPublish to gh-pages\n\nCreate a publish.yml file in .github/workflows directory with the following content\nAdd _site and .quarto to .gitignore file to ignore those folders staging.\nTo render codes, setup a virtual env and update the publish.yml.\nRef: Executing Codes, Quarto setting up virtual_envs\n\n\n\nimgs\n\n  \n\n\nYAML Codes\n\n- source: project\n    quarto-pub:\n        - id: 1a303cb3-ce18-42f5-85e8-873316e2d3d8\n        url: 'https://quartopub.com/sites/yuvraj-dhepe/data-science-blog'\n\non:\n  workflow_dispatch:\n  push:\n    branches: master\n\nname: Quarto Publish on Github Pages\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n        with:\n          node-version: 16.x\n        # Update Node.js version to 16\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2 #using quarto-development github pages to use actions for gh-pages posting. \n        with:\n          # to install LaTeX to build PDF book\n          tinytex: true\n\n      - name: Render and Publish #Publishing to github pages and rendering.\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions so no need to add it again in secrets"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html",
    "href": "docs/projects/B1_Sales_Insights.html",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "",
    "text": "AtliQ Hardware supplies computer hardware and peripherals to various clients across India and has a head office in Delhi and regional offices throughout India.\nBhavin Patel, the sales director of the company, is facing challenges in tracking the sales in the dynamically growing market and getting insights into his business.\nThe regional managers tend to give verbal feedback that paints a rosy picture and provide complex Excel files, making it challenging for Bhavin to get a clear picture of the business.\nBhavin wants simple and digestible insights, such as the top five customers, the weakest regions, and the year-to-date revenue, etc,. to make data-driven decisions for the business.\nThe ultimate goal is to improve the declining sales and make informed business decisions based on real-time data analytics.\nPowerBI can provide a solution to visualize the data and present simple and actionable insights for Bhavin.\n\nP.S. The problem statement is virtual and doesn‚Äôt relate to any real-world entity, any such occurence is just a matter of coincidence."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "href": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Project Planning and Data Discovery",
    "text": "Project Planning and Data Discovery\n\nProject planning using AIMS grid.\n\nBhavin patel who is a sales director of at Atliq hardware realizes the pains of getting sales insights and thinks of having a data analytical solution for this problem.\nHe calls a couple of people like an IT head or a head of data department and schedules a meeting on how exactly we should do this data analytics project.\nTogether with IT head, he also gathers other stake holders in the room together and they brainstorm on project planning using AIMS grid.\n\nAIMS grid\n\nAIMS grid is a project management tool having four components.\n\nFirst component is Purpose\n\nDetermining what is the pain point and what needs to be done to resolve the pain point\n\nSecond component is the Stakeholders\n\nIn this project who all will be involved\nThe main stakeholders involved for this data analytical projects are the marketing team ,the sales team who is facing these issues to do proper sales analysis\nIT team (The Falcons) a group of software engineers that AtliQ hardware has who manages all the software requirements of Atliq\nThe data analytics (The Data Masters) team which is inhouse\n\nThird component is the end result i.e., once the project is over what needs to be achieved\n\nFor the data analytics project the end result is to have a power bi dashboard something that our sales director or even Regional Managers can go and look into and it gives you a real-time information on the sales numbers\n\nFourth component is the success criteria after the project is over, how do we define that the project was successful\n\nIn our project scenario the success criteria would be, we want the costs to go down by 10% on management of these sales data\nWe want to utilize the saved business time by the use of dashboard to take data driven descisions and increase the growth of company by let‚Äôs say 10%\nDashboards uncovering sales order insights with latest data available\nSales team able to take better decisions and prove 10% cost savings of total spend\nSales Analysts stop data gathering manually (merging excel files) in order to save business time and reinvest it value added activity\n\n\n\nOther Important points\n\nFalcons team, manages the software of Atliq Sales Management which is keeping track of all the sales number so whenever they sell any computer or any hard days in any local region this software is printing the invoice so it has all the records stored in a MySQL database.\nData Masters team will reach out to Falcons and to use the AtliQ SQL database because this is the database which has all the records that we need for our analytics and what we‚Äôll do is we‚Äôll integrate MySQL. We will use MySQL as a source in our power bi tool and we will build dashboard on top of it.\n\nReal World Scenarios\n\nIn real world direct access of data from IT team is not allowed cause if the data volume is high we want to make sure that MySQL database is not affected by the queries that Falcons are doing in your PowerBi, so many times companies, have a Data Warehouse as part of the Data Ecosystem.\nMySQL which is also known as OLTP which is online transaction processing system it is a very critical system and a company cannot afford for that system to go down otherwise the regular sales operations gets hampered.\nFor any business analytics is important but then it‚Äôs a secondary thing so what companies do is they pull the data from OLTP which is MySQL in our case they do all the transformation which is also called ETL which means extract transform and load and after doing that transformation they store the data in a data warehouse.\nThere are various types of Data Warehouse management tools like Snowflake and there are various ways to transform the data like using Pandas or other data processing libraries.\nMost companies working on huge data analytical projects have another in-house teams who manage the data warehouse known as Data Engineers (The Data Miners).\n\nConclusion & Data Discovery\n\nAssuming our data is not that big, the data is simply taken from the Falcons, by our Data Masters.\nThe Data Masters will plug PowerBI directly to SQL database and build the necessary analytical dashboard. Also some times data analysts will spend their time to capture information for analytics, which might not be available in the organization at all. But for this project we will not do that.\nOnce the Data Masters have the data, they do data cleaning (data wrangling) and data merging (data munging), all this can be done via specialized software. But our data being simple we will do it via PowerBi itself."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "href": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Data Cleaning and Data Merging or Wrangling",
    "text": "Data Cleaning and Data Merging or Wrangling\n\nLoad the data in SQL taken from the Falcons team\n\nFind Basic Insights about the data in SQL.\nFind out few inconsistencies in data in SQL itself and make a note to remove them in PowerBI later.\n\nDo the ETL in PowerBi\n\nFirst Load the data\nDo the data modelling, i.e., link the tables well, along with their relationships.\nStart data transformations on the data to make it good enough for the data analysis.\nOnce the ETL is done, now start building the dashboards\n\n\n\nBasic Insights from DB\n\n\nDB_Schema\n\n\nCode\nfrom IPython import display\ndisplay.Image(\"./images/DB_Schema.jpg\")\n# Can use ![alternative text](path-to-image) but it works out of zsh\n\n\n\n\n\n\n\nDB_Queries\n\nCustomers Table\n\nGet the total number of customers doing business with AtliQ hardwares\n\nInsights: There are total of 38 customers doing business with AtliQ hardwares\n\nSELECT count(*) FROM sales.customers; \nGet count of customers according to their types\n\nInsights: Only 2 customer types i.e.¬†of Brick Mortar and E-Commerce\n\nSELECT count(cs.customer_code) as total_customer_per_type, cs.customer_type \nFROM sales.CUSTOMERS as cs \nGROUP BY cs.customer_type; \nChecking unique customers in the customer category.\n\nInsights: All the 38 customers are unique\n\nSELECT COUNT(DISTINCT tb.custmer_name) as total_unique_customer_name_count\nFROM \n( \n# Seeing whether each customer_code belongs to unique customer name or not. :- It does.\nSELECT cs.custmer_name, COUNT(cs.customer_code) as total_unique_customer_names\nFROM sales.customers as cs\nGROUP BY cs.custmer_name\n) as tb;\n\n\n\nDate Table\n\nInsights: Total unique dates : 1126 and that is the total dates in the table itself.\nSELECT (count(distinct dt.date)) as total_unique_dates  \nFROM sales.date as dt;\nInsight: Seeing the trend of number of transactions each year\nSELECT dt.year, count(dt.year) as total_transactions_in_each_year \nFROM sales.date as dt\nGROUP BY dt.year;\nInsights: We have on average 30 transactions per month in each year\nSELECT \n    dt.month_name,dt.year,count(dt.month_name) as transactions_per_month_per_year\nFROM\n    sales.date AS dt\nGROUP BY dt.year, dt.month_name;\n\n\n\nMarket table\n\nInsights: Total of 17 markets each having it‚Äôs own unique identifier code\nSELECT count(mk.markets_name) \nFROM sales.markets as mk;\nInconsistency:\n\nThere are only 2 international sales in the data, so it‚Äôs better to not consider them as we don‚Äôt have much information about them now.\n\n\n\n\nTransactions Table\n\nInsights: There are a total of 150283 transaction.\nSELECT count(*) FROM sales.transactions;\nInsights: The total_sales_qty: 2444415 and reverified the same via net_sales_per_month_per_year\nSELECT SUM(tr.sales_qty) as net_sales_in_all\nFROM sales.transactions as tr;\nInsights: Finding the different type of currencies used while doing the transactions.\nSELECT distinct(tr.currency) ,count(*) as number_of_transactions_currency_wise\nFROM sales.transactions as tr\nGROUP BY tr.currency;\nInconsistencies:\n\nFew transactions are in USD, they need to be converted to INR\nThe currency column, has an inherent error to show INR and USD 2 times when grouped by currency. The error was because entry in currency field was INR USDhile creating the data.\n\nRemoved inconsistency by formating the table well.\n\n\n\n\n\nProducts Table\n\nInsights: Finding the distribution of product type\nSELECT pr.product_type, count(pr.product_code) AS product_count_forthis_type\nFROM sales.products as pr\nGROUP BY pr.product_type;\nInsights: Every product code is unique\nSELECT count(distinct pr.product_code) \nFROM sales.products as pr;\nSELECT count(*) FROM sales.products;\nSELECT * FROM sales.products;\n\n\n\n\nGeneral Queries\n\nInsights: Getting revenues per year per product\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Getting revenues per year per product in Chennaicustomers\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020 and st.market_code = \"Mark001\"\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Found out which market makes the most of the sales and which one the least.\nSELECT mk.markets_code, sum(tr.sales_qty) as net_sales_per_market, mk.markets_name\nFROM sales.transactions as tr\nJOIN sales.markets as mk\nON tr.market_code = mk.markets_code\nGROUP BY mk.markets_code\nORDER BY net_sales_per_market DESC;\nInsights: Finding which year has made the most of the transactions and in which month\nSELECT sum(net_sales_per_month_per_year)\nFROM \n( \nSELECT dt.year, dt.month_name, sum(tr.sales_qty) AS net_sales_per_month_per_year\nFROM sales.transactions as tr\nJOIN sales.date as dt\nON dt.date = tr.order_date\nGROUP BY dt.year, dt.month_name\nORDER BY net_sales_per_month_per_year\n) AS tb;\nInsights: Finding sales_qty per product, looking at it seems like own brand is in more demand\nSELECT pr.product_code, sum(tr.sales_qty) as net_sales_per_product, pr.product_type\nFROM sales.transactions as tr\nJOIN sales.products as pr\nON pr.product_code = tr.product_code\nGROUP BY pr.product_code\nORDER BY net_sales_per_product DESC;\n\n\n\n\nETL\n\n\nDid a bit of Data Modelling, linking tables well of the DB\nRemoved the inconsistencies in the PowerBI from Market and Transaction table"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "href": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Dashboard Created in PowerBI",
    "text": "Dashboard Created in PowerBI\n\nP.S. Viewing the dashboard requires PowerBI professional account.\n\nIf the viewer doesn‚Äôt have one, kindly use the pdf link to view overall analytical dashboard\n\n\n\n\n\n\n\n\nMajor Insights from the Dashboard\n\n\nKey Insights Dashboard\n\nDelhi NCR seems to give major Revenue and Sales Qty followed by Mumbai, Ahemdabad and other city ares.\nBrick n Mortar Seems to contribute to majority of Revenue\nProduct revenue has an error, we can‚Äôt see the major revenue generating product code\nOver the years the revenue trend seems to decline\nRevenue seems to increase in Q3 n Q4 generally\n\nProfit Analysis Dashboard\n\nThough revenue contribution of Delhi n major city markets is more, the profit contribution is not much high for them.\n\nAction points for AtliQ:\n\n\nFocus more on the major profit% generating cities\n\n\nFind why the profit % is less in major revenue generating cities\n\n\n\nFocus on the customers which provide more Profit Margin Contribution by encouraging them like by giving them discounts\n\nPeformance Insights Dashboard\n\nAnalyze the data by breakdown to zones, markets and customers\nObserve the zones that don‚Äôt fulfill the profit margin\nObserve Previous year revenue trend as a comparision with current year revenue trend"
  },
  {
    "objectID": "index.html#recent-blogs",
    "href": "index.html#recent-blogs",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "üìÆ Recent Blogs",
    "text": "üìÆ Recent Blogs\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nBlog 1\n\n\n\n\n2/9/22\n\n\nBlog 2\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-projects",
    "href": "index.html#latest-projects",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": " Latest Projects",
    "text": "Latest Projects\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nAtliQ Hardware Sales Insights\n\n\n\n\n2/9/22\n\n\nProject 2\n\n\n\n\n\nNo matching items"
  }
]