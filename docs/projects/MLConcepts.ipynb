{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e6ec2c-f166-494a-8ce2-dd46838968d4",
   "metadata": {},
   "source": [
    "## Machine Learning Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e9947-d5fe-4cb9-8c83-aa0cbb9486e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7299c65-14b9-4599-a6cb-d55d1f3c1d6a",
   "metadata": {},
   "source": [
    "#### Logistic Regression [StatQuest Vid](https://youtu.be/yIYKR4sgzI8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac67b9e-0ebd-4f8b-b6f7-dc8aaa52455e",
   "metadata": {},
   "source": [
    "- Logistic regression needs an adequate sample to represent values across all the response categories. Without a larger, representative sample, the model may not have sufficient statistical power to detect a significant effect.\n",
    "- Logistic regression can also be prone to overfitting, particularly when there is a high number of predictor variables within the model. Regularization is typically used to penalize parameters large coefficients when the model suffers from high dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744239cb-005d-415b-8e94-0abaf7d8ab44",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad00c4-c93a-44fb-bc7d-434756940d5e",
   "metadata": {},
   "source": [
    "- Always remember, that the precision is given by considering that particular class as 1.\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "- For example if we are calculating precision for Normal class, then normal is class 1 and pneumonia is class 0 and vice-versa. So basically the class which is 1 is the positive wali. The class which is taken positive depends on the problem as well. *In classification report of any model, given y_true and y_pred, the classes are mentioned to the left*\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- **Precision (calculated wrt class)**: Jitno ko positive bol raha hein unme se actual positive kitne hein. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- **Specificity(calculated wrt class)**: Sare Actual negative mein se sahi kitne yaad hein. The specificity is intuitively the ability of the classifier to find all the -ve samples. (1-Sensitivity gives the FPR)\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "Specificity = \\frac{TN}{TN+FP}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- **Recall or Sensitivity or TPR (calculated wrt class)**: Sare actual positives mein se sahi kitne yaad hein. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "Recall = \\frac{TP}{TP+FN}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "- **Accuracy(calculated wrt model)**: Sab me se sahi kitno ko pakda.\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- **$F_{\\beta}\\text{score}$** : (calculated wrt class and problem)$$\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "F_{\\beta} = (1+\\beta^2)\\frac{*Precision*Recall}{\\beta^2*Precision+Recall}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "- F1-Score is used as equal weight for precision and recall, while F0.5 is biased towards precision. \n",
    "  - For pneumonia and normal problem, considering Pneumonia as positive class.\n",
    "  - Better situation is to have FP from model, rather than FN. So, we have to give preference Recall where we minimize the FN tendency. Thus we will use F2 Score to give more preference to recall than precision.\n",
    "\n",
    "$$\n",
    "F2= \\frac{5*Precision*Recall}{4*Precision+Recall}\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "F1= \\frac{2*Precision*Recall}{Precision+Recall}= \\frac{2*TP}{2*TP+FP+FN}\\\\\n",
    "$$\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "F0.5=\\frac{1.25*Precision*Recall}{0.25*Precision+Recall}\\\\\n",
    "$$\n",
    "\n",
    "- **Macro Average (macro avg)**:\n",
    "    - The macro average calculates the average metric value for each class independently and then takes the unweighted mean of those values.\n",
    "    - It treats each class equally and gives equal importance to all classes, regardless of their support (the number of instances in each class).\n",
    "    - Macro average is useful when you want to evaluate the overall performance of the model without considering class imbalance.\n",
    "\n",
    "- **Weighted Average (weighted avg)**:\n",
    "    - The weighted average calculates the average metric value for each class independently and then takes the weighted mean of those values, where the weight is proportional to the support of each class.\n",
    "    - It gives more importance to classes with higher support and less importance to classes with lower support.\n",
    "    - Weighted average is useful when you want to evaluate the performance of the model while considering class imbalance. It reflects the overall performance taking into account the distribution of instances across different classes.\n",
    "    - In the classification report you provided, the precision, recall, and F1-score are reported for each class (1, 2, 3, and 4), as well as the overall accuracy. The macro average and weighted average are additional summary metrics that provide a single value representing the average performance across all classes.\n",
    "    \n",
    "- **Interpretting ROC Curve**: [StatQuestVideo](https://www.youtube.com/watch?v=4jRBRDbJemM)\n",
    "    - ROC curves make it easy to identify the best threshold for making a decision\n",
    "    - If AUC for an ROC curve is higher than the other, we would use that model in reality, but it also depends on the problem at hand like in terms of false positives are good or false negatives\n",
    "    - Also interpret the curve in terms of the axes, for example a point at 1,1 in ROC means we are classifying all the positive ones well, but we even have high false -ves, so if we move parallely backwards, that threshold is good, since our true positive rate keeps high and false postive rate is decreasing\n",
    "- **AUC**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9eaa4-5479-49e5-a418-b8e5165b3b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
