<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Unit 2 Learnings from Hugging Face RL Course">

<title>Data Science Blog - Yuvraj Dhepe - RL Unit 2: Introduction to Q Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/lotus.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Data Science Blog - Yuvraj Dhepe - RL Unit 2: Introduction to Q Learning">
<meta property="og:description" content="Unit 2 Learnings from Hugging Face RL Course">
<meta property="og:image" content="https://github.com/Yuvraj-Dhepe/DataBlog_V1/docs/projects/abc/images/RL2_QLearning.jpg">
<meta property="og:site-name" content="Data Science Blog - Yuvraj Dhepe">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Hello There</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/notes/index.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Yuvraj-Dhepe" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-3-deep-q-learning" id="toc-chapter-3-deep-q-learning" class="nav-link active" data-scroll-target="#chapter-3-deep-q-learning">Chapter 3: Deep Q-Learning</a>
  <ul class="collapse">
  <li><a href="#from-q-learning-to-deep-q-learning" id="toc-from-q-learning-to-deep-q-learning" class="nav-link" data-scroll-target="#from-q-learning-to-deep-q-learning">From Q-Learning to Deep Q-Learning</a></li>
  <li><a href="#the-deep-q-network-dqn" id="toc-the-deep-q-network-dqn" class="nav-link" data-scroll-target="#the-deep-q-network-dqn">The Deep Q-Network (DQN)</a></li>
  <li><a href="#the-deep-q-learning-algorithm" id="toc-the-deep-q-learning-algorithm" class="nav-link" data-scroll-target="#the-deep-q-learning-algorithm">The Deep Q-Learning Algorithm</a></li>
  <li><a href="#double-dqn" id="toc-double-dqn" class="nav-link" data-scroll-target="#double-dqn">Double DQN</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/quarto-dev/quarto-demo/blob/main/docs/projects/abc/RL3_DeepQ-Learning.ipynb" class="toc-action">Edit this page</a></p><p><a href="https://github.com/quarto-dev/quarto-demo/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">RL Unit 2: Introduction to Q Learning</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Re-inforcement Learning</div>
    <div class="quarto-category">Regression Project</div>
  </div>
  </div>

<div>
  <div class="description">
    Unit 2 Learnings from Hugging Face RL Course
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="chapter-3-deep-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3-deep-q-learning">Chapter 3: Deep Q-Learning</h2>
<ul>
<li>Back in previous Q-learning unit:
<ul>
<li>We implemented a Q-learning algorithm from scratch and trained it on Taxi-v3 and FrozenLake-v1 env’s</li>
<li>We got excellent results with this simple algorithm, but these environments were relatively simple because the state space was discrete and small</li>
<li>However, we need to work on a bit complex problems as well, such as Atari games which has <span class="math inline">\(10^9\)</span> to <span class="math inline">\(10^{11}\)</span> states</li>
<li>In such huge state space, producing and updating a Q-table can become ineffective</li>
<li>Thus we will use Deep Q-Learning, uses Neural Network that takes a state and approximates Q-values for each action based on that state</li>
<li>In this unit, we will train an agent to play Space Invaders and other Atari environments using RL-Zoo, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.</li>
</ul></li>
</ul>
<section id="from-q-learning-to-deep-q-learning" class="level3">
<h3 class="anchored" data-anchor-id="from-q-learning-to-deep-q-learning">From Q-Learning to Deep Q-Learning</h3>
<ul>
<li>Q-Learning is an algorithm we use to train our Q-function, an action value function that determines the value of being at a particular state and taking a specific action at that state.</li>
<li>Q here stands for quality of that action at that state, internally Q-function, is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Q-table serves as the memory of our Q-function</li>
<li>However, Q-learning is a tabular method, this is fine for small state space, but if state space becomes large Q-learning is not scalable to such problems.</li>
<li>For ex. Atari environments have an observation space with a shape of (210,160,3)* containing values of 0 to 255 this gives us N = <span class="math inline">\(256^{210x160x3}\)</span> possible observations (for comparision we have approx <span class="math inline">\(10^{80}\)</span> atoms in the observable universe)</li>
<li>So overall we can say that we will have a Q-table of N by A, and that’s again huge</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg" width="600" height="600"></p>
<ul>
<li>Thus we can see that the state space is gigantic, due to this, creating and updating a Q-table for that environment would not be efficient.</li>
<li>In this case, the best idea is to approximate Q-values using a <em>parameterized Q-function <span class="math inline">\(Q_{\theta}(s,a)\)</span></em></li>
<li>We will use a neural network that approximates Q-values for a given state, for each possible action at that state</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg" width="600," height="600"></p>
</section>
<section id="the-deep-q-network-dqn" class="level3">
<h3 class="anchored" data-anchor-id="the-deep-q-network-dqn">The Deep Q-Network (DQN)</h3>
<ul>
<li>This is the architecture of our Deep Q-Learning network:</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg" width="600" height="600"></p>
<ul>
<li>As input, we take a stack of 4 frames passed through the network as a state and output a vector of Q-values for each possible action at that state.</li>
<li>Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take</li>
<li>When the Neural Network is initialized, the Q-value estimation is terrible, but during training, our Deep Q-Network agent will associate a situation with the appropriate action and learn to play the game well.</li>
</ul>
<section id="preprocessing-the-input-and-temporal-limitation" class="level4">
<h4 class="anchored" data-anchor-id="preprocessing-the-input-and-temporal-limitation">Preprocessing the input and temporal limitation</h4>
<ul>
<li>We need to preprocess the input, it’s an essential step since we would like to reduce the complexity of our state to reduce the computation time needed for training</li>
<li>To achieve this, we reduce the state space to 84x84 and grayscale it. We can do this since the colors in Atari environments, don’t add important information. This is a big improvement since we reduce our three color channels (RGB) to 1.</li>
<li>We can also crop a part of this scren in some games if it doesn’t add any crucial information. Then we stack 4 frames together.</li>
<li>This stacking is necessary since it helps us to handle the problem of temporal limitation. Basically having a single frame doesn’t give us any idea about motion, however if we stack more frames we capture temporal information.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/preprocessing.jpg" width="600" height="600"><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation.jpg" width="600" height="600"></p>
<ul>
<li>These stacked frames are processed by 3 convolutional layers. These layers allow us to capture and exploit spatial relationships in images. But also, because the frames are stacked together, we can exploit some temporal properties across those frames. Finally, we have a couple of fully connected layers that output a Q-value for each possible action at that state.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation-2.jpg" width="600" height="600"><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg" width="600" height="600"></p>
<ul>
<li><strong>So we can basically see that Deep Q-learning given a state, uses a neural network to approximate, the different Q-values for each possible action at that state.</strong></li>
</ul>
</section>
</section>
<section id="the-deep-q-learning-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-deep-q-learning-algorithm">The Deep Q-Learning Algorithm</h3>
<ul>
<li>Now we know Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state (value-function estimation)</li>
<li>The main difference between Q-Learning and Deep Q-Learning is that during <strong>training phase, instead of updating the Q-value of a state-action pair directly</strong> as we have done with Q-Learning, in Deep Q-Learning, we create a <strong>loss function that compares our Q-value prediction and the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better</strong> <em>So we need to have Q-targets and Q-value predictions</em></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg" height="600" width="600"><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg" height="600" width="600"></p>
<ul>
<li>The Deep Q-Learning training algorithm has 2 phases:
<ul>
<li>Sampling: we perform actions and store the observed experience tuples in a replay memory</li>
<li>Training: Select a small batch of tuples randomly and learn from this batch using a gradient descent update step</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg" width="600" height="600"></p>
<ul>
<li><p>This is not the only difference compared with Q-Learning.</p></li>
<li><p>Deep Q-Learning training <strong>might suffer from instability,</strong> mainly because of combining a non-linear Q-value function (Neural Network) and bootstrapping (when we update targets with existing estimates and not an actual complete return)</p></li>
<li><p>To help us stabilize the training, we implement 3 different solutions:</p>
<ul>
<li>Experience Replay to make more efficient use of experiences</li>
<li>Fixed Q-Target to stabilize the training</li>
<li>Double Deep Q-Learning, to handle the problem of the overestimation of Q-values</li>
</ul></li>
<li><p>Let’s go through them!</p></li>
</ul>
<section id="experience-replay-to-make-more-efficient-use-of-experiences" class="level4">
<h4 class="anchored" data-anchor-id="experience-replay-to-make-more-efficient-use-of-experiences">Experience Replay to make more efficient use of experiences</h4>
<ul>
<li>Why to create a replay memory?
<ul>
<li>Experience Replay in Deep Q-Learning has 2 functions:
<ul>
<li><strong>1. Make more efficient use of the experiences during the training.</strong> Usually, in online re-inforcement learning, the agent interacts with the environment, gets experience (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient
<ul>
<li>Experience replay helps by using the experiences of the training more efficiently. We use a <strong>replay buffer</strong> that saves experience samples <strong>that we can reuse during the training</strong></li>
<li>This allows the agent to learn from the <strong>same experiences multiple times</strong>.</li>
</ul></li>
<li><strong>2. Avoid forgetting previous experiences and reduce the correlation between experiences.</strong>
<ul>
<li><p>The problem we get if we give sequential samples of experiences to our neural network is that it <strong>tends to forget the previous experiences as it gets new experiences.</strong> For instance, if the agent is in the first level and then in the second, which is different, it can forget how to behave and play in the first level.</p></li>
<li><p>The solution is to create a Replay Buffer that stores experience tuples while interacting with the environment and then sample a small batch of tuples. This prevents <strong>the network from only learning about what it has done immediately before</strong></p></li>
</ul></li>
</ul></li>
<li>Experience replay also has other benefits. By randomly sampling the experiences, we remove correlation in the observation sequences and avoid action values from <strong>Oscilating or Diverging catastrophically</strong></li>
<li>In Deep Q-Learning psuedocode, we initialize a replay memory buffer D with capacity N (N is a hyperparameter that you can define). We then store experiences in the memory and sample a batch of experiences to feed the Deep Q-Network during the training phase.</li>
</ul></li>
<li><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay-pseudocode.jpg" width="600" height="600"></li>
</ul>
</section>
<section id="fixed-q-target-to-stabilize-the-training" class="level4">
<h4 class="anchored" data-anchor-id="fixed-q-target-to-stabilize-the-training">Fixed Q-Target to stabilize the training</h4>
<ul>
<li><p>When we want to calculate the TD error (aka the loss), we calculate the <strong>difference between the TD target (Q-Target) and the current Q-value (estimation of Q)</strong></p></li>
<li><p>But we don’t have any <strong>idea of the real TD target.</strong> We need to estimate it. Using the Bellman equation, we saw that the TD target is just the reward of taking that action at that state plus the discounted highest Q-value for the next state.</p></li>
<li><p>However, the problem is that we are using the same parameters (weights) for estimating the TD target and the Q-value. Consequently, there is a <strong>significant correlation</strong> between the TD target and the parameters we are changing. Therefore, <strong>at every step of training, both our Q-values and the target values shift</strong>. We’re getting closer to our target, but the target is also moving. It’s like chasing a moving target! This can lead to significant oscillation in training.</p>
<p>For ex.</p>
<ul>
<li>It’s like if you were a cowboy (the Q estimation) and you wanted to catch a cow (the Q-target). Your goal is to get closer (reduce the error).</li>
<li>At each time step, you’re trying to approach the cow, which also moves at each time step (because you move the same parameters)</li>
<li>This leads to a bizzare path of chasing (a significant oscillating in training)</li>
<li>Instead, what we see in the psuedo-code is that we:
<ul>
<li>Use a separate network with fixed parameters for estimating the TD Target</li>
<li>Copy the parameters from our Deep Q-Network every C steps to update the target network</li>
</ul></li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/fixed-q-target-pseudocode.jpg" width="600" height="600"></p>
</section>
</section>
<section id="double-dqn" class="level3">
<h3 class="anchored" data-anchor-id="double-dqn">Double DQN</h3>
<ul>
<li><p>Double DQN’s, or Double Q-Learning neural networks, were introduced by Hado van Hasselt. This method <strong>handles the problem of the overestimation of Q-values</strong></p></li>
<li><p>To understand this problem, remember how we can calculate the TD Target: <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg" width="600" height="600"></p></li>
<li><p>We face a simple problem by calculating the TD target: How are we sure that <strong>the best action for the next state is the action with the highest Q-value?</strong></p></li>
<li><p>We know that the accuracy of Q-values depend on what action we tried and what neighboring states we explored</p></li>
<li><p>Consequently, we don’t have enough information about the best action to take at the beginning of the training. Therefore, taking the maximum Q-value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly <strong>given a higher Q value than the optimal best action, the learning will be complicated</strong></p></li>
<li><p>The solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q-value generation. We:</p>
<ul>
<li>Use our DQN network to select the best action to take for the next state (the action with the highest Q-value)</li>
<li>Use our Target network to calculate the target Q-value of taking that action at the next state</li>
</ul></li>
<li><p>Therefore, Double DQN helps us reduce the overestimation of Q-values and, as a consequence, helps us train faster and with stable learning.</p></li>
<li><p>Since these 3 improvements in Deep Q-Learning, many more have been added, such as Prioritized Experience Replay and Dueling Deep Q-Learning.</p></li>
</ul>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li><p>Tabular Method: Type of problem in which the state and action spaces are small enough to approximate value functions to be represented as arrays and tables. Q-learning is an example of tabular method since a table is used to represent the value for different state-action pairs.</p></li>
<li><p>Deep Q-Learning: Method that trains a neural network to approximate, given a state, the different Q-values for each possible action at that state. It is used to solve problems when observational space is too big to apply a tabular Q-Learning approach.</p></li>
<li><p>Temporal Limitation is a difficulty presented when the environment state is represented by frames. A frame by itself does not provide temporal information. In order to obtain temporal information, we need to stack a number of frames together.</p></li>
<li><p>Phases of Deep Q-Learning:</p>
<ul>
<li>Sampling: Actions are performed, and observed experience tuples are stored in a replay memory.</li>
<li>Training: Batches of tuples are selected randomly and the neural network updates its weights using gradient descent.</li>
</ul></li>
<li><p>Solutions to stabilize Deep Q-Learning:</p>
<ul>
<li>Experience Replay: A replay memory is created to save experiences samples that can be reused during training. This allows the agent to learn from the same experiences multiple times. Also, it helps the agent avoid forgetting previous experiences as it gets new ones.</li>
<li>Random sampling from replay buffer allows to remove correlation in the observation sequences and prevents action values from oscillating or diverging catastrophically.</li>
<li>Fixed Q-Target: In order to calculate the Q-Target we need to estimate the discounted optimal Q-value of the next state by using Bellman equation. The problem is that the same network weights are used to calculate the Q-Target and the Q-value. This means that everytime we are modifying the Q-value, the Q-Target also moves with it. To avoid this issue, a separate network with fixed parameters is used for estimating the Temporal Difference Target. The target network is updated by copying parameters from our Deep Q-Network after certain C steps.</li>
<li>Double DQN: Method to handle overestimation of Q-Values. This solution uses two networks to decouple the action selection from the target Value generation:
<ul>
<li>DQN Network to select the best action to take for the next state (the action with the highest Q-Value)</li>
<li>Target Network to calculate the target Q-Value of taking that action at the next state. This approach reduces the Q-Values overestimation, it helps to train faster and have more stable learning.</li>
</ul></li>
</ul></li>
</ul>
<div class="cell" data-tags="[]" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> {<span class="dv">1</span>:<span class="st">'A'</span>,<span class="dv">2</span>:<span class="st">'B'</span>,<span class="dv">3</span>:<span class="st">'C'</span>}</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>a.update({<span class="dv">1</span>:<span class="st">'D'</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-tags="[]" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>{1: 'D', 2: 'B', 3: 'C'}</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="Yuvraj-Dhepe/DataBlog_V1" data-repo-id="R_kgDOJMncgA" data-category="Announcements" data-category-id="DIC_kwDOJMncgM4CVqbZ" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light_protanopia" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">This page is built with ❤️ and <a href="https://quarto.org">Quarto</a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/yuvraj-shivaji-dhepe-22974919a/">
      <i class="bi bi-linkedin" role="img" aria-label="My LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Yuvraj-Dhepe">
      <i class="bi bi-github" role="img" aria-label="My Github">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/@yuvrajdhepe">
      <i class="bi bi-medium" role="img" aria-label="My Medium">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>