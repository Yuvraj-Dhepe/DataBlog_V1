{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59037183-1204-4a34-8152-0d5ade5aeb8d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RL Unit 2: Introduction to Q Learning\"\n",
    "description: \"Unit 2 Learnings from Hugging Face RL Course\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "render-on-save: true\n",
    "execute:\n",
    "    eval: false\n",
    "    echo: true\n",
    "jupyter: python3\n",
    "output:\n",
    "  quarto::html_document:\n",
    "    self_contained: false\n",
    "    keep_md: false\n",
    "\n",
    "categories:\n",
    "    - Re-inforcement Learning\n",
    "    - Regression Project\n",
    "image: ./images/RL2_QLearning.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8f618-3c17-4ecc-aad2-91852465d340",
   "metadata": {},
   "source": [
    "<img src = \"\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7a651-5fb4-4e8f-99ca-da5d0a903e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chapter 3: Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c2209-4b10-4b0b-b4fc-3c2ccd3ca845",
   "metadata": {},
   "source": [
    "- Back in previous Q-learning unit:\n",
    "    - We implemented a Q-learning algorithm from scratch and trained it on Taxi-v3 and FrozenLake-v1 env's\n",
    "    - We got excellent results with this simple algorithm, but these environments were relatively simple because the state space was discrete and small\n",
    "    - However, we need to work on a bit complex problems as well, such as Atari games which has $10^9$ to $10^{11}$ states\n",
    "    - In such huge state space, producing and updating a Q-table can become ineffective\n",
    "    - Thus we will use Deep Q-Learning, uses Neural Network that takes a state and approximates Q-values for each action based on that state\n",
    "    - In this unit, we will train an agent to play Space Invaders and other Atari environments using RL-Zoo, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6734eb-9ef2-41ea-8629-81c1f5f47d58",
   "metadata": {},
   "source": [
    "### From Q-Learning to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38702161-050f-47e7-b9bc-303e340599c7",
   "metadata": {},
   "source": [
    "- Q-Learning is an algorithm we use to train our Q-function, an action value function that determines the value of being at a particular state and taking a specific action at that state.\n",
    "- Q here stands for quality of that action at that state, internally Q-function, is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Q-table serves as the memory of our Q-function\n",
    "- However, Q-learning is a tabular method, this is fine for small state space, but if state space becomes large Q-learning is not scalable to such problems.\n",
    "- For ex. Atari environments have an observation space with a shape of (210,160,3)* containing values of 0 to 255 this gives us N = $256^{210x160x3}$ possible observations (for comparision we have approx $10^{80}$ atoms in the observable universe)\n",
    "- So overall we can say that we will have a Q-table of N by A, and that's again huge\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a310c1-3246-4637-a8ae-b7c58fac0340",
   "metadata": {},
   "source": [
    "- Thus we can see that the state space is gigantic, due to this, creating and updating a Q-table for that environment would not be efficient.\n",
    "- In this case, the best idea is to approximate Q-values using a *parameterized Q-function $Q_{\\theta}(s,a)$*\n",
    "- We will use a neural network that approximates Q-values for a given state, for each possible action at that state\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" width = 600, height = 600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578b7fe-d386-49f8-9fde-d4913b7b2711",
   "metadata": {},
   "source": [
    "### The Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a7efa-b41b-4348-b2ae-a9ad77b06400",
   "metadata": {},
   "source": [
    "- This is the architecture of our Deep Q-Learning network:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" width = 600 height = 600>\n",
    "\n",
    "- As input, we take a stack of 4 frames passed through the network as a state and output a vector of Q-values for each possible action at that state.\n",
    "- Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take\n",
    "- When the Neural Network is initialized, the Q-value estimation is terrible, but during training, our Deep Q-Network agent will associate a situation with the appropriate action and learn to play the game well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc3b03-72ff-48ff-9ea1-cd10ec4ceed2",
   "metadata": {},
   "source": [
    "#### Preprocessing the input and temporal limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcb2ae-d759-4568-ae2f-127e016148ba",
   "metadata": {},
   "source": [
    "- We need to preprocess the input, it's an essential step since we would like to reduce the complexity of our state to reduce the computation time needed for training\n",
    "- To achieve this, we reduce the state space to 84x84 and grayscale it. We can do this since the colors in Atari environments, don't add important information. This is a big improvement since we reduce our three color channels (RGB) to 1.\n",
    "- We can also crop a part of this scren in some games if it doesn't add any crucial information. Then we stack 4 frames together. \n",
    "- This stacking is necessary since it helps us to handle the problem of temporal limitation. Basically having a single frame doesn't give us any idea about motion, however if we stack more frames we capture temporal information.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/preprocessing.jpg\" width = 600 height = 600><img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation.jpg\" width = 600 height = 600>\n",
    "\n",
    "- These stacked frames are processed by 3 convolutional layers. These layers allow us to capture and exploit spatial relationships in images. But also, because the frames are stacked together, we can exploit some temporal properties across those frames. Finally, we have a couple of fully connected layers that output a Q-value for each possible action at that state.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation-2.jpg\" width = 600 height = 600><img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" width = 600 height = 600>\n",
    "\n",
    "- **So we can basically see that Deep Q-learning given a state, uses a neural network to approximate, the different Q-values for each possible action at that state.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9da71-f62a-45e4-bbc3-f195e194ead4",
   "metadata": {},
   "source": [
    "### The Deep Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ba8cf-73a9-464c-8045-14ff05723489",
   "metadata": {},
   "source": [
    "- Now we know Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state (value-function estimation)\n",
    "- The main difference between Q-Learning and Deep Q-Learning is that during **training phase, instead of updating the Q-value of a state-action pair directly** as we have done with Q-Learning, in Deep Q-Learning, we create a **loss function that compares our Q-value prediction and the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better**\n",
    "*So we need to have Q-targets and Q-value predictions*\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" height = 600 width = 600><img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" height = 600 width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1e621-7f4e-43fa-aaaa-aa30473aab91",
   "metadata": {},
   "source": [
    "- The Deep Q-Learning training algorithm has 2 phases:\n",
    "    - Sampling: we perform actions and store the observed experience tuples in a replay memory\n",
    "    - Training: Select a small batch of tuples randomly and learn from this batch using a gradient descent update step\n",
    "    \n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44237cb-9ad8-428c-8fbd-b4a0aa39b49d",
   "metadata": {},
   "source": [
    "- This is not the only difference compared with Q-Learning. \n",
    "- Deep Q-Learning training **might suffer from instability,** mainly because of combining a non-linear Q-value function (Neural Network) and bootstrapping (when we update targets with existing estimates and not an actual complete return)\n",
    "\n",
    "- To help us stabilize the training, we implement 3 different solutions:\n",
    "    - Experience Replay to make more efficient use of experiences\n",
    "    - Fixed Q-Target to stabilize the training\n",
    "    - Double Deep Q-Learning, to handle the problem of the overestimation of Q-values\n",
    "    \n",
    "- Let's go through them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8910dc-8345-42c3-b40e-946b7cee618b",
   "metadata": {},
   "source": [
    "#### Experience Replay to make more efficient use of experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf102886-e0be-49f1-998d-5edcb2d465b5",
   "metadata": {},
   "source": [
    "- Why to create a replay memory?\n",
    "    - Experience  Replay in Deep Q-Learning has 2 functions:\n",
    "        - **1. Make more efficient use of the experiences during the training.** Usually, in online re-inforcement learning, the agent interacts with the environment, gets experience (state, actoin, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient\n",
    "            - Experience replay helps by using the experiences of the training more efficiently. We use a **replay buffer** that saves experience samples **that we can reuse during the training**\n",
    "            - This allows the agent to learn from the **same experiences multiple times**.\n",
    "\n",
    "        - **2. Avoid forgetting previous experiences and reduce the correlation between experiences.**\n",
    "            - The problem we get if we give sequential samples of experiences to our neural network is that it **tends to forget the previous experiences as it gets new experiences.** For instance, if the agent is in the first level and then in the second, which is different, it can forget how to behave and play in the first level.\n",
    "            \n",
    "            - The solution is to create a Replay Buffer that stores experience tuples while interacting with the environment and then sample a small batch of tuples. This prevents **the network from only learning about what it has done immediately before**\n",
    "    \n",
    "    - Experience replay also has other benefits. By randomly sampling the experiences, we remove correlation in the observation sequences and avoid action values from **Oscilating or Diverging catastrophically**\n",
    "    - In Deep Q-Learning psuedocode, we initialize a replay memory buffer D with capacity N (N is a hyperparameter that you can define). We then store experiences in the memory and sample a batch of experiences to feed the Deep Q-Network during the training phase.\n",
    "\n",
    "- <img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay-pseudocode.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4dc6f-ab23-4d27-894e-fe5acfcbe9c0",
   "metadata": {},
   "source": [
    "#### Fixed Q-Target to stabilize the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f47c3-3807-4bf2-8a0f-68880d2c15ec",
   "metadata": {},
   "source": [
    "- When we want to calculate the TD error (aka the loss), we calculate the **difference between the TD target (Q-Target) and the current Q-value (estimation of Q)**\n",
    "- But we don't have any **idea of the real TD target.** We need to estimate it. Using the Bellman equation, we saw that the TD target is just the reward of taking that action at that state plus the discounted highest Q-value for the next state.\n",
    "\n",
    "- However, the problem is that we are using the same parameters (weights) for estimating the TD target and the Q-value. Consequently, there is a **significant correlation** between the TD target and the parameters we are changing. Therefore, **at every step of training, both our Q-values and the target values shift**. We're getting closer to our target, but the target is also moving. It's like chasing a moving target! This can lead to significant oscillation in training.\n",
    "\n",
    "     For ex.\n",
    "    - It's like if you were a cowboy (the Q estimation) and you wanted to catch a cow (the Q-target). Your goal is to get closer (reduce the error).\n",
    "    - At each time step, you're trying to approach the cow, which also moves at each time step (because you move the same parameters) \n",
    "    - This leads to a bizzare path of chasing (a significant oscillating in training)\n",
    "    - Instead, what we see in the psuedo-code is that we:\n",
    "        - Use a separate network with fixed parameters for estimating the TD Target\n",
    "        - Copy the parameters from our Deep Q-Network every C steps to update the target network\n",
    "        \n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/fixed-q-target-pseudocode.jpg\" width = 600 height = 600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d12e7-0150-4e69-a673-0ab1cd6916fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec135d61-6f2c-43b6-aa94-b4ea0465737c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3101fa6e-64c2-404d-8688-c9a6ba746bfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f5193c8-2604-4c82-a40d-15c0386928c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04e69519-3278-4ee0-bd70-9f252de9996c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "358afa37-3382-4821-811a-cc094adb0a3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "551ea7d6-ed90-4ea7-b0e2-239c5d25d918",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd4064a-2981-49b7-95c7-03fb3b3a5e75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3cdde44-f6c9-4588-8e9f-74f9bfea25de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb52b7c-6a41-4273-a525-80b818087f14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30f46328-5c4c-4e65-ab3b-c1a9846f3b51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff11bb12-6122-4278-80d0-e50f66294d51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0400bf3e-a732-4a5b-8542-2906773665e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9544d0da-8864-4b52-9960-a51cc1e931f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135f90c6-e1e9-46a2-8f5a-e8bd8af72bed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e70d4b93-4786-4798-8199-7de25858447f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "476c89b0-4984-48bd-87ea-ff641ca94872",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb69dd3-0ca8-4cf4-af59-0dd32ff4a28e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ea1e4d-0425-4e73-9a34-3ea2f2a14014",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef89baa8-df4a-4df9-8ec2-4401560ee26e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2bc8177-5556-4526-9d0d-c369b9040b37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2911473-b1b3-4289-aeec-1f0b97925ef8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cdda6e7-4d2b-47dc-9da8-9788922a7e27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6bbbe8-d112-4aaa-ae72-39f9b7428c7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d2eac0-daef-4a54-bb1a-01ef7cd4273b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "503214ef-a763-4b2c-b475-87983cf2d02c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d265db0f-47bd-43e6-9850-17f91a92ca66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a148ba09-4104-4a57-a252-31e606fd268e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3098c2cc-7ade-4f46-a1b7-8011f65198b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626a2aa2-12c8-428a-97a9-1fa46124c50c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40872221-2f2e-4210-8466-8cedea26dc59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26ce2a04-cf8e-488e-bf05-4a1dde2f2a97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "824f99d8-8e7f-4ef0-a04d-95b85dca4c42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf10069-498e-4f71-9ff9-ae2aa43adf1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aa96fc6-8d20-4497-b89a-5467893cfad3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f056fa8-1abf-4eb8-942a-db8a48f6e7d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dacb405-536b-4f5a-be0c-96119eacc03f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c96c99a-8ce4-4a73-8e5a-10082d7254ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd33b05-d318-4508-8089-9c11a9b0151f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac226ecb-dd60-42eb-9819-b5a7adc387cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f4cdc3-5526-4a6c-ac0b-4a71323015f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad4db6c4-c2fb-4583-b7f9-0960248ccc0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cd34f66-d45a-45af-81b0-62d401ecac9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a97180e4-bb72-4ce0-8b1e-fe17a586bedc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0fe92a1-f9c2-41bf-afa1-5dc32aeac889",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c332e44d-737f-45bd-b0d5-3b2b132189ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49bc9e60-b877-4641-9a99-02bfb86bd450",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01650bac-cbac-499f-91ca-c5df6e58d594",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca9d9a8-a776-4fe0-8b64-9a4d68b58d4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09ef539-541c-4e24-8d8e-6a98de6367a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1682438-92a0-439f-aa0a-101f3896c23c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17e1a58-8e03-4ba1-a8a0-11b4a33b2389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbb7ca7c-2700-40a7-8a0f-9f43e6ed5c76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9457ad46-b01c-4638-a639-7766a65f02c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4712127e-ea66-491c-9891-47549d721e2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c619f446-fc66-4306-a50e-352e53f6dd6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb972b0-f166-48b7-afb4-73ce7285d554",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90e6ee7e-f06e-4b3f-bfdf-cb1e78bcb38a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c722ad-7812-4013-a6c9-0e6215f8276d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aded28b-3bc8-406a-97b3-e7b411401617",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4321d5b-cd38-4186-9a23-3e1cc80e04b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08bda89c-96d9-45a1-8bda-9597dd3ca1a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df7a621-6deb-4ee7-948d-3d111798333e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee122ca8-c6a4-43b3-b75c-f76a94a561ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca86083-d552-4caf-8d71-34a279917bd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a3b9cdd-4f7d-48ce-b76d-78027c859708",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de199a85-59ff-416b-9278-5647e0b1661e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f669c018-097e-4543-8a0e-38367ada0816",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "901df7f5-ac24-4a78-bd98-d3714f5222de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "646f815f-03f5-49a6-b575-14154ac4f2ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "223e6a10-b069-490c-9027-5e45a19fc0f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66941938-43bb-404b-89ca-9f695c1222fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
