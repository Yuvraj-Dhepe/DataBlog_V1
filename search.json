[
  {
    "objectID": "index.html#recent-blogs",
    "href": "index.html#recent-blogs",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "📮 Recent Blogs",
    "text": "📮 Recent Blogs\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nBlog 1\n\n\n\n\n2/9/22\n\n\nBlog 2\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-projects",
    "href": "index.html#latest-projects",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": " Latest Projects",
    "text": "Latest Projects\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n4/10/23\n\n\nEnd to End Machine Learning Project on Student Performance Dataset\n\n\n\n\n7/28/22\n\n\nAtliQ Hardware Sales Insights\n\n\n\n\n2/9/22\n\n\nSpiritual Youtube Channels Data Scraping\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html",
    "href": "docs/projects/B1_Sales_Insights.html",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "",
    "text": "AtliQ Hardware supplies computer hardware and peripherals to various clients across India and has a head office in Delhi and regional offices throughout India.\nBhavin Patel, the sales director of the company, is facing challenges in tracking the sales in the dynamically growing market and getting insights into his business.\nThe regional managers tend to give verbal feedback that paints a rosy picture and provide complex Excel files, making it challenging for Bhavin to get a clear picture of the business.\nBhavin wants simple and digestible insights, such as the top five customers, the weakest regions, and the year-to-date revenue, etc,. to make data-driven decisions for the business.\nThe ultimate goal is to improve the declining sales and make informed business decisions based on real-time data analytics.\nPowerBI can provide a solution to visualize the data and present simple and actionable insights for Bhavin.\n\nP.S. The problem statement is virtual and doesn’t relate to any real-world entity, any such occurence is just a matter of coincidence."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "href": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Project Planning and Data Discovery",
    "text": "Project Planning and Data Discovery\n\nProject planning using AIMS grid.\n\nBhavin patel who is a sales director of at Atliq hardware realizes the pains of getting sales insights and thinks of having a data analytical solution for this problem.\nHe calls a couple of people like an IT head or a head of data department and schedules a meeting on how exactly we should do this data analytics project.\nTogether with IT head, he also gathers other stake holders in the room together and they brainstorm on project planning using AIMS grid.\n\nAIMS grid\n\nAIMS grid is a project management tool having four components.\n\nFirst component is Purpose\n\nDetermining what is the pain point and what needs to be done to resolve the pain point\n\nSecond component is the Stakeholders\n\nIn this project who all will be involved\nThe main stakeholders involved for this data analytical projects are the marketing team ,the sales team who is facing these issues to do proper sales analysis\nIT team (The Falcons) a group of software engineers that AtliQ hardware has who manages all the software requirements of Atliq\nThe data analytics (The Data Masters) team which is inhouse\n\nThird component is the end result i.e., once the project is over what needs to be achieved\n\nFor the data analytics project the end result is to have a power bi dashboard something that our sales director or even Regional Managers can go and look into and it gives you a real-time information on the sales numbers\n\nFourth component is the success criteria after the project is over, how do we define that the project was successful\n\nIn our project scenario the success criteria would be, we want the costs to go down by 10% on management of these sales data\nWe want to utilize the saved business time by the use of dashboard to take data driven descisions and increase the growth of company by let’s say 10%\nDashboards uncovering sales order insights with latest data available\nSales team able to take better decisions and prove 10% cost savings of total spend\nSales Analysts stop data gathering manually (merging excel files) in order to save business time and reinvest it value added activity\n\n\n\nOther Important points\n\nFalcons team, manages the software of Atliq Sales Management which is keeping track of all the sales number so whenever they sell any computer or any hard days in any local region this software is printing the invoice so it has all the records stored in a MySQL database.\nData Masters team will reach out to Falcons and to use the AtliQ SQL database because this is the database which has all the records that we need for our analytics and what we’ll do is we’ll integrate MySQL. We will use MySQL as a source in our power bi tool and we will build dashboard on top of it.\n\nReal World Scenarios\n\nIn real world direct access of data from IT team is not allowed cause if the data volume is high we want to make sure that MySQL database is not affected by the queries that Falcons are doing in your PowerBi, so many times companies, have a Data Warehouse as part of the Data Ecosystem.\nMySQL which is also known as OLTP which is online transaction processing system it is a very critical system and a company cannot afford for that system to go down otherwise the regular sales operations gets hampered.\nFor any business analytics is important but then it’s a secondary thing so what companies do is they pull the data from OLTP which is MySQL in our case they do all the transformation which is also called ETL which means extract transform and load and after doing that transformation they store the data in a data warehouse.\nThere are various types of Data Warehouse management tools like Snowflake and there are various ways to transform the data like using Pandas or other data processing libraries.\nMost companies working on huge data analytical projects have another in-house teams who manage the data warehouse known as Data Engineers (The Data Miners).\n\nConclusion & Data Discovery\n\nAssuming our data is not that big, the data is simply taken from the Falcons, by our Data Masters.\nThe Data Masters will plug PowerBI directly to SQL database and build the necessary analytical dashboard. Also some times data analysts will spend their time to capture information for analytics, which might not be available in the organization at all. But for this project we will not do that.\nOnce the Data Masters have the data, they do data cleaning (data wrangling) and data merging (data munging), all this can be done via specialized software. But our data being simple we will do it via PowerBi itself."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "href": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Data Cleaning and Data Merging or Wrangling",
    "text": "Data Cleaning and Data Merging or Wrangling\n\nLoad the data in SQL taken from the Falcons team\n\nFind Basic Insights about the data in SQL.\nFind out few inconsistencies in data in SQL itself and make a note to remove them in PowerBI later.\n\nDo the ETL in PowerBi\n\nFirst Load the data\nDo the data modelling, i.e., link the tables well, along with their relationships.\nStart data transformations on the data to make it good enough for the data analysis.\nOnce the ETL is done, now start building the dashboards\n\n\n\nBasic Insights from DB\n\n\nDB_Schema\n\n\nCode\nfrom IPython import display\ndisplay.Image(\"./images/DB_Schema.jpg\")\n# Can use ![alternative text](path-to-image) but it works out of zsh\n\n\n\n\n\n\n\nDB_Queries\n\nCustomers Table\n\nGet the total number of customers doing business with AtliQ hardwares\n\nInsights: There are total of 38 customers doing business with AtliQ hardwares\n\nSELECT count(*) FROM sales.customers; \nGet count of customers according to their types\n\nInsights: Only 2 customer types i.e. of Brick Mortar and E-Commerce\n\nSELECT count(cs.customer_code) as total_customer_per_type, cs.customer_type \nFROM sales.CUSTOMERS as cs \nGROUP BY cs.customer_type; \nChecking unique customers in the customer category.\n\nInsights: All the 38 customers are unique\n\nSELECT COUNT(DISTINCT tb.custmer_name) as total_unique_customer_name_count\nFROM \n( \n# Seeing whether each customer_code belongs to unique customer name or not. :- It does.\nSELECT cs.custmer_name, COUNT(cs.customer_code) as total_unique_customer_names\nFROM sales.customers as cs\nGROUP BY cs.custmer_name\n) as tb;\n\n\n\nDate Table\n\nInsights: Total unique dates : 1126 and that is the total dates in the table itself.\nSELECT (count(distinct dt.date)) as total_unique_dates  \nFROM sales.date as dt;\nInsight: Seeing the trend of number of transactions each year\nSELECT dt.year, count(dt.year) as total_transactions_in_each_year \nFROM sales.date as dt\nGROUP BY dt.year;\nInsights: We have on average 30 transactions per month in each year\nSELECT \n    dt.month_name,dt.year,count(dt.month_name) as transactions_per_month_per_year\nFROM\n    sales.date AS dt\nGROUP BY dt.year, dt.month_name;\n\n\n\nMarket table\n\nInsights: Total of 17 markets each having it’s own unique identifier code\nSELECT count(mk.markets_name) \nFROM sales.markets as mk;\nInconsistency:\n\nThere are only 2 international sales in the data, so it’s better to not consider them as we don’t have much information about them now.\n\n\n\n\nTransactions Table\n\nInsights: There are a total of 150283 transaction.\nSELECT count(*) FROM sales.transactions;\nInsights: The total_sales_qty: 2444415 and reverified the same via net_sales_per_month_per_year\nSELECT SUM(tr.sales_qty) as net_sales_in_all\nFROM sales.transactions as tr;\nInsights: Finding the different type of currencies used while doing the transactions.\nSELECT distinct(tr.currency) ,count(*) as number_of_transactions_currency_wise\nFROM sales.transactions as tr\nGROUP BY tr.currency;\nInconsistencies:\n\nFew transactions are in USD, they need to be converted to INR\nThe currency column, has an inherent error to show INR and USD 2 times when grouped by currency. The error was because entry in currency field was INR USDhile creating the data.\n\nRemoved inconsistency by formating the table well.\n\n\n\n\n\nProducts Table\n\nInsights: Finding the distribution of product type\nSELECT pr.product_type, count(pr.product_code) AS product_count_forthis_type\nFROM sales.products as pr\nGROUP BY pr.product_type;\nInsights: Every product code is unique\nSELECT count(distinct pr.product_code) \nFROM sales.products as pr;\nSELECT count(*) FROM sales.products;\nSELECT * FROM sales.products;\n\n\n\n\nGeneral Queries\n\nInsights: Getting revenues per year per product\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Getting revenues per year per product in Chennaicustomers\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020 and st.market_code = \"Mark001\"\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Found out which market makes the most of the sales and which one the least.\nSELECT mk.markets_code, sum(tr.sales_qty) as net_sales_per_market, mk.markets_name\nFROM sales.transactions as tr\nJOIN sales.markets as mk\nON tr.market_code = mk.markets_code\nGROUP BY mk.markets_code\nORDER BY net_sales_per_market DESC;\nInsights: Finding which year has made the most of the transactions and in which month\nSELECT sum(net_sales_per_month_per_year)\nFROM \n( \nSELECT dt.year, dt.month_name, sum(tr.sales_qty) AS net_sales_per_month_per_year\nFROM sales.transactions as tr\nJOIN sales.date as dt\nON dt.date = tr.order_date\nGROUP BY dt.year, dt.month_name\nORDER BY net_sales_per_month_per_year\n) AS tb;\nInsights: Finding sales_qty per product, looking at it seems like own brand is in more demand\nSELECT pr.product_code, sum(tr.sales_qty) as net_sales_per_product, pr.product_type\nFROM sales.transactions as tr\nJOIN sales.products as pr\nON pr.product_code = tr.product_code\nGROUP BY pr.product_code\nORDER BY net_sales_per_product DESC;\n\n\n\n\nETL\n\n\nDid a bit of Data Modelling, linking tables well of the DB\nRemoved the inconsistencies in the PowerBI from Market and Transaction table"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "href": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Dashboard Created in PowerBI",
    "text": "Dashboard Created in PowerBI\n\nP.S. Viewing the dashboard requires PowerBI professional account.\n\nIf the viewer doesn’t have one, kindly use the pdf below to view overall analytical dashboard or the following link to download it.\n\n\nDownload File\n\n\n\n        \n        \n        \n    \n\n\n\n\n\n  \n    \n  \n\n\n\n\nMajor Insights from the Dashboard\n\n\nKey Insights Dashboard\n\nDelhi NCR seems to give major Revenue and Sales Qty followed by Mumbai, Ahemdabad and other city ares.\nBrick n Mortar Seems to contribute to majority of Revenue\nProduct revenue has an error, we can’t see the major revenue generating product code\nOver the years the revenue trend seems to decline\nRevenue seems to increase in Q3 n Q4 generally\n\nProfit Analysis Dashboard\n\nThough revenue contribution of Delhi n major city markets is more, the profit contribution is not much high for them.\n\nAction points for AtliQ:\n\n\nFocus more on the major profit% generating cities\n\n\nFind why the profit % is less in major revenue generating cities\n\n\n\nFocus on the customers which provide more Profit Margin Contribution by encouraging them like by giving them discounts\n\nPeformance Insights Dashboard\n\nAnalyze the data by breakdown to zones, markets and customers\nObserve the zones that don’t fulfill the profit margin\nObserve Previous year revenue trend as a comparision with current year revenue trend"
  },
  {
    "objectID": "docs/projects/StudPerformance_end_to_end.html",
    "href": "docs/projects/StudPerformance_end_to_end.html",
    "title": "End to End Machine Learning Project on Student Performance Dataset",
    "section": "",
    "text": "Due to extensive work being done in any field of data, it’s essential that a data person in a team has to wear multiple hats, while doing the work, to gain experience on what it means to wear those hat’s I worked on creating an end to end project on simple data set, but by doing detailed work on each and every step of the process which includes:\n\nCreating modular folder structure\nDeciding on the dataset\nSetup of the environment\nDesign and Development of Components and Pipelines, where components interact with data in backend, whereas the pipelines interact with the user and components to derive insights and finally provide result to the user.\n\nNotebook checkpoints\n\nSTAR ANALYSIS\nExplained every point of the star method, step by step in detail.\n\nEx. Action will be broken down to A1, A2 to follow up the notebook.\nWe use acronyms like T1 representing Task 1, A1.1 representing subaction 1 of Action 1.\n\n\n\n\n\n\n\nSituation: To gain experience in end-to-end machine learning project development and deployment.\nTask: Create a machine learning project from scratch, following the entire pipeline from data collection to model deployment.\nAction: Develop a project idea, collect and preprocess the data, design and train the machine learning model, evaluate the model’s performance, and deploy the model into a production - environment.\nResult: Gain hands-on experience in end-to-end machine learning project development and deployment, with a fully functional machine learning system that can be used for real-world applications.\n\n\n\n\nS1. Need of gaining exposure in real-world ML project development and deployment\nS2. A way to improve my Data Science profile, with such projects\nS3. Building skillset to be of use in the real-world, and not be limited to books\n\nWith the situation being clear let’s jump to a bit about task that was required to be done for this situation\n\n\n\n\nT1. Creating a folder structure, for a real-world entity project.\n\nUses: Introduction of Modularity to the project, rather than a single jupyter notebook file doing all the job.\n\nT2. Creating an environment and setup file to run this ML pipeline from scratch.\nA. Developing an End to End ML pipeline and then performing web deployment for using the ML model.\n\nWith the basic overview of task now, let’s look onto every task in details\n\n\n\nCreating a folder structure for our real-world project. This is an essential part for any real-world code project, as it introduces modularity to our code. This modularity helps us to deal with complexity of huge projects in simple way, where a team can work together on different parts of the project, re-use each others work and combine it all at the end.\n\n\n\n\nFirst setup a github repo (ML_Web_Project is my repo), keeping all the options to default.\nLocally setup a folder (END_To_END_ML_PROJECT is my base local folder setup on WSL, but the one can use windows or mac as well)\n\nOpen this directory in vscode\nOpen a terminal\n\nSecondly let’s create a conda environment named venv into this local folder, so to have packages locally to run the project. bash     conda create -p venv python==3.8 -y\n\nActivate this environment from the base folder\n\nconda activate venv/ # don't forget '/' cause it tells that this environment is in a folder named venv\nLink the local folder to the github repo\n\nFirst do git init in the local folder\nFollow all the steps mentioned in the github repo you created to do the syncing of local folder to the repo.\nAfter the update of git back in 2021, one needs to setup ssh-keys to use the github repo or use tokens, I prefer to use ssh-keys, follow the steps here.\nCreate a default .gitignore for python in github repo online.\nFinally do a git pull, to sync the changes locally as well.\nLater on whenever there are enough changes to the local code, follow the steps of git add, commit and push with a useful commit message.\n\nBy now local repo should have a .gitignore, README.md, venv/ in their local repo, after this create the following folder structure locally.\n\n- END_TO_END_ML_PROJECT\n    - setup.py # The setup script is the center of all activity in building, distributing, and installing modules that are necessary to run the ML pipeline. # Consider it as the core of the ML Pipeline. This setup.py will help to use our ML pipeline as a package itself and can even be deployed to Pypi.\n    - requirements.txt # All packages that need to be installed before running the project. # This is the part that gives energy to the core.\n    - data # The folder which consist of datasets used in the project.\n        - StudentsPerformance.csv\n    - notebook # jupyter notebooks, consisting of all codes which helps to find patterns in data and give a big picture code, later to be broken down into src folders.\n        - EDA_notebook.ipynb \n        - Model_train.ipynb\n    - src # The backbone containing all the source codes for creation of ML pipeline package.\n        - __init__.py\n        - exception.py # Helps in producing custom exceptions.\n        - logger.py # Contains the code that will help in creation of logs, and trace errors if caused any during the realtime.\n        - utils.py # Contains all the utilities that can be reused across the whole project.\n        - components # The major components of the project, which deal with data cleaning, transformation, model training etc.\n            - __init__.py\n            - data_ingestion.py\n            - data_transformation.py\n            - model_trainer.py\n        - pipeline # The complete pipelines built via use of components for further deployment of the model.\n            - __init__.py\n            - predict_pipeline.py\n            - train_pipeline.py\n\n\n\n\n\nCreating an environment and setup file which later can be used to condense our ML pipeline in form of package. In this part we build the foundation for our ML pipeline, by creating the code for setup.py file.\n\n\n\n\nCode for setup file,\n\nfrom setuptools import find_packages,setup\nfrom typing import List\n\ndef get_requirements(file_path:str)->List[str]:\n    '''\n    This function will return the list of requirements\n    '''\n    requirements = []\n    file = open(file_path,'r')\n    \n    for line in file:\n        if \"-e .\" not in line:\n            requirements.append(line.strip('\\n'))\n    file.close()\n    \n    #print(requirements)\n    return requirements\n    \n# With this setup we parse our requirements file to get the requirements installed for our project, one can make this static via use of package names in form of a list, instead of parsing a requirements file.\nsetup(\n    name='mlproject',\n    version='0.0.1',\n    author='<Your Name>',\n    author_email='<Your Email>',\n    packages=find_packages(), # This will use the codes or modules that we write for our ML pipeline, to ensure that our every module can be used for building the package, we have a __init__.py in src, or any directory that can be reused.\n    install_requires=get_requirements('requirements.txt') \n)\n\ncontents of requirements.txt file\n\npandas\nnumpy\nseaborn\nmatplotlib\nscikit-learn\ncatboost\nxgboost\ndill\ntensorboard\n-e . # This triggers the setup .py file automatically, but this is not readed when setup.py is called as per our above code.\n\nOnce these 2 files are setup, simply run:\n\npip install -r requirements.txt\n\nThis will install all the necessary packages in our virtual environment and create a new directory .egg-info which will help to create the ML pipeline package for us.\n\n\n\n\n\n\n\nA1. Project Idea: Using a student performance data to predict it’s grades or scores, depending on the other features of the dataset.\nA2. Data Collection and Preprocessing: We first do all EDA in a jupyter notebook to find patterns in the data and getting to know the type of preprocessing required to be done on the dataset.\n\nFor simple application the data is simply imported in form of csv file, but all this can even be done by getting data from Data Warehouse as well.\n\nA3. Design and Development of ML pipeline components: After EDA, we try to create simple modular codes in a jupyter notebook, which do the job of development, training and evaluation of ML model. Later these modular codes are more or less split into the folder structure that we created earlier.\nA4. Deployment of model into a production environment: We use cloud tools like AWS or Streamlit or Flask n Django or any other web service to deploy the ML model online to be used on realtime data provided by user or fetched from a source.\n\n\n\n\nProject Idea\n\nWe will use a simple student performance dataset, to predict the child’s maths scores via the rest of the features of the dataset.\nI am using this dataset, because it’s having a mixed of categorical and numerical features, we can have a good amount of EDA done on this simple data, and last but not the least train many regression algorithms on this simple data easily.\n\nData Collection & Preprocessing\n\nWe will use jupyter notebooks, to majority of the EDA, and finding the patterns.\n\nLink to EDA Ipynb File\n\nOnce the EDA is done, we will also have basic models run on the data, in another jupyter notebook, so that we have basic model pipeline code in place as well.\n\nLink to Models Ipynb File\n\n\n\n\n\n\n\nDesign and Development of ML pipeline components in form of modular codes\nSteps\n\nCreation of utility codes, logging and exception handling module that will be used all over the components, pipelines.\nCreation of Components modules inside the package consisting of Data Ingestion, Data Transformation and Model Trainer Component.\n\nCreation of train and predict pipelines modules that will be connected to the above components, and will be a pipeline connecting the frontend user and the backend model of Machine learning.\n\n\n\n\n\n\n\n#Common functionalities for the whole project: Utilities.py\nimport os\nimport sys\n\nimport dill\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import r2_score\n\nfrom src.exception import CustomException\nfrom sklearn.model_selection import GridSearchCV\n\n\ndef save_object(file_path,obj):\n    try:\n        dir_path = os.path.dirname(file_path)\n        os.makedirs(dir_path,exist_ok=True)\n        \n        file_obj = open(file_path,\"wb\")\n        dill.dump(obj,file_obj)\n        \n    except Exception as e:\n        raise CustomException(e,sys)\n\ndef evaluate_models(X_train, y_train, X_test,y_test,models, param):\n    try:\n        report = {}\n        \n        for i in range(len(list(models))):\n            model = list(models.values())[i]\n            para=param[list(models.keys())[i]]\n\n            gs = GridSearchCV(model,para,cv=3)\n            gs.fit(X_train,y_train)\n\n            model.set_params(**gs.best_params_)\n            model.fit(X_train,y_train)\n            \n            #model.fit(X_train,y_train)\n            \n            y_train_pred = model.predict(X_train)\n            y_test_pred = model.predict(X_test)\n            \n            train_model_score = r2_score(y_train,y_train_pred)\n            test_model_score = r2_score(y_test,y_test_pred)\n            \n            report[list(models.keys())[i]] = test_model_score\n        \n        return report\n    \n    except Exception as e:\n        raise CustomException(e,sys)\n            \ndef load_object(file_path):\n    try:\n        file_obj = open(file_path,\"rb\")\n        return dill.load(file_obj)\n        file_obj.close()\n        \n    except Exception as e:\n        raise CustomException(e,sys)\n\n\n\n# Logger is for the purpose of logging all the events in the program from execution to termination.\n# For example, whenever there is an exception, we can log the exception info in a file via use of logger.\n\n# Read logger documentation at https://docs.python.org/3/library/logging.html\nimport logging\nimport os\nfrom datetime import datetime\n\nLOG_FILE_NAME = f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\nlogs_path = os.path.join(os.getcwd(), \"logs\",LOG_FILE_NAME) # This will create logs folder in the same working directory where this file is present\nos.makedirs(logs_path,exist_ok=True) # Keep appending the logs in the same directory even if there are multiple runs of the program\n\nLOG_FILE_PATH = os.path.join(logs_path,LOG_FILE_NAME)\n\nlogging.basicConfig(filename=LOG_FILE_PATH,\n                    level=logging.INFO,\n                    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s: %(message)s\",\n                    datefmt='%m/%d/%Y %I:%M:%S %p'\n                    ) #This is the change of basic configuration for the logger\n\nif __name__ == '__main__':\n    logging.info(\"This is a test log\")\n    logging.warning(\"This is a warning log\")\n    logging.error(\"This is an error log\")\n    logging.critical(\"This is a critical log\")\n\n\n\n# We use this custom exception handling in the project to handle all the errors that will come into the project, simply we can say that we are handling all the errors that will come into the project in a single place.\n\n\nimport sys\n\n# Sys module in python provides various functions and variables that are used to manipulate different parts of the python runtime environment. It allows operating on the python interpreter as it provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n# Read more about sys module here: https://docs.python.org/3/library/sys.html\nfrom src.logger import logging\n\n\ndef error_message_detail(error,error_detail:sys):\n    _,_,exec_tb = error_detail.exc_info()\n    file_name = exec_tb.tb_frame.f_code.co_filename\n    error_message = f\"Error occured in python script name {file_name} on line number {exec_tb.tb_lineno} and error is {str(error)}\"\n    \n    return error_message\n    \nclass CustomException(Exception):\n    def __init__(self,error_message,error_detail:sys):\n        super().__init__(error_message)\n        self.error_message = error_message_detail(error_message,error_detail= error_detail)\n        #self.error_detail = error_detail        \n        \n    def __str__(self):\n        return f\"{self.error_message}\"\n\n# Read more about custom exception handling here: https://www.programiz.com/python-programming/user-defined-exception\n\nif __name__ == '__main__':\n    try:\n        a = 10\n        b = 0\n        c = a/b\n        print(c)\n    except Exception as e:\n        logging.error(e)\n        raise CustomException(e,error_detail=sys)\n\n\n\n\n\n\n\nData being a central component of any project, in this component, we write classes such as DataIngestionConfig and DataIngestion.\n\nDataIngestionConfig consists of public path variables to train, test and raw data.\nDataIngestion helps to create an object which invokes an object of DataIngestionConfig during initialization and retrieves public path variables.\nBy use of those paths, we read data, split them up and save them to the directory by use of initiate_data_ingestion method.\n\nData ingestion is a crucial step in any project that involves handling data. This process involves extracting data from different sources, such as databases or warehouses, and loading it into a centralized location, such as a data warehouse, data lake, or data mart. Typically, this task is performed by a specialized big data team, whose responsibility is to ensure that data is obtained from various sources and stored in different formats, such as Hadoop or MongoDB.\nAs Data Scientists, it’s essential to have knowledge of how to extract data from different sources, such as Hadoop, MongoDB, MySQL, or Oracle, and make it available for analysis. Since data is a critical asset in any project, understanding the process of data ingestion is vital to ensure that the data is organized and stored in a way that facilitates analysis.\n\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom src.components.data_transformation import (DataTransformation,\n                                                DataTransformationConfig)\nfrom src.components.model_trainer import ModelTrainer, ModelTrainerConfig\nfrom src.exception import CustomException\nfrom src.logger import logging\n\n\n@dataclass\nclass DataIngestionConfig:\n    '''\n    Used for defining the configuration for data ingestion.\n    '''\n    train_data_path: str = os.path.join('artifacts', 'train.csv')\n    test_data_path: str = os.path.join('artifacts', 'test.csv') \n    raw_data_path: str = os.path.join('artifacts', 'data.csv')\n\nclass DataIngestion:\n    '''\n    Used for ingesting data by making use of the configuration defined in DataIngestionConfig.\n    '''\n    def __init__(self,ingestion_config: DataIngestionConfig = DataIngestionConfig()):\n        self.ingestion_config = ingestion_config\n    \n    def initiate_data_ingestion(self,raw_data_path: str = None):\n        try:\n            # Reading data here.\n            logging.info(\"Initiating data ingestion\")\n            if raw_data_path is not None:\n                self.ingestion_config.raw_data_path = raw_data_path\n                data = pd.read_csv(self.ingestion_config.raw_data_path)\n            else:\n                data = pd.read_csv('data/NewSPerformance.csv')\n                        \n            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)\n            data.to_csv(self.ingestion_config.train_data_path,index=False,header=True)\n            logging.info(\"Data ingestion completed\")\n            \n            logging.info(\"Train test split initiated\")\n            train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 18)\n\n            train_set.to_csv(self.ingestion_config.train_data_path,index = False, header = True)\n            test_set.to_csv(self.ingestion_config.test_data_path,index = False, header = True)\n            logging.info(\"Train test split ingestion completed\")\n            \n            return (\n                self.ingestion_config.train_data_path,\n                self.ingestion_config.test_data_path\n            )\n        except Exception as e:\n            logging.error(\"Error occured in data ingestion\")\n            raise CustomException(e,sys)\n    \n\nif __name__ == '__main__':\n    obj = DataIngestion()\n    train_data, test_data = obj.initiate_data_ingestion()\n    \n    data_transformation = DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n    train_arr, test_arr,_ = data_transformation.initiate_data_transformation(train_data,test_data)\n    \n    modeltrainer = ModelTrainer()\n    print(modeltrainer.initiate_model_trainer(train_arr, test_arr))\n\n\n\n\nOnce data ingestion is done, Data transformation component is used to transform the data, to make it useful for analysis and train models on it.\n\nDataTransformationConfig class in this component stores public path variable to store the preprocessing object in pickle type data, to be later used during building the web app.\nDataTransformation class helps to create an object which invokes DataTransformationConfig Object to get access to preprocessing object path.\nWe have a get_data_transformer_object method, that returns a preprocessor object which can preprocess numerical and categorical columns\nBy use of the get_data_transformer_object method, in initiate_data_transformation method, to do all the preprocessing on the train and test files, whose path is available from Data Ingestion component. After all the preprocessing we return train and test array consisting of feature and target variables.\n\nA data transformation component is a crucial part of the data science process, which involves transforming raw data into a format that can be used for analysis. Data Scientists play a vital role in this process as they use various techniques such as feature engineering, feature selection, feature scaling, data cleaning, and handling null values to ensure the quality of data used for analysis. By understanding the process of data transformation, Data Scientists can generate valuable insights from raw data and make informed business decisions.\n\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.utils import save_object\n\n# Defining the paths for the data ingestion\n# di_obj = DataIngestion.DataIngestionConfig() # Not required as we already are doing the Doing the addition of paths in data_ingestion.py itself.\n# di_obj.train_data_path = \"data/train_data.csv\"\n# di_obj.test_data_path = \"data/test_data.csv\"\n\n@dataclass #This is a decorator which is used to create a dataclass variables.\nclass DataTransformationConfig:\n    '''\n    We are creating a dataclass variable which will be used to store the paths for the data transformation transformer object.\n    '''\n    preprocessor_obj_file_path = os.path.join(\"artifacts\",\"preprocessor.pkl\")\n\nclass DataTransformation:\n    \n    def __init__(self,transformation_config: DataTransformationConfig = DataTransformationConfig()):\n        self.data_transformation_config = transformation_config\n\n    def get_data_transformer_object(self):\n        '''\n        This function is responsible for creating a preprocessing data transformation object.\n        '''\n        try:\n            numerical_columns = [\"writing_score\",\"reading_score\"]\n            categorical_columns = [\n                \"gender\",\n                \"race_ethnicity\",\n                \"parental_level_of_education\",\n                \"lunch\",\n                \"test_preparation_course\"\n                ]\n            \n            num_pipeline = Pipeline(\n                steps=[\n                    (\"imputer\",SimpleImputer(strategy = 'median')),\n                    (\"scaler\",StandardScaler())\n                ]\n            )\n            \n            cat_pipeline = Pipeline(\n                steps = [\n                    (\"imputer\",SimpleImputer(strategy = 'most_frequent')),\n                    (\"one_hot_encoder\",OneHotEncoder()),\n                    ('scaler',StandardScaler(with_mean=False))\n                ]\n            )\n            \n            logging.info(f\"Numerical columns:{numerical_columns}\")\n            logging.info(f\"Categorical columns:{categorical_columns}\")\n            \n            preprocessor = ColumnTransformer(\n                [\n                    (\"num_pipeline\",num_pipeline,numerical_columns),\n                    ('cat_pipeline',cat_pipeline,categorical_columns)\n                ]\n            )\n            \n            return preprocessor\n        except Exception as e:\n            raise CustomException(e,sys)\n        \n    def initiate_data_transformation(self,train_path,test_path):\n        '''\n        Here we use the preprocessing object to transform the data.\n        '''\n            \n        try:\n            train_df = pd.read_csv(train_path)\n            test_df= pd.read_csv(test_path)\n            \n            \n            logging.info(\"Read train and test data completed\") \n            \n            logging.info(\"Obtaining preprocessing object and starting processing.\")\n            preprocessing_obj = self.get_data_transformer_object()\n            target_column_name = \"math_score\"\n            numerical_columns = [\"writing_score\",\"reading_score\"]\n            \n            input_feature_train_df = train_df.drop(columns = [target_column_name],axis=1)\n            target_feature_train_df = train_df[target_column_name]\n            \n            input_feature_test_df = test_df.drop(columns = [target_column_name],axis=1)\n            target_feature_test_df = test_df[target_column_name]\n            \n            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n            input_feature_test_arr = preprocessing_obj.fit_transform(input_feature_test_df)\n            \n            train_arr = np.c_[\n                input_feature_train_arr,np.array(target_feature_train_df)\n                ]\n            \n            test_arr = np.c_[\n                input_feature_test_arr,np.array(target_feature_test_df)\n                ]\n            \n            logging.info(f\"Saved Preprocessing object at a particular filepath \")\n            save_object(\n                file_path = self.data_transformation_config.preprocessor_obj_file_path,\n                obj = preprocessing_obj\n            )\n            return(\n                train_arr,\n                test_arr,\n                self.data_transformation_config.preprocessor_obj_file_path,\n            )\n            \n                \n        except Exception as e:\n            raise CustomException(e,sys)\n\n\n\n\nModel Trainer(MT): We can run various models, once above components have turned data to desired format. This component consists of 2 classes as follows.\n\nModelTrainerConfig class stores public path variable to store the model object once trained in the pickle format.\nModelTrainer class, uses initiate_model_trainer method, that access train and test array from Data Transformation component. This method is able to train various models together on the train array and then finally make predictions on the test array, by using the best model from the various models being used on the base of r2_scores. Also in this method we use the ModelTrainerConfig object to store this trained model in local directory and last but not least we also return the r2_score for the best model on test data in this method itself.\n\nThe model trainer component is responsible for training machine learning models on the transformed data. Data Scientists use this component to select an appropriate algorithm, tune hyperparameters, and train the model on the data. The trained model is then evaluated for its performance, and if it meets the desired level of accuracy, it is deployed for production use. The role of Data Scientists in this component is to select and fine-tune the machine learning models that best fit the problem at hand, and ensure that the models meet the business requirements. Ultimately, the model trainer component helps Data Scientists to generate insights and make predictions that can drive business decisions.\n\nimport os\nimport sys\nfrom dataclasses import dataclass\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import (AdaBoostRegressor, GradientBoostingRegressor,\n                              RandomForestRegressor)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\nfrom src.exception import CustomException\nfrom src.logger import logging\nfrom src.utils import evaluate_models, save_object\n\n\n@dataclass\nclass ModelTrainerConfig:\n    # This class is used to store the configs, or any other files generated in this particular python file.\n    trained_model_file_path = os.path.join(\"artifacts\",\"model.pkl\")\n\nclass ModelTrainer:\n    def __init__(self,model_train_config:ModelTrainerConfig = ModelTrainerConfig() ) -> None:\n        self.model_trainer_config = model_train_config\n        \n    \n    def initiate_model_trainer(self,train_array,test_array):\n        try:\n            logging.info(\"Split training and test input data\")\n            X_train, y_train, X_test, y_test = (\n                train_array[:,:-1],\n                train_array[:,-1],\n                test_array[:,:-1],\n                test_array[:,-1]\n            )\n            \n            models = {\n                \"Random Forest\": RandomForestRegressor(),\n                \"Decision Tree\": DecisionTreeRegressor(),\n                \"Gradient Boosting\": GradientBoostingRegressor(),\n                \"Linear Regression\": LinearRegression(),\n                \"XGBRegressor\": XGBRegressor(),\n                \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n                \"AdaBoost Regressor\": AdaBoostRegressor(),\n            }\n            params ={\n                \"Decision Tree\": {\n                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n                    # 'splitter':['best','random'],\n                    # 'max_features':['sqrt','log2'],\n                },\n                \"Random Forest\":{\n                    # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n                    # 'max_features':['sqrt','log2',None],\n                    'n_estimators': [8,16,32,64,128,256]\n                },\n                \"Gradient Boosting\":{\n                    # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n                    'learning_rate':[.1,.01,.05,.001],\n                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n                    # 'criterion':['squared_error', 'friedman_mse'],\n                    # 'max_features':['auto','sqrt','log2'],\n                    'n_estimators': [8,16,32,64,128,256]\n                },\n                \"Linear Regression\":{},\n                \"XGBRegressor\":{\n                    'learning_rate':[.1,.01,.05,.001],\n                    'n_estimators': [8,16,32,64,128,256]\n                },\n                \"CatBoosting Regressor\":{\n                    'depth': [6,8,10],\n                    'learning_rate': [0.01, 0.05, 0.1],\n                    'iterations': [30, 50, 100]\n                },\n                \"AdaBoost Regressor\":{\n                    'learning_rate':[.1,.01,0.5,.001],\n                    # 'loss':['linear','square','exponential'],\n                    'n_estimators': [8,16,32,64,128,256]\n                }\n                \n            }\n            \n            model_report: dict = evaluate_models(\n                X_train = X_train,\n                y_train =  y_train, \n                X_test  = X_test,\n                y_test = y_test,\n                models = models,\n                param = params\n                )\n            \n            \n            \n            # To get best model score from dict\n            best_model_score = max(sorted(model_report.values()))\n            \n            # To get best model name from dict\n            best_model_name = list(model_report.keys())[\n                list(model_report.values()).index(best_model_score)\n                ]\n            best_model = models[best_model_name]\n            \n            if best_model_score<0.6:\n                raise CustomException(\"No best model found\")\n            logging.info(f\"Best found model on both training and testing dataset\")\n            \n            save_object(\n                file_path = self.model_trainer_config.trained_model_file_path,\n                obj = best_model\n            )\n            \n            predicted = best_model.predict(X_test)\n            r2_square = r2_score(y_test,predicted)\n            \n            return r2_square\n            \n\n        except Exception as e:\n            raise CustomException(e,sys)\n\n\n\n\n\n\n\nA pipeline that interacts with the DI, DT, MT components to process the raw data available in the frontend.\n\nThis pipeline has a TrainPipeline class, which takes in the raw data and uses the train method which interacts with the DI, DT and MT components to simply return the best models r2 score in the end.\n\n\n\n\n\nTo run this pipeline, and train models, simply run the file with appropriate raw data\n\npython3 ./src/pipeline/train_pipeline.py\n# Train Pipeline File\nimport sys\nimport pandas as pd\nfrom src.logger import logging\nfrom src.exception import CustomException\nfrom src.components import data_ingestion as di\nfrom src.components import data_transformation as dt\nfrom src.components import model_trainer as mt\n\n\nclass TrainPipeline:\n    def __init__(self, raw_data_path=None):\n        self.raw_data_path = raw_data_path\n\n    def train(self):\n        try:\n            logging.info(\"Initiating data ingestion\")\n            di_obj = di.DataIngestion()\n            train_data, test_data = di_obj.initiate_data_ingestion(raw_data_path=self.raw_data_path)\n            logging.info(\"Data ingestion completed\")\n            \n            logging.info(\"Initiating data transformation\")\n            dt_obj = dt.DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n            train_arr, test_arr,_ = dt_obj.initiate_data_transformation(train_data,test_data)\n            logging.info(\"Data transformation completed and saved preprocessor object\")\n            \n            \n            logging.info(\"Training the model\")\n            mt_obj = mt.ModelTrainer()\n            print(f\"Best Models r2_score: {mt_obj.initiate_model_trainer(train_arr, test_arr)}\")\n            logging.info(\"Model training completed and saved the best model\")\n    \n        except Exception as e:\n            raise CustomException(e, sys)\n\nif __name__ == \"__main__\":\n    train_pipeline_obj = TrainPipeline(\"data/NewSPerformance.csv\")\n    train_pipeline_obj.train()\n\n\n\n\n\nPredict Pipeline: A pipeline that takes the user inputs and makes prediction on the given data by using the trained model and other objects like preprocessor obj, created via the train pipeline.\n\nThis pipeline consists of CustomData class which takes the user inputs submitted to our application and returns a data frame out of the inputs.\nPredictPipeline class, takes the CustomData class returned df object as features, scales them via the DT component generated transformer and finally, makes predictions by using the best model, from the MT component and showcases them back to the user.\n\n\n# Prediction pipeline file.\nimport sys\nimport pandas as pd\nfrom src.exception import CustomException\nfrom src.utils import load_object\nfrom src.logger import logging\n\n\nclass PredictPipeline:\n    def __init__(self):\n        pass\n    \n    def predict(self,features):\n        try:\n            logging.info(\"Predicting the data\")\n            model_path  = \"artifacts/model.pkl\"\n            preprocessor_path = \"artifacts/preprocessor.pkl\"\n            \n            model = load_object(file_path = model_path)\n            preprocessor = load_object(file_path = preprocessor_path)\n        \n            data_scaled = preprocessor.transform(features)\n            predictions = model.predict(data_scaled)\n            logging.info(\"Predictions completed\")\n            return pd.DataFrame(predictions,columns=[\"predictions\"])\n        \n        except Exception as e:\n            raise CustomException(e,sys)    \n\n\nclass CustomData:\n    def __init__(  self,\n        gender: str,\n        race_ethnicity: str,\n        parental_level_of_education,\n        lunch: str,\n        test_preparation_course: str,\n        reading_score: int,\n        writing_score: int):\n\n        self.gender = gender\n\n        self.race_ethnicity = race_ethnicity\n\n        self.parental_level_of_education = parental_level_of_education\n\n        self.lunch = lunch\n\n        self.test_preparation_course = test_preparation_course\n\n        self.reading_score = reading_score\n\n        self.writing_score = writing_score\n\n    def get_data_as_data_frame(self):\n        try:\n            logging.info(\"Creating a data frame from the custom data\")\n            custom_data_input_dict = {\n                \"gender\": [self.gender],\n                \"race_ethnicity\": [self.race_ethnicity],\n                \"parental_level_of_education\": [self.parental_level_of_education],\n                \"lunch\": [self.lunch],\n                \"test_preparation_course\": [self.test_preparation_course],\n                \"reading_score\": [self.reading_score],\n                \"writing_score\": [self.writing_score],\n            }\n            logging.info(\"Data frame created\")\n            return pd.DataFrame(custom_data_input_dict)\n\n        except Exception as e:\n            raise CustomException(e, sys)\n\nThe train and predict pipelines are a critical component of the machine learning process. The train pipeline is responsible for training the machine learning model on the training data. This process involves selecting an appropriate algorithm, fine-tuning hyperparameters, and fitting the model to the training data.\nOnce the model is trained, it is deployed to the predict pipeline, which is responsible for making predictions on new data. The predict pipeline involves processing the data, applying any necessary transformations, and using the trained model to generate predictions.\nData Scientists play a crucial role in both the train and predict pipelines. They must ensure that the training data is representative of the problem at hand, and that the model is trained and optimized to meet the desired level of accuracy. In addition, they must ensure that the predict pipeline is efficient and reliable, and that the model generates accurate predictions in real-time.\nUltimately, the train and predict pipelines are essential to the machine learning process, as they allow Data Scientists to build and deploy models that can generate valuable insights and drive business decisions."
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html",
    "href": "docs/projects/Youtube_API_EDA.html",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "With millions of users and billions of views, YouTube has become a major platform for spirituality content creators to share their knowledge and insights with a global audience. However, understanding what makes a video successful on YouTube can be a challenge, as the platform’s algorithm is complex and constantly evolving. Aspiring spirituality content creators can benefit from analyzing successful channels in their niche and identifying trends in their topics and presentation styles. In this project, we will explore the statistics of 9 popular spirituality channels on YouTube to gain insights on their audience, content, and engagement metrics.\n\n\n\nWithin this project, I would like to explore the following:\n\nGetting to know Youtube API and how to obtain video data.\nAnalyzing video data and verify different common “myths” about what makes a video do well on Youtube, for example:\n\nDoes the number of likes and comments matter for a video to get more views?\nDoes the video duration matter for views and interaction (likes/ comments)?\nDoes title length matter for views?\nHow many tags do good performing videos have? What are the common tags among these videos?\nAcross all the creators I take into consideration, how often do they upload new videos? On which days in the week?\n\nExplore the trending topics using NLP techniques\n\nWhich popular topics are being covered in the videos (e.g. using wordcloud for video titles)?\n\nWhich questions are being asked in the comment sections in the videos\n\n\n\n\n\nObtain video meta data via the API app, from top 10 youtube niche channels.\nPreprocess data and engineer aditional features for analysis\nExploratory data analysis\nConclusions\n\n\n\n\n\nCreated my own dataset usign the Google API version 3.0\nThe channels are included as per my liking and self-thoughts about spirituality.\nAlso I have chosen channels based on their subscriber counts.\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport os\nimport time\nimport numpy as np\nfrom dateutil import parser\nimport isodate\nimport datetime\n\n# Data visualization libraries\nimport matplotlib\n#matplotlib.use('TkAgg') #default backend 'module://matplotlib_inline.backend_inline'\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nsns.set(style = 'darkgrid', color_codes=True)\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150}) #fig = plt.figure(dpi=200,figsize = (16,20)) similar\n#matplotlib.use(\"TkAgg\")\n%matplotlib inline\n# #plt.rcParams['font.family'] = 'Lohit-Devanagari'\n# #plt.rcParams[\"font.path\"] = \"/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf\"\n# english_font = fm.FontProperties(family = 'Arial', size = 14)\n# #mangal_font = fm.FontProperties(fname = \"~/downloads/fonts/mangal.ttf\",size = 14)\n# #%matplotlib inline\n\n#NLP Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom wordcloud import WordCloud\n# To install wordcloud use: python -m pip install -e git+https://github.com/amueller/word_cloud#egg=wordcloud\n# Google API\nfrom googleapiclient.discovery import build\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/yuvi_dh/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/yuvi_dh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n\n\n\nCreated a project on Google Developer Console\nRequested an Authorization Credential API Key\nEnabled Youtube API for the project work to send API requests to Youtube API services.\nGot the channel ID’s from my favorite channels which I would like to get stats on.\nFinally created the functions for getting the channel stats.\n\n\n\nCode\napi_key_1 = os.environ.get('yt_1')\n#api_key_2 = os.environ.get('yt_2')\n#api_key_3 = os.environ.get('yt_3')\n#api_key = api_key_1\n#print(api_key)\n\n\n\n\nCode\nimport os\napi_key_1 = os.environ.get('yt_1')\napi_key_2 = os.environ.get('yt_2')\napi_key_3 = os.environ.get('yt_3')\napi_key = api_key_1\n#print(api_key)\n\n# channel_ids = ['UCCy2rLnGdwoJcSAtixNdsPQ', # The Sanskrit Channel\n#               'UCzszIh4jH06kYp7k_DxhH5A',  # Chinmaya Channel\n#               'UCtDAJiFT4sy42oNPA8zo0sw',  # Star Bharat\n#               'UCdsQsaeI8pQZtgrMmbjGUug',  # Swaminarayan Aksharpith\n#               'UCqFg6QnwgtVHo1iFgpxrx-A',  # Mayapur TV\n#               'UCutvkeF3tVgItCX31QhJ2Dw',  # Nova Spiritual India\n#               'UCypj9Vvizo4cCERfDFIG3zw',  # Shemaroo Bhakti Darshan\n#               'UCxoQaZS8YdKkyfBwGZay-Xg',  # Hyper Quest\n#               'UC8HRYUBXTHv4mJ67Y5FitSg']  # Rajshri Soul\nchannel_ids = [\n    \"UCgeicB5AuF3MyyUto0-M5Lw\",  # Actualized.org\n    \"UCOnnmKlDZltHAqJLz-XIpGA\",  # Universe Inside You\n    \"UC48MclMZIY_EaOQwatzCpvw\",  # Aaron Doughty\n    \"UCg3F5jxUrSvJQICen48cX4w\",  # Mindvalley\n    \"UCEcMWs6GudljuLw0-Umf97A\",  # Spirit Science\n    \"UCFVqzO9_qHVckKqNC95o9tw\",  # Gaia    \n    \"UC7IcJI8PUf5Z3zKxnZvTBog\",  # The School of Life\n    \"UCz22l7kbce-uFJAoaZqxD1A\",  # Gaur Gopal Das\n    \"UCFJZQtrh5Ksncayy2FaoNbQ\",  # Vishuddha Das\n    \"UCkJEpR7JmS36tajD34Gp4VA\",  # Psych2Go\n]\nyoutube = build('youtube', 'v3',developerKey=api_key)\n\n\n\n\n\n\nCode\ndef get_channel_stats(channel_ids,yt=youtube):\n    '''\n    Get Channel statistics: title subscriber count, view count, video count, upload playlist\n    \n    Params:\n    youtube: the build object from googleapiclient.discovery\n    channel_ids: list of channel IDs\n    \n    Returns:\n    Dataframe containing the channel statistics for all channels in the provided list\n    \n    '''\n    all_data = []\n    request = youtube.channels().list(\n        part = 'snippet,contentDetails,statistics,brandingSettings',\n        id=','.join(channel_ids))\n    response = request.execute()\n    \n    for i in range(len(response['items'])):\n        data = dict(channelName = response['items'][i]['snippet']['title'],\n                    #countryName = response['items'][i]['snippet'][\"country\"],\n                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n                    views = response['items'][i]['statistics']['viewCount'],\n                    totalVideos = response['items'][i]['statistics']['videoCount'],\n                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'],\n                   publishedAt = isodate.parse_datetime(response['items'][i]['snippet']['publishedAt']))\n                    \n        all_data.append(data)\n    return pd.DataFrame(all_data)\n\n\n\n\nCode\ndef get_video_ids(playlist_id, max_results=1500,yt = youtube):\n    \"\"\"\n    Get list of video IDs of all videos in the given playlist, up to a maximum of 1500 videos\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    playlist_id: playlist ID of the channel\n    max_results: maximum number of videos to retrieve (default: 1500)\n    \n    Returns:\n    List of video IDs of all videos in the playlist, up to the maximum number of videos specified\n    \n    \"\"\"\n    \n    request = youtube.playlistItems().list(\n                part='contentDetails',\n                playlistId = playlist_id,\n                maxResults = min(max_results, 50))\n    response = request.execute()\n    \n    video_ids = []\n    num_videos = 0\n    \n    for i in range(len(response['items'])):\n        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n        num_videos += 1\n        if num_videos >= max_results:\n            break\n        \n    next_page_token = response.get('nextPageToken')\n    more_pages = True\n    \n    while more_pages and num_videos < max_results:\n        if next_page_token is None:\n            more_pages = False\n        else:\n            request = youtube.playlistItems().list(\n                        part='contentDetails',\n                        playlistId = playlist_id,\n                        maxResults = min(max_results - num_videos, 50),\n                        pageToken = next_page_token)\n            response = request.execute()\n    \n            for i in range(len(response['items'])):\n                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n                num_videos += 1\n                if num_videos >= max_results:\n                    break\n            \n            next_page_token = response.get('nextPageToken')\n        \n    return video_ids\n\n\n\n\nCode\ndef get_video_details(video_ids,yt = youtube):\n    \"\"\"\n    Get video statistics of all videos with given IDs\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with statistics of videos, i.e.:\n        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n        'duration', 'definition', 'caption'\n    \"\"\"\n        \n    all_video_info = []\n    \n    for i in range(0, len(video_ids), 50):\n        request = youtube.videos().list(\n            part=\"snippet,contentDetails,statistics\",\n            id=','.join(video_ids[i:i+50])\n        )\n        response = request.execute() \n\n        for video in response['items']:\n            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n                             'contentDetails': ['duration', 'definition', 'caption']\n                            }\n            video_info = {}\n            video_info['video_id'] = video['id']\n\n            for k in stats_to_keep.keys():\n                for v in stats_to_keep[k]:\n                    try:\n                        video_info[v] = video[k][v]\n                    except:\n                        video_info[v] = None\n\n            all_video_info.append(video_info)\n            \n    return pd.DataFrame(all_video_info)\n\n\n\n\nCode\ndef get_comments_in_videos(video_ids, yt = youtube):\n    \"\"\"\n    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with video IDs and associated top level comment in text.\n    \n    \"\"\"\n    all_comments = []\n    \n    for video_id in video_ids:\n        try:   \n            request = youtube.commentThreads().list(\n                part=\"snippet,replies\",\n                videoId=video_id\n            )\n            response = request.execute()\n        \n            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n\n            all_comments.append(comments_in_video_info)\n            \n        except: \n            # When error occurs - most likely because comments are disabled on a video\n            print('Could not get comments for video ' + video_id)\n        \n    return pd.DataFrame(all_comments) \n\n\n\n\n\nUsing the get_channel_stats function defined below, now we are going to obtain the channel statistics for the above channels in scope\n\n\nCode\nchannel_data = get_channel_stats(channel_ids)\nchannel_data.to_csv(\"./files/spiritual_channel_data_original.csv\")\n\n\n\n\nCode\n#channel_data\n\n\n\n\nCode\n# Creation of copy so that I save a dummy df and also a csv, to not keep reusing youtube credits i.e. 10k per day.\nl_channel_data = pd.read_csv(\"./files/spiritual_channel_data_original.csv\",index_col=0)\nl_channel_data\n\n\n\n\n\n\n  \n    \n      \n      channelName\n      subscribers\n      views\n      totalVideos\n      playlistId\n      publishedAt\n    \n  \n  \n    \n      0\n      Aaron Doughty\n      1450000\n      141121587\n      1793\n      UU48MclMZIY_EaOQwatzCpvw\n      2014-07-10 04:24:58+00:00\n    \n    \n      1\n      The School of Life\n      8380000\n      820210489\n      902\n      UU7IcJI8PUf5Z3zKxnZvTBog\n      2010-05-18 16:46:57+00:00\n    \n    \n      2\n      Psych2Go\n      10800000\n      1477376571\n      2380\n      UUkJEpR7JmS36tajD34Gp4VA\n      2014-10-05 06:27:31+00:00\n    \n    \n      3\n      Gaia\n      1520000\n      127649328\n      645\n      UUFVqzO9_qHVckKqNC95o9tw\n      2008-08-06 15:26:41+00:00\n    \n    \n      4\n      Universe Inside You\n      1790000\n      118060252\n      126\n      UUOnnmKlDZltHAqJLz-XIpGA\n      2017-03-30 10:55:39+00:00\n    \n    \n      5\n      Mindvalley\n      2260000\n      468381830\n      1662\n      UUg3F5jxUrSvJQICen48cX4w\n      2014-04-23 08:21:13+00:00\n    \n    \n      6\n      Gaur Gopal Das\n      4860000\n      288878180\n      356\n      UUz22l7kbce-uFJAoaZqxD1A\n      2016-04-12 18:16:24+00:00\n    \n    \n      7\n      Vishuddha Das\n      596000\n      40838427\n      381\n      UUFJZQtrh5Ksncayy2FaoNbQ\n      2014-09-24 14:04:59+00:00\n    \n    \n      8\n      Actualized.org\n      1100000\n      107549192\n      528\n      UUgeicB5AuF3MyyUto0-M5Lw\n      2012-01-23 20:08:34+00:00\n    \n    \n      9\n      Spirit Science\n      1300000\n      79311991\n      306\n      UUEcMWs6GudljuLw0-Umf97A\n      2011-12-29 05:49:29+00:00\n    \n  \n\n\n\n\n\n\nCode\n# Copy used for further manipulation and original of l_channel_data can be used to load this chdd any time.\nchdd = l_channel_data.copy()\nchdd\n\n\n\n\n\n\n  \n    \n      \n      channelName\n      subscribers\n      views\n      totalVideos\n      playlistId\n      publishedAt\n    \n  \n  \n    \n      0\n      Aaron Doughty\n      1450000\n      141121587\n      1793\n      UU48MclMZIY_EaOQwatzCpvw\n      2014-07-10 04:24:58+00:00\n    \n    \n      1\n      The School of Life\n      8380000\n      820210489\n      902\n      UU7IcJI8PUf5Z3zKxnZvTBog\n      2010-05-18 16:46:57+00:00\n    \n    \n      2\n      Psych2Go\n      10800000\n      1477376571\n      2380\n      UUkJEpR7JmS36tajD34Gp4VA\n      2014-10-05 06:27:31+00:00\n    \n    \n      3\n      Gaia\n      1520000\n      127649328\n      645\n      UUFVqzO9_qHVckKqNC95o9tw\n      2008-08-06 15:26:41+00:00\n    \n    \n      4\n      Universe Inside You\n      1790000\n      118060252\n      126\n      UUOnnmKlDZltHAqJLz-XIpGA\n      2017-03-30 10:55:39+00:00\n    \n    \n      5\n      Mindvalley\n      2260000\n      468381830\n      1662\n      UUg3F5jxUrSvJQICen48cX4w\n      2014-04-23 08:21:13+00:00\n    \n    \n      6\n      Gaur Gopal Das\n      4860000\n      288878180\n      356\n      UUz22l7kbce-uFJAoaZqxD1A\n      2016-04-12 18:16:24+00:00\n    \n    \n      7\n      Vishuddha Das\n      596000\n      40838427\n      381\n      UUFJZQtrh5Ksncayy2FaoNbQ\n      2014-09-24 14:04:59+00:00\n    \n    \n      8\n      Actualized.org\n      1100000\n      107549192\n      528\n      UUgeicB5AuF3MyyUto0-M5Lw\n      2012-01-23 20:08:34+00:00\n    \n    \n      9\n      Spirit Science\n      1300000\n      79311991\n      306\n      UUEcMWs6GudljuLw0-Umf97A\n      2011-12-29 05:49:29+00:00\n    \n  \n\n\n\n\n\n\nCode\n# Setting Numeric n Categorical columns\nnumeric_cols = ['subscribers','views','totalVideos']\nchdd[numeric_cols] = chdd[numeric_cols].apply(pd.to_numeric,errors = 'coerce')\n\n# Convert publishedAt column to datetime\nchdd['publishedAt'] =(pd.to_datetime(chdd['publishedAt']))\n\n# Extract year, month, and time into separate columns\nchdd['publishingYear'] = chdd['publishedAt'].dt.year\nchdd['publishingMonth'] = chdd['publishedAt'].dt.month\nchdd['publishingTime'] = chdd['publishedAt'].dt.time\n\n# Get month name\nchdd['publishingMonthName'] = chdd['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nchdd.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\n# chdd['channelName'] = chdd['channelName'].replace('STAR भारत','Star Bharat')\n# chdd \n# Was for other spiritual channels, but isn't necessary now.\n\n\n\n\nCode\nchdd.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   channelName          10 non-null     object\n 1   subscribers          10 non-null     int64 \n 2   views                10 non-null     int64 \n 3   totalVideos          10 non-null     int64 \n 4   playlistId           10 non-null     object\n 5   publishingYear       10 non-null     int64 \n 6   publishingMonth      10 non-null     int64 \n 7   publishingTime       10 non-null     object\n 8   publishingMonthName  10 non-null     object\ndtypes: int64(5), object(4)\nmemory usage: 800.0+ bytes\n\n\n\n\n\n\n\nCode\nmatplotlib.get_backend()\n\n\n'module://matplotlib_inline.backend_inline'\n\n\n\n\nCode\n#matplotlib.use??\n#sns.barplot??\n\n\n\n\nCode\n# Fixing colors for each channel\n#palette = sns.color_palette('pastel6', n_colors=10)\ncolors = plt.cm.tab10.colors[:10]\n#colors = sns.color_palette('Set1', 10)\nchannel_colors = {}\nchdd.sort_values('subscribers',ascending=False,inplace=True)\nfor i, channel in enumerate(chdd['channelName']):\n    channel_colors[channel] = colors[i]\n\n\n\n\nCode\nax = sns.barplot(x='channelName', y='subscribers', data=chdd.sort_values('subscribers', ascending=False), palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n#%matplotlib inline\nax = sns.barplot(x='channelName', y='views', data=chdd.sort_values('views', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, some channels have more subscribers but less views and vice versa. For example, GGD channel has significantly more subscribers than Mind Valley channel, but less views in total.\nPsych2Go and The School of Life hold onto their ranks in both views and subscriber count\n\n\n\n\n\nIn the next step, we will obtain the video statistics for all the channels. In total, we obtained 8700 videos as seen in below.\n\n\nCode\n# # Create a dataframe with video statistics and comments from all channels\n# video_df = pd.DataFrame()\n# comments_df = pd.DataFrame()\n\n# for c in channel_data['channelName'].unique():\n#     print(\"Getting video information from channel: \" + c)\n#     playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n#     video_ids = get_video_ids(playlist_id,max_results=2000,yt = youtube)\n    \n#     # get video data\n#     video_data = get_video_details(video_ids,yt = youtube)\n#     # get comment data\n#     comments_data = get_comments_in_videos(video_ids,yt = youtube)\n\n#     # append video data together and comment data toghether\n#     video_df = video_df.append(video_data, ignore_index=True)\n#     comments_df = comments_df.append(comments_data, ignore_index=True)\n\n\n\n\nCode\n# video_df.to_csv(\"./files/spirituality_video_df_original.csv\")\n# comments_df.to_csv(\"./files/spirituality_comments_df_original.csv\")\n\n\n\n\nCode\nl_video_df = pd.read_csv(\"./files/spirituality_video_df_original.csv\",index_col=0)\nl_comments_df = pd.read_csv(\"./files/spirituality_comments_df_original.csv\",index_col=0)\n\n\n\n\nCode\nviddf = l_video_df.copy()\ncomdf = l_comments_df.copy()\n\n\n\n\nCode\n# Create publish day (in the week) column\nviddf['publishedAt'] =  viddf['publishedAt'].apply(lambda x: parser.parse(x)) \nviddf['pushblishDayName'] = viddf['publishedAt'].apply(lambda x: x.strftime(\"%A\"))\n\n# Convert publishedAt column to datetime\nviddf['publishedAt'] =(pd.to_datetime(viddf['publishedAt']))\n\n\n# Extract year, month, and time into separate columns\nviddf['publishingYear'] = viddf['publishedAt'].dt.year\nviddf['publishingMonth'] = viddf['publishedAt'].dt.month\nviddf['publishingTime'] = viddf['publishedAt'].dt.time\n\n# Get month name\nviddf['publishingMonthName'] = viddf['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nviddf.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\nviddf\n\n\n\n\n\n\n  \n    \n      \n      video_id\n      channelTitle\n      title\n      description\n      tags\n      viewCount\n      likeCount\n      favouriteCount\n      commentCount\n      duration\n      definition\n      caption\n      pushblishDayName\n      publishingYear\n      publishingMonth\n      publishingTime\n      publishingMonthName\n    \n  \n  \n    \n      0\n      Q8dOR0bN-Mw\n      Gaia\n      Man Able to Project Thoughts Through Crystals\n      Learn how crystals can hold a powerful place i...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      7218.0\n      1309.0\n      NaN\n      31.0\n      PT1M\n      hd\n      False\n      Saturday\n      2023\n      4\n      15:00:28\n      April\n    \n    \n      1\n      Ja1m4mHjZJY\n      Gaia\n      FULL EPISODE: Channeling - A Bridge to the Beyond\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      18104.0\n      1394.0\n      NaN\n      113.0\n      PT26M53S\n      hd\n      False\n      Friday\n      2023\n      3\n      16:00:07\n      March\n    \n    \n      2\n      qixrU_pwvD0\n      Gaia\n      This Man Taught Princess Diana to Express Hers...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      9462.0\n      640.0\n      NaN\n      51.0\n      PT4M47S\n      hd\n      False\n      Wednesday\n      2023\n      3\n      16:00:40\n      March\n    \n    \n      3\n      deZsy9GYn8w\n      Gaia\n      Mysterious, Ancient Satellite Is Monitoring Earth\n      Five of the world’s leading experts unravel my...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      73915.0\n      10072.0\n      NaN\n      338.0\n      PT1M\n      hd\n      False\n      Tuesday\n      2023\n      3\n      15:00:21\n      March\n    \n    \n      4\n      Kbfpd8zp3mk\n      Gaia\n      How Shamanic Dancing Leads to Altered States o...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      14585.0\n      932.0\n      NaN\n      54.0\n      PT3M56S\n      hd\n      False\n      Monday\n      2023\n      3\n      16:00:21\n      March\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8695\n      ZS3sOfs5jBY\n      Actualized.org\n      Get Coached\n      http://www.actualized.org/coaching\\n\\nResults ...\n      ['life coaching', 'Coaching (Profession)']\n      10102.0\n      323.0\n      NaN\n      31.0\n      PT16M23S\n      hd\n      False\n      Saturday\n      2013\n      5\n      11:48:19\n      May\n    \n    \n      8696\n      mlIYVsuIofs\n      Actualized.org\n      Be Different to Be Successful\n      How doing things differently in your life is n...\n      NaN\n      27343.0\n      929.0\n      NaN\n      41.0\n      PT26M5S\n      hd\n      False\n      Friday\n      2013\n      4\n      10:24:43\n      April\n    \n    \n      8697\n      8cbtMhHpLC8\n      Actualized.org\n      Why Life Coaching Works\n      An explanation of how coaching works and why i...\n      NaN\n      20081.0\n      647.0\n      NaN\n      42.0\n      PT21M\n      hd\n      False\n      Friday\n      2013\n      4\n      08:53:35\n      April\n    \n    \n      8698\n      C1QYF5WYzCo\n      Actualized.org\n      How to Invest In Yourself\n      How a long-term investment mindset in yourself...\n      NaN\n      61685.0\n      1805.0\n      NaN\n      193.0\n      PT19M13S\n      hd\n      True\n      Friday\n      2013\n      4\n      07:29:27\n      April\n    \n    \n      8699\n      _874QVgwvEk\n      Actualized.org\n      Mastery Part 1\n      Test video for self-development blog. This vid...\n      ['mastery', 'self-help', 'self-development', '...\n      5496.0\n      217.0\n      NaN\n      63.0\n      PT16M12S\n      hd\n      False\n      Wednesday\n      2012\n      8\n      08:24:31\n      August\n    \n  \n\n8700 rows × 17 columns\n\n\n\nLet’s take a look at the comment_df as well. We only get 8674 comments in total due to the fact that we limited to 10 first comments on the video to avoid exceeding the Youtube API quota limit.\n\n\nCode\ncomdf\n\n\n\n\n\n\n  \n    \n      \n      video_id\n      comments\n    \n  \n  \n    \n      0\n      Q8dOR0bN-Mw\n      ['Intentionally concentrating the mind setting...\n    \n    \n      1\n      Ja1m4mHjZJY\n      ['8,000+ Films, Shows & Classes on Gaia. Start...\n    \n    \n      2\n      qixrU_pwvD0\n      ['8,000+ Films, Shows & Classes on Gaia. Start...\n    \n    \n      3\n      deZsy9GYn8w\n      ['Watch more of the Awakening Conference with ...\n    \n    \n      4\n      Kbfpd8zp3mk\n      ['8,000+ Films, Shows & Classes on Gaia. Start...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      8669\n      ZS3sOfs5jBY\n      ['Thanks', 'A legend is born..', 'Currently go...\n    \n    \n      8670\n      mlIYVsuIofs\n      ['Great video 💫', 'Inspiring', 'Thanks', 'Than...\n    \n    \n      8671\n      8cbtMhHpLC8\n      ['Thanks.', '0:00 a legend was born', 'Magic.'...\n    \n    \n      8672\n      C1QYF5WYzCo\n      ['Thanks', \"If you discovered this/him you've ...\n    \n    \n      8673\n      _874QVgwvEk\n      ['Proud of you man. You helped me a lot', 'Who...\n    \n  \n\n8674 rows × 2 columns\n\n\n\n\n\n\n\nTo be able to make use of the data for analysis, we need to perform a few pre-processing steps. Firstly, I would like reformat some columns, especially the date and time columns such as “pushlishedAt” and “duration”. In addition, I also think it is necessary to enrich the data with some new features that might be useful for understanding the videos’ characteristics. Also I removed the favorite count column as it’s completely blank, rest other columns having null values weren’t modified for simplicity.\n\n\n\n\nCode\nviddf.isnull().sum(axis = 0)\n\n\nvideo_id                  0\nchannelTitle              0\ntitle                     0\ndescription               3\ntags                    554\nviewCount                 5\nlikeCount                 6\nfavouriteCount         8700\ncommentCount              8\nduration                  0\ndefinition                0\ncaption                   0\npushblishDayName          0\npublishingYear            0\npublishingMonth           0\npublishingTime            0\npublishingMonthName       0\ndtype: int64\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nax = sns.heatmap(viddf.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nviddf.publishingYear.sort_values().value_counts()\n# Videos are from 2011 to 2023\n#- During the covid time the channels became more active it seems over all.\n\n\n2022    1667\n2021    1355\n2020    1236\n2018    1027\n2019     950\n2017     863\n2016     516\n2023     392\n2015     355\n2014     248\n2013      45\n2012      44\n2011       2\nName: publishingYear, dtype: int64\n\n\n\n\n\n\n\nCode\ncols = ['viewCount', 'likeCount','commentCount']\nviddf[cols] = viddf[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n\n\n\n\nI want to enrich the data for further analyses, for example: - convert video duration to seconds instead of the current default string format - calculate number of tags for each video - calculate comments and likes per 1000 view ratio - calculate title character length\n\n\nCode\n# convert duration to seconds\nviddf['durationSecs'] = viddf['duration'].apply(lambda x: isodate.parse_duration(x))\nviddf['durationSecs'] = viddf['durationSecs'].astype('timedelta64[s]')\n\n\n\n\nCode\n# Add number of tags\nviddf['tagsstr'] = viddf.tags.apply(lambda x: 0 if x is None else str((x))) #tags were not in proper format so converting them to str\nviddf['tagsCount'] = viddf.tagsstr.apply(lambda x: 0 if (x == 0 or x =='nan') else len(eval(x)))\n\n\n\n\nCode\n# Comments and likes per 1000 view ratio\nviddf['likeRatio'] = viddf['likeCount']/ viddf['viewCount'] * 1000\nviddf['commentRatio'] = viddf['commentCount']/ viddf['viewCount'] * 1000\n\n\n\n\nCode\n# Title character length\nviddf['titleLength'] = viddf['title'].apply(lambda x: len(x))\n\n\n\n\nCode\n# Dropping the favourite Count as all of it is empty\nviddf.drop(['favouriteCount'],axis = 1, inplace=True)\n\n\n\n\nCode\n#Observing df before proceeding further\nviddf \n\n\n\n\n\n\n  \n    \n      \n      video_id\n      channelTitle\n      title\n      description\n      tags\n      viewCount\n      likeCount\n      commentCount\n      duration\n      definition\n      ...\n      publishingYear\n      publishingMonth\n      publishingTime\n      publishingMonthName\n      durationSecs\n      tagsstr\n      tagsCount\n      likeRatio\n      commentRatio\n      titleLength\n    \n  \n  \n    \n      0\n      Q8dOR0bN-Mw\n      Gaia\n      Man Able to Project Thoughts Through Crystals\n      Learn how crystals can hold a powerful place i...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      7218.0\n      1309.0\n      31.0\n      PT1M\n      hd\n      ...\n      2023\n      4\n      15:00:28\n      April\n      60.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      181.352175\n      4.294819\n      45\n    \n    \n      1\n      Ja1m4mHjZJY\n      Gaia\n      FULL EPISODE: Channeling - A Bridge to the Beyond\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      18104.0\n      1394.0\n      113.0\n      PT26M53S\n      hd\n      ...\n      2023\n      3\n      16:00:07\n      March\n      1613.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      76.999558\n      6.241715\n      49\n    \n    \n      2\n      qixrU_pwvD0\n      Gaia\n      This Man Taught Princess Diana to Express Hers...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      9462.0\n      640.0\n      51.0\n      PT4M47S\n      hd\n      ...\n      2023\n      3\n      16:00:40\n      March\n      287.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      67.638977\n      5.389981\n      63\n    \n    \n      3\n      deZsy9GYn8w\n      Gaia\n      Mysterious, Ancient Satellite Is Monitoring Earth\n      Five of the world’s leading experts unravel my...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      73915.0\n      10072.0\n      338.0\n      PT1M\n      hd\n      ...\n      2023\n      3\n      15:00:21\n      March\n      60.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      136.264628\n      4.572820\n      49\n    \n    \n      4\n      Kbfpd8zp3mk\n      Gaia\n      How Shamanic Dancing Leads to Altered States o...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      14585.0\n      932.0\n      54.0\n      PT3M56S\n      hd\n      ...\n      2023\n      3\n      16:00:21\n      March\n      236.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      63.901268\n      3.702434\n      61\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8695\n      ZS3sOfs5jBY\n      Actualized.org\n      Get Coached\n      http://www.actualized.org/coaching\\n\\nResults ...\n      ['life coaching', 'Coaching (Profession)']\n      10102.0\n      323.0\n      31.0\n      PT16M23S\n      hd\n      ...\n      2013\n      5\n      11:48:19\n      May\n      983.0\n      ['life coaching', 'Coaching (Profession)']\n      2\n      31.973867\n      3.068699\n      11\n    \n    \n      8696\n      mlIYVsuIofs\n      Actualized.org\n      Be Different to Be Successful\n      How doing things differently in your life is n...\n      NaN\n      27343.0\n      929.0\n      41.0\n      PT26M5S\n      hd\n      ...\n      2013\n      4\n      10:24:43\n      April\n      1565.0\n      nan\n      0\n      33.975789\n      1.499470\n      29\n    \n    \n      8697\n      8cbtMhHpLC8\n      Actualized.org\n      Why Life Coaching Works\n      An explanation of how coaching works and why i...\n      NaN\n      20081.0\n      647.0\n      42.0\n      PT21M\n      hd\n      ...\n      2013\n      4\n      08:53:35\n      April\n      1260.0\n      nan\n      0\n      32.219511\n      2.091529\n      23\n    \n    \n      8698\n      C1QYF5WYzCo\n      Actualized.org\n      How to Invest In Yourself\n      How a long-term investment mindset in yourself...\n      NaN\n      61685.0\n      1805.0\n      193.0\n      PT19M13S\n      hd\n      ...\n      2013\n      4\n      07:29:27\n      April\n      1153.0\n      nan\n      0\n      29.261571\n      3.128800\n      25\n    \n    \n      8699\n      _874QVgwvEk\n      Actualized.org\n      Mastery Part 1\n      Test video for self-development blog. This vid...\n      ['mastery', 'self-help', 'self-development', '...\n      5496.0\n      217.0\n      63.0\n      PT16M12S\n      hd\n      ...\n      2012\n      8\n      08:24:31\n      August\n      972.0\n      ['mastery', 'self-help', 'self-development', '...\n      5\n      39.483261\n      11.462882\n      14\n    \n  \n\n8700 rows × 22 columns\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.set(rc={'figure.figsize':(10,12),'figure.dpi':100})\nax = sns.barplot(x='channelName', y='totalVideos', data=chdd.sort_values('totalVideos', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# group by channel and year, count the videos\ncount_data = viddf.groupby(['channelTitle', 'publishingYear'])['video_id'].count().reset_index(name=\"count\")\n\n# plot using seaborn\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150,})\nax = sns.barplot(data=count_data, x='publishingYear', y='count', hue='channelTitle', palette=channel_colors)\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., ncol=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWith the video statistics for all channel, now we can see how the views are distributed per channel. - Boxplot provides us with the following insights: - Mindvalley and Psychgo have too many videos that went viral over time. - The School of life and Universe inside you seem to have more broad range of video views over time as their IQR seems more via the box length. - I think the views and subscriber count are correlated since the channel having more subscribers tend to have more views.\n\nViolinplot confirms us that few channels seem to have quite variation among views of the videos, like Aaron Daughty and Vishuddha Das videos have recieved more varied views for the videos.\n\n\n\n\n\n\nCode\nax = sns.violinplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\n#ax.set_ylim(ymin = -1e3, ymax = 1e5)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nax = sns.boxplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\nax.set_ylim(ymin = -1e3, ymax = 2.4e6)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFirstly, I would like to check if comments and likes do correlate with how many views a video would get. In the plots below, it can be observed that the number of views and number of comments/ likes strongly correlated with each other. The number of likes seems to suggest stronger correlation than the number of comments. However, this is expected as the more people watching a video, the more likely this video will get comments and likes. To correct for this factor, we will plot these relationships again using the comments per 1000 view and likes per 1000 view ratios.\n\n\n\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentCount\", y = \"viewCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"likeCount\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\nNow we will take a look at the correlation if we look at the comment ratio and like ratio instead of the absolute number. It seems that more views is leading to more comments and more likes as well, but after a certain point I think, with views viewers, don’t write comments that much.\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentRatio\", y = \"viewCount\", ax=ax[0])\n#ax[0].set_ylim(0,9e6)\n#ax[1].set_ylim(0,9e6)\nsns.scatterplot(data = viddf, x = \"likeRatio\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in the histogram below, most videos are between 1600 to 1800 seconds, which is about 20 to 30 minutes. Here I have to limit the duration to 10,000 because of some really long videos (potentially streaming videos).\n\n\n\n\n\nCode\nax = sns.histplot(data=viddf[viddf['durationSecs'] < 10000], x=\"durationSecs\", bins=30, color=\"#9368b7\")\nplt.show()\n\n\n\n\n\nNow we plot the duration against comment count and like count. It can be seen that actually shorter videos tend to get more likes and comments than very long videos.\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nfig, ax =plt.subplots(1,2)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"commentCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"likeCount\", ax=ax[1])\n#ax[0].set_ylim(0,1e5)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n#ax[1].set_ylim(0,1e5)\nax[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThere is no clear relationship between title length and views as seen the scatterplot below, but most-viewed videos tend to have average title length of 35-60 characters\n\n\nCode\nax = sns.scatterplot(data = viddf, x = \"titleLength\", y = \"viewCount\")\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_ylim(0,1e7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAs I’m interested to see what the creators are making videos about and which terms most frequently appear in their video titles, I will create a wordcloud for the most common words. We first need to remove the stopwords such as “you”, “I”, “the”, etc. which do note contribute a lot to the meaning of the title. It can be seen that the main words posted in title are Life, Attraction, Love, Meditation, People and Thing.\n\n\nCode\nstop_words = set(stopwords.words('english'))\nviddf['title_no_stopwords'] = viddf['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in viddf['title_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(30, 20))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n\nwordcloud = WordCloud(width = 1920, height = 780, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()\n\n\n\n\n\n\n\nCode\n# for i in sorted(wordcloud.words_.items(), key = lambda x: x[1],reverse = False):\n#     if i[0] == 'sign gaur':\n#         print(i)\n\n\n\n\n\nIt seems that most videos have between 10 and 45 tags. The relationship between number of tags and view count is not clearly seen, but too few tags or too many tags do seem to correlate with fewer views.\n\n\nCode\nplot = sns.scatterplot(data = viddf, x = \"tagsCount\", y = \"viewCount\")\nplot.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIt’s interesting to see that more videos are uploaded on Mondays, Wednesdays and Fridays. It seems the pattern is alternative in uploading the videos. This might be because of maintaining a consistency on channel, like when the user can more expect the videos, on a consistent basis.\n\n\nCode\nday_df = pd.DataFrame(viddf['pushblishDayName'].value_counts())\nweekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nday_df = day_df.reindex(weekdays)\nax = day_df.reset_index().plot.bar(x='index', y='pushblishDayName', rot=0)\nplt.xlabel(\"Days\")\nplt.ylabel(\"Counts\")\nplt.legend(labels = [\"Counts\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLike the video titles, the video comments also revolve around Love, Life, Sign, Thing, Attraction words\n\n\nCode\nstop_words = set(stopwords.words('english'))\ncomdf['comments_no_stopwords'] = comdf['comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in comdf['comments_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nwordcloud = WordCloud(width = 1980, height = 720, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()"
  },
  {
    "objectID": "Self_Instructions.html",
    "href": "Self_Instructions.html",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "",
    "text": "References\n\nRead Later\n\nGithub_Actions\nGithub_Pages\n\n\n\n\nPublishing Content\n\nCreate a gh-pages branch\n\ngit checkout –orphan gh-pages #Creation of new branch\ngit reset –hard # make sure you’ve committed changes before running this! # This makes the gh-pages branch as the current one.\ngit commit –allow-empty -m “Initialising gh-pages branch”\ngit push  gh-pages #Remember for current branch the repo_name is my_repo, but by default the name is origin.\nFor publishing to github pages follow the site:https://quarto.org/docs/publishing/github-pages.html\n\nOnce this is done do gh-configuration on github repo.\n\ngithub-pages_config\n\nPublish first to Quarto Pub\n\nAdd the following content to _publish.yml in the main directory\nRun the command quarto publish with Quarto-Pub and follow the authorization steps.\n\nPublish to gh-pages\n\nCreate a publish.yml file in .github/workflows directory with the following content\nAdd _site and .quarto to .gitignore file to ignore those folders staging.\nTo render codes, setup a virtual env and update the publish.yml.\nRef: Executing Codes, Quarto setting up virtual_envs\n\n\n\nimgs\n\n  \n\n\nYAML Codes\n\n- source: project\n    quarto-pub:\n        - id: 1a303cb3-ce18-42f5-85e8-873316e2d3d8\n        url: 'https://quartopub.com/sites/yuvraj-dhepe/data-science-blog'\n\non:\n  workflow_dispatch:\n  push:\n    branches: master\n\nname: Quarto Publish on Github Pages\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n        with:\n          node-version: 16.x\n        # Update Node.js version to 16\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2 #using quarto-development github pages to use actions for gh-pages posting. \n        with:\n          # to install LaTeX to build PDF book\n          tinytex: true\n\n      - name: Render and Publish #Publishing to github pages and rendering.\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions so no need to add it again in secrets"
  }
]