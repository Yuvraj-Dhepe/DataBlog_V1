{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e40b221-dadd-4143-b6af-f77a2108eccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Introduction to RL HF\"\n",
    "description: \"Unit 1 Learning from Hugging Face RL Course\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "render-on-save: true\n",
    "execute:\n",
    "    eval: false\n",
    "    echo: true\n",
    "jupyter: python3\n",
    "output:\n",
    "  quarto::html_document:\n",
    "    self_contained: false\n",
    "    keep_md: false\n",
    "\n",
    "categories:\n",
    "    - End To End Project\n",
    "    - Regression Project\n",
    "image: ./images/RL1_Introduction.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2ef30-5563-4f58-b53a-efef994c46d6",
   "metadata": {},
   "source": [
    "# Hugging Face Reinforcement Learning course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760d75b-6010-4a7b-8e52-c6138ce405b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chapter 1: INTRODUCTION TO DEEP REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eed186-0436-4e5d-b9a5-c832924d20f9",
   "metadata": {},
   "source": [
    "- Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback\n",
    "- RL Process: Imagine an agent learning to play a platform game:\n",
    "    - Our agent receives state $S_0$ from the environment - we receive the first frame of our game\n",
    "    - Based on that state $S_0$, the Agent takes action $A_0$ - our agent will move to the right\n",
    "    - The environment goes to a new state $S_1$ - new frame\n",
    "    - The environment gives some reward $R_1$ to the agent - we're not dead (Positive Reward +1)\n",
    "- This RL loop outputs a sequence of state, action, reward, and next state: $S_0, A_0, R_1, S_1$\n",
    "- The agent's goal is to maximize it's cumulative reward, callled the expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1168c5e-c6ba-46ac-b0aa-1598601e59b0",
   "metadata": {},
   "source": [
    "### The reward hypothesis: the central idea of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91212a-a0f1-49f2-ab13-87e42313c0ae",
   "metadata": {},
   "source": [
    "- RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\n",
    "- That’s why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb7ec3-b364-47ae-859f-265ec86b84bb",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "- Markov property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a34a45-0425-4668-987e-bfd7bdd1bda4",
   "metadata": {},
   "source": [
    "### Observations/States Space\n",
    "- Observations/States are the information our agent gets from he environment. In the case of a video game, it can be a frame, in case of a trading agent, it can be the value of a certain stock.\n",
    "- There is a differentiation to make between observation and state, however:\n",
    "    - State s: is a complete description of the state of the world.\n",
    "        - In a chess game, we have access to the whole board information, so we receive a state from the environment. In other words, the environment is fully observed.\n",
    "    - Observation o: is a partial description of the state. \n",
    "        - In Super Mario Bros, we are in a partially observed environment. We receive an observation since we only see a part of the level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cc71b-a79d-41db-9c91-a49edd43b7b4",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "- Action space is the set of all possible actions in an environment.\n",
    "    - The actions can come from a discrete or continuous space:\n",
    "        - Discrete space: the number of possible actions is finite. Ex. In Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.\n",
    "        - Continous space: A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21,1°, 21,2°, honk, turn right 20°.\n",
    "    - *Taking this information into consideration is crucial because it will have importance when choosing the RL algorithm in the future.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1968f1c-15df-4766-8ac9-262b1ecfcf89",
   "metadata": {},
   "source": [
    "### Rewards and the discounting\n",
    "- The rewared is fundamental in RL because it's the only feedback for the agent. Because of this our agent knows if the action taken was good or not.\n",
    "    - The cumulative reward at each time step t, equals the sum of all rewards in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb282ad-2ff8-442e-9068-f2d07d113989",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/RL_1_rewards_2.jpg\" alt=\"Alt text\" title=\"Optional title\" width = 200 height = 300>\n",
    "\n",
    "However, in reality, we can’t just add them like that. The rewards that come sooner (at the beginning of the game) are more likely to happen since they are more predictable than the long-term future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fac2ac-c504-4189-aa55-d60a4bbee0ab",
   "metadata": {},
   "source": [
    "<img src=\"./images/RL_1_rewards_3.jpg\" alt=\"Alt text\" title=\"Optional title\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10944c1-3e77-4c47-b15f-81124b7e337f",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Let’s say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (that can move too). The mouse’s goal is to eat the maximum amount of cheese before being eaten by the cat.\n",
    "- As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).\n",
    "- Consequently, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we’re not really sure we’ll be able to eat it.\n",
    "\n",
    "- To discount the rewards, we proceed like this:\n",
    "    - 1. We define a discount rate called gamma. It must be between 0 and 1. Most of the time between 0.95 and 0.99.\n",
    "        - The larger the gamma, the smaller the discount. This means our agent cares more about the long-term reward.\n",
    "        - On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n",
    "    - 2. Then, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.\n",
    "\n",
    "Our discounted expected cumulative reward is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cdde7-1590-4cfe-85bf-53946e34dcd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/RL_1_rewards_4.jpg\" alt=\"Alt text\" title=\"Optional title\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67d7c4-b2c6-4f3b-99a0-32492711924d",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050785f-3e0f-47b3-a54c-d1016c25963d",
   "metadata": {},
   "source": [
    "- A task is an instance of a Reinforcement learning problem. We can have 2 types of tasks: episodic and continuing.\n",
    "- Episodic task\n",
    "    - In this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States.\n",
    "        - For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ends when you’re killed or you reached the end of the level.\n",
    "- Continuing task:\n",
    "    - These are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.\n",
    "        - For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e8d5a-3036-486b-9230-cdef26019afe",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5de30c-681a-4001-87de-daf30fc08e35",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Exploration is exploring the environment by trying random actions in order to find more information about the environment\n",
    "- Exploitation is exploiting known information to maximize the reward\n",
    "- **Remember the goal of our RL agent is to maximize the expected cumulative reward. However, one can fall in the trap of exploiting the known rewards all the time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb3ac1-e159-403f-8b40-be29a6db8369",
   "metadata": {},
   "source": [
    "Example\n",
    "- In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).\n",
    "    - However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).\n",
    "    - But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).\n",
    "    \n",
    "- If it’s still confusing, think of a real problem: the choice of picking a restaurant:\n",
    "    - Exploitation: You go to the same one that you know is good every day and take the risk to miss another better restaurant.\n",
    "    - Exploration: Try restaurants you never went to before, with the risk of having a bad experience but the probable opportunity of a fantastic experience.\n",
    "\n",
    "- This is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment.\n",
    "    - Therefore, we must define a rule that helps to handle this trade-off. We’ll see the different ways to handle it in the future units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59061db-da30-4066-a62a-0b404aa74664",
   "metadata": {},
   "source": [
    "### Two main approaches to solving the RL problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d7ada-caf3-4b77-b6e9-1fc143b95320",
   "metadata": {},
   "source": [
    "- After taking a look at the RL framework \n",
    "    - RL process which consists of:\n",
    "        - Observations or States Space\n",
    "        - Action Space\n",
    "        - Rewards and it's discounting\n",
    "        - Tasks, i.e. an instance of reinforcement learning. Episodic or Continuing tasks\n",
    "    - Reward Hypothesis: Every goal can be described as a maximization of the expected return.\n",
    "    - Markov Property\n",
    "    - Exploration vs Exploitation.\n",
    "- We now have to see how this whole RL framework can be used to solve the RL problems\n",
    "    - In other words, how do we build an RL agent that can select the actions that maximize it's expected cumulative rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865ca02-eaa6-4f99-9a3e-50448a075000",
   "metadata": {},
   "source": [
    "- Policy $\\pi$\n",
    "    - The brain of our agent defining the behaviour of our agent\n",
    "    - Describes which action to take in which state\n",
    "    - This is what we want to learn to solve the RL problem via the framework.\n",
    "    - 2 Approaches to find the policy: Policy Based Methods and Value Based Methods\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98866e2-eb3d-4be0-874d-50daab544607",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Policy based methods:\n",
    "    - Learn the policy function directly.\n",
    "        - This function defines a mapping from each state to the best corresponding action or a probability distribution over the set of all the possible actions at that state\n",
    "        - There are 2 types of policy, \n",
    "            - Deterministic policy: Given a state this policy returns the same action\n",
    "            - Stochastic policy: Outputs a probability distribution over all the actions in a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841da7c4-ce0c-49e7-b3b7-4f8f65950324",
   "metadata": {},
   "source": [
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072ed6c-dade-4b8c-9d24-2e10ef5c2f06",
   "metadata": {},
   "source": [
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c26ef-9ae3-44f0-93d9-136e69cef08e",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Value based methods:\n",
    "    - Indirect way of learning policy\n",
    "    - Learn a value function, which maps a state to the expected value of being at that state.\n",
    "        - Value of that state is the expected discounted return the agent can get if it starts in that state, and then acts according to our policy.\n",
    "        - Act according to our policy just means that our policy is going to the state with the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd047f-54b7-46a7-afb4-78e6a9bbfa4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca073b-baae-4de8-ad4d-8af9beb9ade0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62ae03-109c-4de9-b768-e68d86f01144",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75a045-cbbe-4805-b6e9-99ad6f04bb49",
   "metadata": {},
   "source": [
    "### The Deep in Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b93c4-0283-4cc8-b2c7-611e595fd002",
   "metadata": {},
   "source": [
    "- Deep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems — hence the name “deep”.\n",
    "- In the next unit, we’ll learn about two value-based algorithms: Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning.\n",
    "    - In Q-learning approach, we use a traditional algorithm to create a Q table that helps us find what action to take for each state.\n",
    "    - In Deep Q-learning approach, we will use a Neural Network (to approximate the Q value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c6d01-d54e-4810-a1c4-513a37459532",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272b656-b255-4366-8f50-fbf8ba14869d",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883567f-ebe1-445d-86ad-b2226a63bab7",
   "metadata": {},
   "source": [
    "- Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.\n",
    "- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.\n",
    "- The RL process is a loop that outputs a sequence of state, action, reward and next state.\n",
    "- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.\n",
    "- To solve an RL problem, you want to find an optimal policy. The policy is the “brain” of your agent, which will tell us what action to take given a state. The optimal policy is the one which gives you the actions that maximize the expected return.\n",
    "\n",
    "- There are two ways to find your optimal policy:\n",
    "    - By training your policy directly: policy-based methods.\n",
    "    - By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n",
    "    \n",
    "- Finally, we speak about Deep RL because we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name “deep”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c8cbe-f4a7-4342-8477-839bc0c98fcd",
   "metadata": {},
   "source": [
    "### Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f72ce1-2ba2-4e63-a1a6-c93ccd0072c7",
   "metadata": {},
   "source": [
    "- Markov Property\n",
    "    - It implies that the action taken by our agent is conditional solely on the present state and independent of the past states and actions.\n",
    "\n",
    "- Observations/State\n",
    "    - State: Complete description of the state of the world.\n",
    "    - Observation: Partial description of the state of the environment/world.\n",
    "- Actions\n",
    "    - Discrete Actions: Finite number of actions, such as left, right, up, and down.\n",
    "    - Continuous Actions: Infinite possibility of actions; for example, in the case of self-driving cars, the driving scenario has an infinite possibility of actions occurring.\n",
    "\n",
    "- Rewards and Discounting\n",
    "    - Rewards: Fundamental factor in RL. Tells the agent whether the action taken is good/bad.\n",
    "    - RL algorithms are focused on maximizing the cumulative reward.\n",
    "\n",
    "- Reward Hypothesis: RL problems can be formulated as a maximisation of (cumulative) return.\n",
    "    - Discounting is performed because rewards obtained at the start are more likely to happen as they are more predictable than long-term rewards.\n",
    "\n",
    "- Tasks\n",
    "    - Episodic: Has a starting point and an ending point.\n",
    "    - Continuous: Has a starting point but no ending point.\n",
    "\n",
    "- Exploration v/s Exploitation Trade-Off\n",
    "    - Exploration: It’s all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.\n",
    "    - Exploitation: It’s about exploiting what we know about the environment to gain maximum rewards.\n",
    "    - Exploration-Exploitation Trade-Off: It balances how much we want to explore the environment and how much we want to exploit what we know about the environment.\n",
    "\n",
    "- Policy\n",
    "    - Policy: It is called the agent’s brain. It tells us what action to take, given the state.\n",
    "    - Optimal Policy: Policy that maximizes the expected return when an agent acts according to it. It is learned through training.\n",
    "    - Policy-based Methods:\n",
    "        - An approach to solving RL problems.\n",
    "        - In this method, the Policy is learned directly.\n",
    "        - Will map each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.\n",
    "    - Value-based Methods:\n",
    "        - Another approach to solving RL problems.\n",
    "        - Here, instead of training a policy, we train a value function that maps each state to the expected value of being in that state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
