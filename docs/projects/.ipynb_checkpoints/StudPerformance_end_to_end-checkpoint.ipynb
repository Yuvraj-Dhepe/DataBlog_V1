{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ef0ddc-8892-4993-87a5-8ec1b435e164",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"End to End Machine Learning Project on Student Performance Dataset\"\n",
    "description: \"An End to End ML project\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "render-on-save: true\n",
    "jupyter: python3\n",
    "output:\n",
    "  quarto::html_document:\n",
    "    self_contained: false\n",
    "    keep_md: false\n",
    "\n",
    "categories:\n",
    "    - End To End Project\n",
    "    - Regression Project\n",
    "    \n",
    "image: ./images/StudentPerformance.jpg\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbe1e8-3ba8-451a-8611-5e21347aa221",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End to End ML Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd52503-c3c4-4d1a-b5c4-71b74a21f38c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A brief intro of the work:\n",
    "Due to extensive work being done in any field of data, it's essential that a data person in a team has to wear multiple hats, while doing the work, to gain experience on what it means to \n",
    "wear those hat's I worked on creating an end to end project on simple data set, but by doing detailed work on each and every step of the process which includes:\n",
    "- Creating [modular folder structure](#t1.1-folder-structure-creation)\n",
    "- Deciding on the [dataset](#action-a1-&-a2)\n",
    "- Setup of the [environment](#task-t2)\n",
    "- Design and [Development of Components and Pipelines](#action-a3), where components interact with data in backend, whereas the pipelines interact with the user and components to derive insights and finally provide result to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb802e-51f2-445e-960f-0a3e40fd8ac6",
   "metadata": {},
   "source": [
    "- Notebook checkpoints\n",
    "    - STAR ANALYSIS\n",
    "    - Explained every point of the star method, step by step in detail.\n",
    "        - Ex. Action will be broken down to A1, A2 to follow up the notebook.\n",
    "        - We use acronyms like T1 representing Task 1, A1.1 representing subaction 1 of Action 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675034d4-a71a-4dcc-9952-0c24f943a7c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STAR ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ec304-5de7-4068-84cb-7e3641863ed0",
   "metadata": {},
   "source": [
    "- Situation: To gain experience in end-to-end machine learning project development and deployment.\n",
    "- Task: Create a machine learning project from scratch, following the entire pipeline from data collection to model deployment.\n",
    "- Action: Develop a project idea, collect and preprocess the data, design and train the machine learning model, evaluate the model's performance, and deploy the model into a production - environment.\n",
    "- Result: Gain hands-on experience in end-to-end machine learning project development and deployment, with a fully functional machine learning system that can be used for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847de1db-3434-4fcf-b59b-130118bb5bc1",
   "metadata": {},
   "source": [
    "### Situation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b8f23-be18-4041-9cd9-7d24ba2e815d",
   "metadata": {},
   "source": [
    "- S1. Need of gaining exposure in real-world ML project development and deployment\n",
    "- S2. A way to improve my Data Science profile, with such projects\n",
    "- S3. Building skillset to be of use in the real-world, and not be limited to books\n",
    "\n",
    "With the situation being clear let's jump to a bit about task that was required to be done for this situation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7393936-a8d1-4832-8106-406867e8e081",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83bf1b-4957-42a4-b3cd-3ded44a5ae27",
   "metadata": {},
   "source": [
    "- T1. Creating a folder structure, for a real-world entity project.\n",
    "    - Uses: Introduction of Modularity to the project, rather than a single jupyter notebook file doing all the job.\n",
    "- T2. Creating an environment and setup file to run this ML pipeline from scratch.\n",
    "- A. Developing an End to End ML pipeline and then performing web deployment for using the ML model.\n",
    "\n",
    "With the basic overview of task now, let's look onto every task in details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb204094-bf31-485e-88fe-37d873b8f7af",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task T1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003fd11-9178-4296-a9f9-7daaee64a5d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Creating a folder structure for our real-world project.\n",
    "This is an essential part for any real-world code project, as it introduces modularity to our code. This modularity helps us to deal with complexity of huge projects in simple way, where a team can work together on different parts of the project, re-use each others work and combine it all at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c4778-0264-40e0-a485-78a62f7ec5c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### T1.1 Folder Structure Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c206440-c81b-49ce-ad87-5ca6c0a4c5c5",
   "metadata": {},
   "source": [
    "- First setup a github repo (ML_Web_Project is my repo), keeping all the options to default.\n",
    "- Locally setup a folder (END_To_END_ML_PROJECT is my base local folder setup on WSL, but the one can use windows or mac as well)\n",
    "    - Open this directory in vscode\n",
    "    - Open a terminal\n",
    "- Secondly let's create a conda environment named venv into this local folder, so to have packages locally to run the project.\n",
    "    ``` bash\n",
    "    conda create -p venv python==3.8 -y\n",
    "    ```\n",
    "    - Activate this environment from the base folder\n",
    "    ``` bash\n",
    "    conda activate venv/ # don't forget '/' cause it tells that this environment is in a folder named venv\n",
    "    ```\n",
    "- Link the local folder to the github repo\n",
    "    - First do git init in the local folder\n",
    "    - Follow all the steps mentioned in the github repo you created to do the syncing of local folder to the repo.\n",
    "    - After the update of git back in 2021, one needs to setup ssh-keys to use the github repo or use tokens, I prefer to use ssh-keys, follow the steps [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\n",
    "    - Create a default .gitignore for python in github repo online.\n",
    "    - Finally do a git pull, to sync the changes locally as well.\n",
    "    - Later on whenever there are enough changes to the local code, follow the steps of git add, commit and push with a useful commit message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505446b-0e87-403f-86b3-8c46225435e5",
   "metadata": {},
   "source": [
    "- By now local repo should have a .gitignore, README.md, venv/ in their local repo, after this create the following folder structure locally.\n",
    "\n",
    "``` bash\n",
    "- END_TO_END_ML_PROJECT\n",
    "    - setup.py # The setup script is the center of all activity in building, distributing, and installing modules that are necessary to run the ML pipeline. # Consider it as the core of the ML Pipeline. This setup.py will help to use our ML pipeline as a package itself and can even be deployed to Pypi.\n",
    "    - requirements.txt # All packages that need to be installed before running the project. # This is the part that gives energy to the core.\n",
    "    - data # The folder which consist of datasets used in the project.\n",
    "        - StudentsPerformance.csv\n",
    "    - notebook # jupyter notebooks, consisting of all codes which helps to find patterns in data and give a big picture code, later to be broken down into src folders.\n",
    "        - EDA_notebook.ipynb \n",
    "        - Model_train.ipynb\n",
    "    - src # The backbone containing all the source codes for creation of ML pipeline package.\n",
    "        - __init__.py\n",
    "        - exception.py # Helps in producing custom exceptions.\n",
    "        - logger.py # Contains the code that will help in creation of logs, and trace errors if caused any during the realtime.\n",
    "        - utils.py # Contains all the utilities that can be reused across the whole project.\n",
    "        - components # The major components of the project, which deal with data cleaning, transformation, model training etc.\n",
    "            - __init__.py\n",
    "            - data_ingestion.py\n",
    "            - data_transformation.py\n",
    "            - model_trainer.py\n",
    "        - pipeline # The complete pipelines built via use of components for further deployment of the model.\n",
    "            - __init__.py\n",
    "            - predict_pipeline.py\n",
    "            - train_pipeline.py\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95375c99-7298-4f3f-b5b3-90de6cdb96c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task T2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b929d-fea3-4c67-a66b-40b27d17bdc1",
   "metadata": {},
   "source": [
    "- Creating an environment and setup file which later can be used to condense our ML pipeline in form of package.\n",
    "In this part we build the foundation for our ML pipeline, by creating the code for setup.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab90ba8-9bb6-473d-b582-b25fb838e0c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Task T2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe9985-4ef3-41da-9b98-e8a39a80605c",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Code for setup file, \n",
    "\n",
    "``` python\n",
    "from setuptools import find_packages,setup\n",
    "from typing import List\n",
    "\n",
    "def get_requirements(file_path:str)->List[str]:\n",
    "    '''\n",
    "    This function will return the list of requirements\n",
    "    '''\n",
    "    requirements = []\n",
    "    file = open(file_path,'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if \"-e .\" not in line:\n",
    "            requirements.append(line.strip('\\n'))\n",
    "    file.close()\n",
    "    \n",
    "    #print(requirements)\n",
    "    return requirements\n",
    "    \n",
    "# With this setup we parse our requirements file to get the requirements installed for our project, one can make this static via use of package names in form of a list, instead of parsing a requirements file.\n",
    "setup(\n",
    "    name='mlproject',\n",
    "    version='0.0.1',\n",
    "    author='<Your Name>',\n",
    "    author_email='<Your Email>',\n",
    "    packages=find_packages(), # This will use the codes or modules that we write for our ML pipeline, to ensure that our every module can be used for building the package, we have a __init__.py in src, or any directory that can be reused.\n",
    "    install_requires=get_requirements('requirements.txt') \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44067c4d-419d-46f2-8e6e-7052e3193473",
   "metadata": {
    "tags": []
   },
   "source": [
    "- contents of requirements.txt file\n",
    "``` bash\n",
    "pandas\n",
    "numpy\n",
    "seaborn\n",
    "matplotlib\n",
    "scikit-learn\n",
    "catboost\n",
    "xgboost\n",
    "dill\n",
    "tensorboard\n",
    "-e . # This triggers the setup .py file automatically, but this is not readed when setup.py is called as per our above code.\n",
    "```\n",
    "\n",
    "- Once these 2 files are setup, simply run:\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- This will install all the necessary packages in our virtual environment and create a new directory <base_folder_name>.egg-info which will help to create the ML pipeline package for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d012e3-2540-4bf2-8abf-a0154ccbd114",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249f633-ab46-48e5-bc50-9e572bf0c567",
   "metadata": {},
   "source": [
    "- A1. Project Idea: Using a student performance data to predict it's grades or scores, depending on the other features of the dataset.\n",
    "- A2. Data Collection and Preprocessing: We first do all EDA in a jupyter notebook to find patterns in the data and getting to know the type of preprocessing required to be done on the dataset.\n",
    "    - For simple application the data is simply imported in form of csv file, but all this can even be done by getting data from Data Warehouse as well.\n",
    "- A3. Design and Development of ML pipeline components: After EDA, we try to create simple modular codes in a jupyter notebook, which do the job of development, training and evaluation of ML model. Later these modular codes are more or less split into the folder structure that we created earlier.\n",
    "- A4. Deployment of model into a production environment: We use cloud tools like AWS or Streamlit or Flask n Django or any other web service to deploy the ML model online to be used on realtime data provided by user or fetched from a source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54fc6d-e3c6-420f-8641-4fbc787ee7c6",
   "metadata": {},
   "source": [
    "#### Action A1 & A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f05969-e3e0-4f81-954f-2aa74df18afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Project Idea \n",
    "    - We will use a simple student performance dataset, to predict the child's maths scores via the rest of the features of the dataset.\n",
    "    - I am using this dataset, because it's having a mixed of categorical and numerical features, we can have a good amount of EDA done on this simple data, and last but not the least train many regression algorithms on this simple data easily.\n",
    "- Data Collection & Preprocessing\n",
    "    - We will use jupyter notebooks, to majority of the EDA, and finding the patterns. \n",
    "        - [Link to EDA Ipynb File](https://drive.google.com/file/d/1fTR9FDHklJu9iIcg04tCZ7ofImfxu6SO/view?usp=share_link)\n",
    "    - Once the EDA is done, we will also have basic models run on the data, in another jupyter notebook, so that we have basic model pipeline code in place as well.\n",
    "        - [Link to Models Ipynb File](https://drive.google.com/file/d/1HxHrjj3Q9zV3r0N9NpqyzlW73cdxEaEO/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea7cfc-144c-441f-b95d-0a1ca683dc24",
   "metadata": {},
   "source": [
    "#### Action A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2223226-7eb9-45bf-b1f4-f2866288d512",
   "metadata": {},
   "source": [
    "- Design and Development of ML pipeline components in form of modular codes\n",
    "- Steps\n",
    "    - Creation of utility codes, logging and exception handling module that will be used all over the components, pipelines.\n",
    "    - Creation of Components modules inside the package consisting of Data Ingestion, Data Transformation and Model Trainer Component.    \n",
    "    - Creation of train and predict pipelines modules that will be connected to the above components, and will be a pipeline connecting the frontend user and the backend model of Machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ae3ae-0431-4b16-8fba-a4c1a0bfa668",
   "metadata": {},
   "source": [
    "#### Action A3.1: Creation of Utilities, Loggers n Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c676996-b8b0-4136-88ff-ac96188a85f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Action A3.1.1: Creation of Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57208b9-2480-4fbd-b7a1-911818f47eea",
   "metadata": {
    "tags": []
   },
   "source": [
    "``` python\n",
    "#Common functionalities for the whole project: Utilities.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from src.exception import CustomException\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def save_object(file_path,obj):\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path,exist_ok=True)\n",
    "        \n",
    "        file_obj = open(file_path,\"wb\")\n",
    "        dill.dump(obj,file_obj)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "\n",
    "def evaluate_models(X_train, y_train, X_test,y_test,models, param):\n",
    "    try:\n",
    "        report = {}\n",
    "        \n",
    "        for i in range(len(list(models))):\n",
    "            model = list(models.values())[i]\n",
    "            para=param[list(models.keys())[i]]\n",
    "\n",
    "            gs = GridSearchCV(model,para,cv=3)\n",
    "            gs.fit(X_train,y_train)\n",
    "\n",
    "            model.set_params(**gs.best_params_)\n",
    "            model.fit(X_train,y_train)\n",
    "            \n",
    "            #model.fit(X_train,y_train)\n",
    "            \n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            train_model_score = r2_score(y_train,y_train_pred)\n",
    "            test_model_score = r2_score(y_test,y_test_pred)\n",
    "            \n",
    "            report[list(models.keys())[i]] = test_model_score\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "            \n",
    "def load_object(file_path):\n",
    "    try:\n",
    "        file_obj = open(file_path,\"rb\")\n",
    "        return dill.load(file_obj)\n",
    "        file_obj.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea9821-95f5-4a13-b768-b4832d68d381",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Action A3.1.2: Creation of Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b13aff-46ce-4d99-9925-238880ce5cfa",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Logger is for the purpose of logging all the events in the program from execution to termination.\n",
    "# For example, whenever there is an exception, we can log the exception info in a file via use of logger.\n",
    "\n",
    "# Read logger documentation at https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_FILE_NAME = f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\n",
    "logs_path = os.path.join(os.getcwd(), \"logs\",LOG_FILE_NAME) # This will create logs folder in the same working directory where this file is present\n",
    "os.makedirs(logs_path,exist_ok=True) # Keep appending the logs in the same directory even if there are multiple runs of the program\n",
    "\n",
    "LOG_FILE_PATH = os.path.join(logs_path,LOG_FILE_NAME)\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILE_PATH,\n",
    "                    level=logging.INFO,\n",
    "                    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s: %(message)s\",\n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %p'\n",
    "                    ) #This is the change of basic configuration for the logger\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info(\"This is a test log\")\n",
    "    logging.warning(\"This is a warning log\")\n",
    "    logging.error(\"This is an error log\")\n",
    "    logging.critical(\"This is a critical log\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eba051-ed90-4cb3-bf33-7de171200a51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Action A3.1.3: Creation of Exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8387e4b-535f-4666-9ad6-2e922a6cef25",
   "metadata": {
    "tags": []
   },
   "source": [
    "``` python\n",
    "# We use this custom exception handling in the project to handle all the errors that will come into the project, simply we can say that we are handling all the errors that will come into the project in a single place.\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# Sys module in python provides various functions and variables that are used to manipulate different parts of the python runtime environment. It allows operating on the python interpreter as it provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n",
    "# Read more about sys module here: https://docs.python.org/3/library/sys.html\n",
    "from src.logger import logging\n",
    "\n",
    "\n",
    "def error_message_detail(error,error_detail:sys):\n",
    "    _,_,exec_tb = error_detail.exc_info()\n",
    "    file_name = exec_tb.tb_frame.f_code.co_filename\n",
    "    error_message = f\"Error occured in python script name {file_name} on line number {exec_tb.tb_lineno} and error is {str(error)}\"\n",
    "    \n",
    "    return error_message\n",
    "    \n",
    "class CustomException(Exception):\n",
    "    def __init__(self,error_message,error_detail:sys):\n",
    "        super().__init__(error_message)\n",
    "        self.error_message = error_message_detail(error_message,error_detail= error_detail)\n",
    "        #self.error_detail = error_detail        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.error_message}\"\n",
    "\n",
    "# Read more about custom exception handling here: https://www.programiz.com/python-programming/user-defined-exception\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        a = 10\n",
    "        b = 0\n",
    "        c = a/b\n",
    "        print(c)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        raise CustomException(e,error_detail=sys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204991de-53d9-40bc-90c0-2d30fedb3f89",
   "metadata": {},
   "source": [
    "##### Action A3.2: Creation of Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bde89f-6ffa-46a5-b43e-edee955df93e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### A3.2.1 Data Ingestion Component\n",
    "- Data being a central component of any project, in this component, we write classes such as DataIngestionConfig and DataIngestion.\n",
    "    - DataIngestionConfig consists of public path variables to train, test and raw data. \n",
    "    - DataIngestion helps to create an object which invokes an object of DataIngestionConfig during initialization and retrieves public path variables.\n",
    "    - By use of those paths, we read data, split them up and save them to the directory by use of initiate_data_ingestion method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44c5e9-e6be-4894-9b62-f55554396a2c",
   "metadata": {},
   "source": [
    "- Data ingestion is a crucial step in any project that involves handling data. This process involves extracting data from different sources, such as databases or warehouses, and loading it into a centralized location, such as a data warehouse, data lake, or data mart. Typically, this task is performed by a specialized big data team, whose responsibility is to ensure that data is obtained from various sources and stored in different formats, such as Hadoop or MongoDB.\n",
    "- As Data Scientists, it's essential to have knowledge of how to extract data from different sources, such as Hadoop, MongoDB, MySQL, or Oracle, and make it available for analysis. Since data is a critical asset in any project, understanding the process of data ingestion is vital to ensure that the data is organized and stored in a way that facilitates analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1680f0-22e7-4e1a-a77c-f8633c880547",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.components.data_transformation import (DataTransformation,\n",
    "                                                DataTransformationConfig)\n",
    "from src.components.model_trainer import ModelTrainer, ModelTrainerConfig\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    '''\n",
    "    Used for defining the configuration for data ingestion.\n",
    "    '''\n",
    "    train_data_path: str = os.path.join('artifacts', 'train.csv')\n",
    "    test_data_path: str = os.path.join('artifacts', 'test.csv') \n",
    "    raw_data_path: str = os.path.join('artifacts', 'data.csv')\n",
    "\n",
    "class DataIngestion:\n",
    "    '''\n",
    "    Used for ingesting data by making use of the configuration defined in DataIngestionConfig.\n",
    "    '''\n",
    "    def __init__(self,ingestion_config: DataIngestionConfig = DataIngestionConfig()):\n",
    "        self.ingestion_config = ingestion_config\n",
    "    \n",
    "    def initiate_data_ingestion(self,raw_data_path: str = None):\n",
    "        try:\n",
    "            # Reading data here.\n",
    "            logging.info(\"Initiating data ingestion\")\n",
    "            if raw_data_path is not None:\n",
    "                self.ingestion_config.raw_data_path = raw_data_path\n",
    "                data = pd.read_csv(self.ingestion_config.raw_data_path)\n",
    "            else:\n",
    "                data = pd.read_csv('data/NewSPerformance.csv')\n",
    "                        \n",
    "            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)\n",
    "            data.to_csv(self.ingestion_config.train_data_path,index=False,header=True)\n",
    "            logging.info(\"Data ingestion completed\")\n",
    "            \n",
    "            logging.info(\"Train test split initiated\")\n",
    "            train_set, test_set = train_test_split(data,test_size = 0.2, random_state = 18)\n",
    "\n",
    "            train_set.to_csv(self.ingestion_config.train_data_path,index = False, header = True)\n",
    "            test_set.to_csv(self.ingestion_config.test_data_path,index = False, header = True)\n",
    "            logging.info(\"Train test split ingestion completed\")\n",
    "            \n",
    "            return (\n",
    "                self.ingestion_config.train_data_path,\n",
    "                self.ingestion_config.test_data_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error occured in data ingestion\")\n",
    "            raise CustomException(e,sys)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj = DataIngestion()\n",
    "    train_data, test_data = obj.initiate_data_ingestion()\n",
    "    \n",
    "    data_transformation = DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n",
    "    train_arr, test_arr,_ = data_transformation.initiate_data_transformation(train_data,test_data)\n",
    "    \n",
    "    modeltrainer = ModelTrainer()\n",
    "    print(modeltrainer.initiate_model_trainer(train_arr, test_arr))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78500e7-aa6d-4f9b-9ce8-6974ed5c3209",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### A3.2.1 Data Transformation Component\n",
    "- Once data ingestion is done, Data transformation component is used to transform the data, to make it useful for analysis and train models on it.\n",
    "    - DataTransformationConfig class in this component stores public path variable to store the preprocessing object in pickle type data, to be later used during building the web app.\n",
    "    - DataTransformation class helps to create an object which invokes DataTransformationConfig Object to get access to preprocessing object path.\n",
    "    - We have a get_data_transformer_object method, that returns a preprocessor object which can preprocess numerical and categorical columns\n",
    "    - By use of the get_data_transformer_object method, in initiate_data_transformation method, to do all the preprocessing on the train and test files, whose path is available from Data Ingestion component. After all the preprocessing we return train and test array consisting of feature and target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf232b-0301-4d46-9f69-ab390f7b41ba",
   "metadata": {},
   "source": [
    "- A data transformation component is a crucial part of the data science process, which involves transforming raw data into a format that can be used for analysis. Data Scientists play a vital role in this process as they use various techniques such as feature engineering, feature selection, feature scaling, data cleaning, and handling null values to ensure the quality of data used for analysis. By understanding the process of data transformation, Data Scientists can generate valuable insights from raw data and make informed business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc9153-26c1-42ba-bda3-33dce222be28",
   "metadata": {},
   "source": [
    "``` python\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "from src.utils import save_object\n",
    "\n",
    "# Defining the paths for the data ingestion\n",
    "# di_obj = DataIngestion.DataIngestionConfig() # Not required as we already are doing the Doing the addition of paths in data_ingestion.py itself.\n",
    "# di_obj.train_data_path = \"data/train_data.csv\"\n",
    "# di_obj.test_data_path = \"data/test_data.csv\"\n",
    "\n",
    "@dataclass #This is a decorator which is used to create a dataclass variables.\n",
    "class DataTransformationConfig:\n",
    "    '''\n",
    "    We are creating a dataclass variable which will be used to store the paths for the data transformation transformer object.\n",
    "    '''\n",
    "    preprocessor_obj_file_path = os.path.join(\"artifacts\",\"preprocessor.pkl\")\n",
    "\n",
    "class DataTransformation:\n",
    "    \n",
    "    def __init__(self,transformation_config: DataTransformationConfig = DataTransformationConfig()):\n",
    "        self.data_transformation_config = transformation_config\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        '''\n",
    "        This function is responsible for creating a preprocessing data transformation object.\n",
    "        '''\n",
    "        try:\n",
    "            numerical_columns = [\"writing_score\",\"reading_score\"]\n",
    "            categorical_columns = [\n",
    "                \"gender\",\n",
    "                \"race_ethnicity\",\n",
    "                \"parental_level_of_education\",\n",
    "                \"lunch\",\n",
    "                \"test_preparation_course\"\n",
    "                ]\n",
    "            \n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\",SimpleImputer(strategy = 'median')),\n",
    "                    (\"scaler\",StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            cat_pipeline = Pipeline(\n",
    "                steps = [\n",
    "                    (\"imputer\",SimpleImputer(strategy = 'most_frequent')),\n",
    "                    (\"one_hot_encoder\",OneHotEncoder()),\n",
    "                    ('scaler',StandardScaler(with_mean=False))\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Numerical columns:{numerical_columns}\")\n",
    "            logging.info(f\"Categorical columns:{categorical_columns}\")\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    (\"num_pipeline\",num_pipeline,numerical_columns),\n",
    "                    ('cat_pipeline',cat_pipeline,categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "        \n",
    "    def initiate_data_transformation(self,train_path,test_path):\n",
    "        '''\n",
    "        Here we use the preprocessing object to transform the data.\n",
    "        '''\n",
    "            \n",
    "        try:\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df= pd.read_csv(test_path)\n",
    "            \n",
    "            \n",
    "            logging.info(\"Read train and test data completed\") \n",
    "            \n",
    "            logging.info(\"Obtaining preprocessing object and starting processing.\")\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "            target_column_name = \"math_score\"\n",
    "            numerical_columns = [\"writing_score\",\"reading_score\"]\n",
    "            \n",
    "            input_feature_train_df = train_df.drop(columns = [target_column_name],axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "            \n",
    "            input_feature_test_df = test_df.drop(columns = [target_column_name],axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.fit_transform(input_feature_test_df)\n",
    "            \n",
    "            train_arr = np.c_[\n",
    "                input_feature_train_arr,np.array(target_feature_train_df)\n",
    "                ]\n",
    "            \n",
    "            test_arr = np.c_[\n",
    "                input_feature_test_arr,np.array(target_feature_test_df)\n",
    "                ]\n",
    "            \n",
    "            logging.info(f\"Saved Preprocessing object at a particular filepath \")\n",
    "            save_object(\n",
    "                file_path = self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj = preprocessing_obj\n",
    "            )\n",
    "            return(\n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                self.data_transformation_config.preprocessor_obj_file_path,\n",
    "            )\n",
    "            \n",
    "                \n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b531214-df10-4e78-8bc1-39a51fccfb42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### A3.1.1 Model Trainer Component\n",
    "- *Model Trainer(MT)*: We can run various models, once above components have turned data to desired format. This component consists of 2 classes as follows.\n",
    "    - ModelTrainerConfig class stores public path variable to store the model object once trained in the pickle format.\n",
    "    - ModelTrainer class, uses initiate_model_trainer method, that access train and test array from Data Transformation component. This method is able to train various models together on the train array and then finally make predictions on the test array, by using the best model from the various models being used on the base of r2_scores. Also in this method we use the ModelTrainerConfig object to store this trained model in local directory and last but not least we also return the r2_score for the best model on test data in this method itself.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70cfc8-d20d-4e14-9cb1-a84f7e4782db",
   "metadata": {},
   "source": [
    "- The model trainer component is responsible for training machine learning models on the transformed data. Data Scientists use this component to select an appropriate algorithm, tune hyperparameters, and train the model on the data. The trained model is then evaluated for its performance, and if it meets the desired level of accuracy, it is deployed for production use. The role of Data Scientists in this component is to select and fine-tune the machine learning models that best fit the problem at hand, and ensure that the models meet the business requirements. Ultimately, the model trainer component helps Data Scientists to generate insights and make predictions that can drive business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238047d7-110c-42f1-9342-be0c212b4d9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, GradientBoostingRegressor,\n",
    "                              RandomForestRegressor)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "from src.utils import evaluate_models, save_object\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    # This class is used to store the configs, or any other files generated in this particular python file.\n",
    "    trained_model_file_path = os.path.join(\"artifacts\",\"model.pkl\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,model_train_config:ModelTrainerConfig = ModelTrainerConfig() ) -> None:\n",
    "        self.model_trainer_config = model_train_config\n",
    "        \n",
    "    \n",
    "    def initiate_model_trainer(self,train_array,test_array):\n",
    "        try:\n",
    "            logging.info(\"Split training and test input data\")\n",
    "            X_train, y_train, X_test, y_test = (\n",
    "                train_array[:,:-1],\n",
    "                train_array[:,-1],\n",
    "                test_array[:,:-1],\n",
    "                test_array[:,-1]\n",
    "            )\n",
    "            \n",
    "            models = {\n",
    "                \"Random Forest\": RandomForestRegressor(),\n",
    "                \"Decision Tree\": DecisionTreeRegressor(),\n",
    "                \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "                \"Linear Regression\": LinearRegression(),\n",
    "                \"XGBRegressor\": XGBRegressor(),\n",
    "                \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "                \"AdaBoost Regressor\": AdaBoostRegressor(),\n",
    "            }\n",
    "            params ={\n",
    "                \"Decision Tree\": {\n",
    "                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "                    # 'splitter':['best','random'],\n",
    "                    # 'max_features':['sqrt','log2'],\n",
    "                },\n",
    "                \"Random Forest\":{\n",
    "                    # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "                    # 'max_features':['sqrt','log2',None],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "                },\n",
    "                \"Gradient Boosting\":{\n",
    "                    # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n",
    "                    # 'criterion':['squared_error', 'friedman_mse'],\n",
    "                    # 'max_features':['auto','sqrt','log2'],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "                },\n",
    "                \"Linear Regression\":{},\n",
    "                \"XGBRegressor\":{\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "                },\n",
    "                \"CatBoosting Regressor\":{\n",
    "                    'depth': [6,8,10],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'iterations': [30, 50, 100]\n",
    "                },\n",
    "                \"AdaBoost Regressor\":{\n",
    "                    'learning_rate':[.1,.01,0.5,.001],\n",
    "                    # 'loss':['linear','square','exponential'],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "                }\n",
    "                \n",
    "            }\n",
    "            \n",
    "            model_report: dict = evaluate_models(\n",
    "                X_train = X_train,\n",
    "                y_train =  y_train, \n",
    "                X_test  = X_test,\n",
    "                y_test = y_test,\n",
    "                models = models,\n",
    "                param = params\n",
    "                )\n",
    "            \n",
    "            \n",
    "            \n",
    "            # To get best model score from dict\n",
    "            best_model_score = max(sorted(model_report.values()))\n",
    "            \n",
    "            # To get best model name from dict\n",
    "            best_model_name = list(model_report.keys())[\n",
    "                list(model_report.values()).index(best_model_score)\n",
    "                ]\n",
    "            best_model = models[best_model_name]\n",
    "            \n",
    "            if best_model_score<0.6:\n",
    "                raise CustomException(\"No best model found\")\n",
    "            logging.info(f\"Best found model on both training and testing dataset\")\n",
    "            \n",
    "            save_object(\n",
    "                file_path = self.model_trainer_config.trained_model_file_path,\n",
    "                obj = best_model\n",
    "            )\n",
    "            \n",
    "            predicted = best_model.predict(X_test)\n",
    "            r2_square = r2_score(y_test,predicted)\n",
    "            \n",
    "            return r2_square\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb2d8f-102c-4bcb-9d3c-6d0cbee4cd52",
   "metadata": {},
   "source": [
    "##### Action A3.3: Creation of train and predict pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e398e-c8d2-4ddb-a9e4-199051dc155d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### A3.3.1 Train Pipeline\n",
    "- A pipeline that interacts with the DI, DT, MT components to process the raw data available in the frontend.\n",
    "    - This pipeline has a TrainPipeline class, which takes in the raw data and uses the train method which interacts with the DI, DT and MT components to simply return the best models r2 score in the end.\n",
    "    \n",
    "###### Currently this pipeline is not in production env, simply used locally to train the model on local data\n",
    "- To run this pipeline, and train models, simply run the file with appropriate raw data \n",
    "```python\n",
    "python3 ./src/pipeline/train_pipeline.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a669a-d192-4de4-b86c-3f8364c63b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "``` python\n",
    "# Train Pipeline File\n",
    "import sys\n",
    "import pandas as pd\n",
    "from src.logger import logging\n",
    "from src.exception import CustomException\n",
    "from src.components import data_ingestion as di\n",
    "from src.components import data_transformation as dt\n",
    "from src.components import model_trainer as mt\n",
    "\n",
    "\n",
    "class TrainPipeline:\n",
    "    def __init__(self, raw_data_path=None):\n",
    "        self.raw_data_path = raw_data_path\n",
    "\n",
    "    def train(self):\n",
    "        try:\n",
    "            logging.info(\"Initiating data ingestion\")\n",
    "            di_obj = di.DataIngestion()\n",
    "            train_data, test_data = di_obj.initiate_data_ingestion(raw_data_path=self.raw_data_path)\n",
    "            logging.info(\"Data ingestion completed\")\n",
    "            \n",
    "            logging.info(\"Initiating data transformation\")\n",
    "            dt_obj = dt.DataTransformation() # We call DataTransformation here, just for the sake of demonstration.\n",
    "            train_arr, test_arr,_ = dt_obj.initiate_data_transformation(train_data,test_data)\n",
    "            logging.info(\"Data transformation completed and saved preprocessor object\")\n",
    "            \n",
    "            \n",
    "            logging.info(\"Training the model\")\n",
    "            mt_obj = mt.ModelTrainer()\n",
    "            print(f\"Best Models r2_score: {mt_obj.initiate_model_trainer(train_arr, test_arr)}\")\n",
    "            logging.info(\"Model training completed and saved the best model\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pipeline_obj = TrainPipeline(\"data/NewSPerformance.csv\")\n",
    "    train_pipeline_obj.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb8261-35f5-40ea-898e-2e2a8730b5ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### A3.3.2 Predict Pipeline Component\n",
    "- *Predict Pipeline*: A pipeline that takes the user inputs and makes prediction on the given data by using the trained model and other objects like preprocessor obj, created via the train pipeline.\n",
    "    - This pipeline consists of CustomData class which takes the user inputs submitted to our application and returns a data frame out of the inputs.\n",
    "    - PredictPipeline class, takes the CustomData class returned df object as features, scales them via the DT component generated transformer and finally, makes predictions by using the best model, from the MT component and showcases them back to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1c888-f331-452c-abe6-63726bbb5114",
   "metadata": {
    "tags": []
   },
   "source": [
    "``` python\n",
    "# Prediction pipeline file.\n",
    "import sys\n",
    "import pandas as pd\n",
    "from src.exception import CustomException\n",
    "from src.utils import load_object\n",
    "from src.logger import logging\n",
    "\n",
    "\n",
    "class PredictPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,features):\n",
    "        try:\n",
    "            logging.info(\"Predicting the data\")\n",
    "            model_path  = \"artifacts/model.pkl\"\n",
    "            preprocessor_path = \"artifacts/preprocessor.pkl\"\n",
    "            \n",
    "            model = load_object(file_path = model_path)\n",
    "            preprocessor = load_object(file_path = preprocessor_path)\n",
    "        \n",
    "            data_scaled = preprocessor.transform(features)\n",
    "            predictions = model.predict(data_scaled)\n",
    "            logging.info(\"Predictions completed\")\n",
    "            return pd.DataFrame(predictions,columns=[\"predictions\"])\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)    \n",
    "\n",
    "\n",
    "class CustomData:\n",
    "    def __init__(  self,\n",
    "        gender: str,\n",
    "        race_ethnicity: str,\n",
    "        parental_level_of_education,\n",
    "        lunch: str,\n",
    "        test_preparation_course: str,\n",
    "        reading_score: int,\n",
    "        writing_score: int):\n",
    "\n",
    "        self.gender = gender\n",
    "\n",
    "        self.race_ethnicity = race_ethnicity\n",
    "\n",
    "        self.parental_level_of_education = parental_level_of_education\n",
    "\n",
    "        self.lunch = lunch\n",
    "\n",
    "        self.test_preparation_course = test_preparation_course\n",
    "\n",
    "        self.reading_score = reading_score\n",
    "\n",
    "        self.writing_score = writing_score\n",
    "\n",
    "    def get_data_as_data_frame(self):\n",
    "        try:\n",
    "            logging.info(\"Creating a data frame from the custom data\")\n",
    "            custom_data_input_dict = {\n",
    "                \"gender\": [self.gender],\n",
    "                \"race_ethnicity\": [self.race_ethnicity],\n",
    "                \"parental_level_of_education\": [self.parental_level_of_education],\n",
    "                \"lunch\": [self.lunch],\n",
    "                \"test_preparation_course\": [self.test_preparation_course],\n",
    "                \"reading_score\": [self.reading_score],\n",
    "                \"writing_score\": [self.writing_score],\n",
    "            }\n",
    "            logging.info(\"Data frame created\")\n",
    "            return pd.DataFrame(custom_data_input_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8c36b-57d1-448f-8877-f57a70c5cc5a",
   "metadata": {},
   "source": [
    "- The train and predict pipelines are a critical component of the machine learning process. The train pipeline is responsible for training the machine learning model on the training data. This process involves selecting an appropriate algorithm, fine-tuning hyperparameters, and fitting the model to the training data.\n",
    "\n",
    "- Once the model is trained, it is deployed to the predict pipeline, which is responsible for making predictions on new data. The predict pipeline involves processing the data, applying any necessary transformations, and using the trained model to generate predictions.\n",
    "\n",
    "- Data Scientists play a crucial role in both the train and predict pipelines. They must ensure that the training data is representative of the problem at hand, and that the model is trained and optimized to meet the desired level of accuracy. In addition, they must ensure that the predict pipeline is efficient and reliable, and that the model generates accurate predictions in real-time.\n",
    "\n",
    "- Ultimately, the train and predict pipelines are essential to the machine learning process, as they allow Data Scientists to build and deploy models that can generate valuable insights and drive business decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
