<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Unit 2 Learnings from Hugging Face RL Course">

<title>Data Science Blog - Yuvraj Dhepe - RL Unit 2: Introduction to Q Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../images/lotus.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Data Science Blog - Yuvraj Dhepe - RL Unit 2: Introduction to Q Learning">
<meta property="og:description" content="Unit 2 Learnings from Hugging Face RL Course">
<meta property="og:image" content="https://github.com/Yuvraj-Dhepe/DataBlog_V1/docs/projects/abc/images/RL2_QLearning.jpg">
<meta property="og:site-name" content="Data Science Blog - Yuvraj Dhepe">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Hello There</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/notes/index.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Yuvraj-Dhepe" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-2-introduction-to-q-learning" id="toc-chapter-2-introduction-to-q-learning" class="nav-link active" data-scroll-target="#chapter-2-introduction-to-q-learning">Chapter 2: INTRODUCTION TO Q-Learning</a>
  <ul class="collapse">
  <li><a href="#rl-recap" id="toc-rl-recap" class="nav-link" data-scroll-target="#rl-recap">RL Recap</a></li>
  <li><a href="#value-based-methods" id="toc-value-based-methods" class="nav-link" data-scroll-target="#value-based-methods">Value-based methods</a></li>
  <li><a href="#bellman-equation-to-simplify-the-value-estimation" id="toc-bellman-equation-to-simplify-the-value-estimation" class="nav-link" data-scroll-target="#bellman-equation-to-simplify-the-value-estimation">Bellman Equation to simplify the value estimation</a></li>
  <li><a href="#monte-carlo-vs-temporal-difference-learning" id="toc-monte-carlo-vs-temporal-difference-learning" class="nav-link" data-scroll-target="#monte-carlo-vs-temporal-difference-learning">Monte Carlo vs Temporal Difference Learning</a></li>
  <li><a href="#introducing-q-learning" id="toc-introducing-q-learning" class="nav-link" data-scroll-target="#introducing-q-learning">Introducing Q-Learning</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  <li><a href="#glossary" id="toc-glossary" class="nav-link" data-scroll-target="#glossary">Glossary</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/quarto-dev/quarto-demo/blob/main/docs/projects/abc/RL2_IntroductionToQ-Learning.ipynb" class="toc-action">Edit this page</a></p><p><a href="https://github.com/quarto-dev/quarto-demo/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">RL Unit 2: Introduction to Q Learning</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Re-inforcement Learning</div>
    <div class="quarto-category">Regression Project</div>
  </div>
  </div>

<div>
  <div class="description">
    Unit 2 Learnings from Hugging Face RL Course
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="chapter-2-introduction-to-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2-introduction-to-q-learning">Chapter 2: INTRODUCTION TO Q-Learning</h2>
<ul>
<li>Back in previous class we learned about Reinforcement Learning, the RL process and the different methods to solve an RL problem.</li>
<li>For this Unit we will be learning about:
<ul>
<li>Value-based Methods</li>
<li>Difference between Monte Carlo and Temporal Difference Learning</li>
<li>Study and implement our first RL algorithm: Q-learning.</li>
</ul></li>
</ul>
<section id="rl-recap" class="level3">
<h3 class="anchored" data-anchor-id="rl-recap">RL Recap</h3>
<ul>
<li>The goal of RL to build an agent that can make smart decisions.</li>
<li>Smart decisions will occur, when the agent will learn from the env, by interacting with it through trial and error and receiving rewards as unique feedback.</li>
<li>It’s goal is to maximize it’s expected cumulative reward.</li>
<li>Thus we need to train the agent’s brain i.e.&nbsp;the policy for this.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg" width="600" height="600"></p>
<ul>
<li>Our goal now from maximizing the expected cumulative reward, now changes to learning a policy which maximizes the expected cumulative reward for us</li>
<li>We do this by 2 methods:
<ul>
<li>Policy based methods: Train the policy directly to learn which action to take given a state</li>
<li>Value based methods: Train a value function to learn which state is more valuable and use this value function to take the action that leads to it</li>
</ul></li>
<li>We will be focusing on value based methods for this unit</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg" height="600" width="600"></p>
</section>
<section id="value-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="value-based-methods">Value-based methods</h3>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg" width="600" height="600"></p>
<p>RL agent’s goal is to have an optimal policy <span class="math inline">\(\pi^*\)</span> - To find this policy we have 2 methods: - Policy Based Methods: Here we don’t need any value function. - We don’t define by hand the behavior of our policy; it’s the training that will define it.</p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg" width="600" height="600"></p>
<ul>
<li>Value Based Methods: Indirectly, by training a value function that outputs the value of a state or a state-action pair.
<ul>
<li>Given this value function, our policy will take an action.</li>
<li>Since the policy is not trained/learned, we need to specify it’s behaviour by hand.</li>
<li>For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we’ll have a Greedy Policy</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg" width="600" height="600"></p>
<ul>
<li><p>Consequently, whatever method you use to solve your problem, you will have a policy. In the case of value-based methods, you don’t train the policy: your policy is just a simple pre-specified function (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.</p></li>
<li><p>So the difference is:</p>
<ul>
<li>In policy-based training, the optimal policy (denoted π*) is found by training the policy directly.</li>
<li>In value-based training, finding an optimal value function (denoted Q* or V*, we’ll study the difference below) leads to having an optimal policy.</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" height="600" width="600"></p>
<ul>
<li><p>In Value based methods we have 2 types of value based functions:</p></li>
<li><p>State value function under a policy <span class="math inline">\(\pi\)</span></p>
<ul>
<li>For each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer)</li>
<li>In Value based methods we have 2 types of value based functions:- If we take the state with value -7: it’s the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg" width="400" height="400"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg" height="400" width="400"></p>
<ul>
<li><p>Action value function In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.</p></li>
<li><p>The value of taking action <span class="math inline">\(a\)</span> in the state <span class="math inline">\(s\)</span> under a policy <span class="math inline">\(\pi\)</span> is :</p></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg" height="400" width="400"><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg" height="400" width="400"></p>
<ul>
<li>We see that the difference is:
<ul>
<li>For the state-value function, we calculate the value of a state <span class="math inline">\(S_t\)</span></li>
<li>For the action-value functions, we calculate the value of the state-action pair <span class="math inline">\((S_t, A_t)\)</span> hence the value of taking that action at that state</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg" width="400" height="400"></p>
<ul>
<li>In either case, whichever value function we choose (state-value or action-value function), the returned value is the expected return.</li>
<li>However, the problem is that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.</li>
<li>This can be a computationally expensive process, and that’s where the Bellman equation comes in to help us.</li>
</ul>
</section>
<section id="bellman-equation-to-simplify-the-value-estimation" class="level3">
<h3 class="anchored" data-anchor-id="bellman-equation-to-simplify-the-value-estimation">Bellman Equation to simplify the value estimation</h3>
<ul>
<li><p>With what we have learned so far, we know that if we calculate <span class="math inline">\(V(S_t)\)</span>, we need to calculate the return starting at that state and then follow the policy forever after. (The policy we defined in the following example is a Greedy Policy; for simplification, we don’t discount the reward).</p></li>
<li><p>So to calculate <span class="math inline">\(V(S_t)\)</span>, we need to calculate the <strong>sum</strong> of the expected rewards. Hence: <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg" width="400" height="400"></p></li>
<li><p>Then to calculate <span class="math inline">\(V(S_{t+1})\)</span>, we need to calculate the return starting at that state <span class="math inline">\(S_{t+1}\)</span> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg" width="400" height="400"></p></li>
<li><p>So basically we’re repeating the computation for the value of different states, which can be tedious if needs to be done for each state value or state-action value.</p></li>
<li><p>So to simplify this we use Bellman equation which is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:</p>
<ul>
<li>The immedicate reward <span class="math inline">\(R_{t+1}+\)</span> the discounted value of the state that follows (gamma * <span class="math inline">\(V(S_{t+1})\)</span>)</li>
<li>If we go back to our example, we can say that the value of State 1 is equal to the expected cumulative return if we start at that state.</li>
<li>To calculate the value of State 1: the sum of rewards if the agent started in that state 1 and then followed the policy for all the time steps.</li>
<li>This is equivalent to $V(S_t) = $ Immediate reward <span class="math inline">\(R_{t+1}\)</span> + Discounted value of the next state (<span class="math inline">\(\gamma * V(S_{t+1}))\)</span></li>
</ul></li>
<li><p>In the interest of simplicity, here we don’t discount, so gamma= 1. But you’ll study an example with gamma = 0.99 in the Q-Learning section of this unit.</p>
<ul>
<li>The value of $V(S_{t+1}) = $ Immediate reward <span class="math inline">\(R_{t+2}\)</span> + Discounted value of the next state (<span class="math inline">\(\gamma * V(S_{t+2}))\)</span></li>
</ul></li>
<li><p>To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows.</p></li>
<li><p>Before going to the next section, think about the role of gamma in the Bellman equation. What happens if the value of gamma is very low (e.g.&nbsp;0.1 or even 0)? What happens if the value is 1? What happens if the value is very high, such as a million?</p></li>
</ul>
</section>
<section id="monte-carlo-vs-temporal-difference-learning" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-vs-temporal-difference-learning">Monte Carlo vs Temporal Difference Learning</h3>
<ul>
<li>Since we know that the RL agent learns by interacting with the environment.</li>
<li>The idea is that given the experience and the received reward, the agent will update it’s value function of policy.</li>
<li>There are 2 different strategies on how to train our value function or policy function.
<ul>
<li>Both of them use experience to solve the RL problem, i.e.&nbsp;the SARSA</li>
<li>The 2 strategies are Monte Carlo and Temporal Differnce
<ul>
<li>Monte Carlo uses an entire episode of experience before learning.</li>
<li>Temporal difference uses only a step. <span class="math inline">\((S_t, A_t, R_{t+1}, S{t+1})\)</span></li>
</ul></li>
</ul></li>
</ul>
<section id="monte-carlo-learning-at-the-end-of-the-episode" class="level4">
<h4 class="anchored" data-anchor-id="monte-carlo-learning-at-the-end-of-the-episode">Monte Carlo: Learning at the end of the episode</h4>
<ul>
<li>Monte Carlo waits until the end of the episode, calculates <span class="math inline">\(G_t\)</span> (return) and uses it as a target for updating <span class="math inline">\(V(S_t)\)</span></li>
<li>So it requires a complete episode of interaction before updating our value function.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg" height="400" width="400"></p>
<ul>
<li>We always start the episode at the same starting point.</li>
<li>The agent takes actions using the policy. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.</li>
<li>We get the reward and the next state.</li>
<li>We terminate the episode if the cat eats the mouse or if the mouse moves &gt; 10 steps.</li>
<li>At the end of the episode, we have a list of State, Actions, Rewards, and Next States tuples For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]…]</li>
<li>The agent will sum the total rewards <span class="math inline">\(G_t\)</span> (to see how well it did).</li>
<li>It will then update <span class="math inline">\(V(S_t)\)</span> based on this formula</li>
<li>Then start a new game with this new knowledge</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg" height="400" width="400"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg" height="400" width="400"></p>
<ul>
<li><p>For instance, if we train a state-value function using Monte Carlo:</p></li>
<li><p>We initialize our value function so that it returns 0 value for each state</p></li>
<li><p>Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount)</p></li>
<li><p>Our mouse explores the environment and takes random actions</p></li>
<li><p>The mouse made more than 10 steps, so the episode ends .</p></li>
<li><p>We have a list of state, action rewards, next_state, we need to calculate the return <span class="math inline">\(G_t\)</span></p></li>
<li><p>$G_t = R_{t+1} +R_{t+2}+R_{t+3} $….</p></li>
<li><p>$G_t = R_{t+1} +R_{t+2}+R_{t+3} $….(for simplicity we don’t discount the rewards)</p></li>
<li><p><span class="math inline">\(G_t = 1+0+0+0+0+0+1+1+0+0\)</span></p></li>
<li><p><span class="math inline">\(G_t = 3\)</span> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg" width="400" height="400"></p></li>
<li><p><strong>I think it’s a hyperparameter, about how many times we iterate before we update the single state</strong></p></li>
</ul>
</section>
<section id="temporal-difference-learning-learning-at-each-step" class="level4">
<h4 class="anchored" data-anchor-id="temporal-difference-learning-learning-at-each-step">Temporal Difference Learning: learning at each step</h4>
<ul>
<li>Temporal differnce, on the other hand, waits for only one interaction (one step) <span class="math inline">\(S_{t+1}\)</span> to form a TD target and update <span class="math inline">\(V(S_t)\)</span> using <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(\gamma*V(S_{t+1})\)</span></li>
<li>The idea with TD is to update the <span class="math inline">\(V(S_t)\)</span> at each step</li>
<li>But because we didn’t experience an entire episode, we don’t have <span class="math inline">\(G_t\)</span> (expected return). Instead, we estimate <span class="math inline">\(G_t\)</span> by adding <span class="math inline">\(R_{t+1}\)</span>(reward that came by current action) and the discounted value of the next state</li>
<li>This is called bootstrapping. It’s called this because TD bases it’s update in part on an existing estimate <span class="math inline">\(V(S_{t+1})\)</span> and not a complete sample <span class="math inline">\(G_t\)</span></li>
<li>This method is called TD(0) or one-step TD (update the value function after any individual step)</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg" height="400" width="400"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg" height="400" width="400"></p>
<ul>
<li>For the mouse cat example we would have something as follows
<ul>
<li>We initialize our value function so that it returns 0 value for each state.</li>
<li>Our learning rate (lr) is 0.1, and our discount rate is 1 (no discount).</li>
<li>Our mouse begins to explore the environment and takes a random action: going to the left</li>
<li>It gets a reward <span class="math inline">\(R_{t+1}\)</span> since it eats a piece of cheese</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg" height="400" width="400"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg" height="400" width="400"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg" height="400" width="400"></p>
<ul>
<li><strong>Here as well, I believe it is a hyper parameter, how many times is a state updated</strong></li>
</ul>
</section>
<section id="to-summarize" class="level4">
<h4 class="anchored" data-anchor-id="to-summarize">To summarize</h4>
<ul>
<li>So basically first we decide which way to train our policy, once that is decided, we ask how do we train that’s where these 2 strategies come into picture.</li>
<li>With Monte Carlo, we update the value function from a complete episode, and so we use the actual accurate discounted return of this episode.</li>
<li>With temporal difference learning, we update the value function from a step, and we replace <span class="math inline">\(G_t\)</span>, which we don’t know with an estimated return called the TD target. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg" height="400" width="400"></li>
</ul>
</section>
</section>
<section id="introducing-q-learning" class="level3">
<h3 class="anchored" data-anchor-id="introducing-q-learning">Introducing Q-Learning</h3>
<p>What is Q-Learning ? - Q learning is an off-policy value-based method that uses a temporal difference approach to train it’s action value function: - Off-policy: We’ll see this at the end. - Value-based method: Finds the optimal policy indirectly by training a value or action-value function that will tell us the value of each state or each state-action pair. - TD approach: updates its action-value function at each step intead of at the end of the episode.</p>
<ul>
<li><p>Q-Learning is the algorithm we use to train our Q-function, an action-value function that determines the value of being at a particular state and taking a specific action at that state. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg" width="400" height="400"></p></li>
<li><p>In Q-Learning the Q stands for quality (the value) of that action at that state. Also to recap here is the difference between value and reward:</p></li>
<li><p>The value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to it’s policy.</p></li>
<li><p>The reward is the feedback the agent gets’ from the environment after performing an action at a state.</p></li>
<li><p>Internally, our Q-function is encoded by a <strong>Q-table, a table where each cell corresponds to a state-action pair value.</strong> Think of this Q-table as <strong>the memory or cheat sheet of our Q-function</strong>.</p></li>
<li><p>So overall in Q-learning we train our action value function known as Q-function. This Q-function is encoded as a Q-table, where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function.</p></li>
<li><p>Let’s take an example with this simple maze: <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg" height="400" width="400"></p></li>
<li><p>The Q-table is initialized. That’s why all the values are = 0. This table contains, for each state and action, the corresponding state-action values. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg" height="400" width="400"></p></li>
<li><p>Here we see that the state-action value of the initial state and going up is 0: <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg" height="400" width="400"></p></li>
<li><p>So: the Q-function uses a Q-table that has the value of each state-action pair. Given a state and action, our Q-function will seacrch inside it’s Q-table to output the value. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg" height="400" width="400"></p></li>
</ul>
<p>If we recap, Q-Learning is the RL algorithm that:</p>
<ul>
<li>Trains a Q-function (an action-value function), which internally is a Q-table that contains all the state-action pair values.</li>
<li>Given a state and action, our Q-function will search its Q-table for the corresponding value.</li>
<li>When the training is done, we have an optimal Q-function, which means we have optimal Q-table.</li>
<li>And if we have an optimal Q-function, we have an optimal policy since we know the best action to take at each state.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" height="400" width="400"></p>
<ul>
<li>In the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time, we initialize the Q-table to 0). As the agent explores the environment and we update the Q-table, it will give us a better and better approximation to the optimal policy.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg" width="400" height="400"></p>
<ul>
<li>Now that we understand what Q-Learning, Q-functions, and Q-tables are, let’s dive deeper into the Q-Learning algorithm.</li>
</ul>
<section id="the-q-learning-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="the-q-learning-algorithm">The Q-Learning Algorithm</h4>
<ul>
<li>This is the Q-Learning pseudocode; let’s study each part and see how it works with a simple example before implementing it. Don’t be intimidated by it, it’s simpler than it looks! We’ll go over each step.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" width="400" height="400"> - Step 1: We initialize the Q-table, <strong>most of the time, we initialize with values of 0</strong></p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg" width="400" height="400"></p>
<ul>
<li>Step 2: Choose an action using the epsilon-greedy strategy</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg" width="400" height="400"></p>
<ul>
<li>The idea is that, with an initial value of ɛ = 1.0:
<ul>
<li>With probability 1 — ɛ : we do exploitation (aka our agent selects the action with the highest state-action pair value).</li>
<li>With probability ɛ: we do exploration (trying random action).</li>
</ul></li>
<li>At the beginning of the training, the probability of doing exploration will be huge since ɛ is very high, so most of the time, we’ll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg" width="400" height="400"></p>
<ul>
<li>Step 3: Perform action <span class="math inline">\(A_t\)</span>, get reward <span class="math inline">\(R_{t+1}\)</span> and the next state <span class="math inline">\(S_{t+1}\)</span></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg" width="400" height="400"></p>
<ul>
<li>Step 4: Update Q(<span class="math inline">\(S_t, A_t\)</span>)</li>
</ul>
<p>Remember that in TD Learning, we update our policy or value function (depending on the RL method we choose) after one step of the interaction.</p>
<ul>
<li>To produce our TD target, we used the immediate reward <span class="math inline">\(R_{t+1}\)</span> plus the discounted value of the next state, computed by <strong>finding the action</strong> that <strong>maximizes the current Q-function at the next state.</strong> (We call that bootstrap).</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg" width="400" height="400"></p>
<ul>
<li>Therefore, our Q(<span class="math inline">\(S_t, A_t\)</span>) update formula goes like this:</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg" width="400" height="400"></p>
<ul>
<li>This means to update our <span class="math inline">\(Q(S_t, A_t)\)</span>:
<ul>
<li>We need <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1}\)</span></li>
<li>To update our Q-value at a given state-action pair, we use the TD target.</li>
</ul></li>
</ul>
<p>How do we form the TD target? - We obtain the reward after taking the action <span class="math inline">\(R_{t+1}\)</span> - To get this best state-action pair value for the next state, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value (So there is no probability involved here simply choose that action which will take us to the next state having max Q-value and thus our Q-value for this state becomes optimal) - Then when the update of this Q-value is done, we start in a new state (which will come by the action that leads us to that state which has the best Q-value) and select our action using a epsilon-greedy policy again - This is why we say that Q Learning is an off-policy algorithm</p>
</section>
<section id="off-policy-vs-on-policy" class="level4">
<h4 class="anchored" data-anchor-id="off-policy-vs-on-policy">Off-policy vs On-policy</h4>
<ul>
<li>The difference is subtle:
<ul>
<li>Off-policy: using a different policy for acting (inference) and updating (training).
<ul>
<li>For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).</li>
<li>Each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained.</li>
</ul></li>
</ul></li>
<li>Acting Policy: <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg" width="600" height="600"></li>
</ul>
<p>Is different from the policy we use during the training part:</p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg" width="300" height="300"></p>
<ul>
<li>On-policy: using the same policy for acting and updating.
<ul>
<li>For instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy</li>
<li>Each update only usees data collected while acting according to the most recent version of the policy</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg" width="600" height="600"></p>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg" width="600" height="600"></p>
</section>
<section id="lets-discuss-this-with-an-example" class="level4">
<h4 class="anchored" data-anchor-id="lets-discuss-this-with-an-example">Let’s discuss this with an example</h4>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-Example-2.jpg" width="600" height="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-1.jpg" height="600" width="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-2.jpg" height="600" width="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-1.jpg" height="600" width="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-3.jpg" height="600" width="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-4.jpg" height="600" width="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg" height="600" width="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-4.jpg" width="600" height="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-6.jpg" width="600" height="600">| <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-7.jpg" width="600" height="600"> <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-8.jpg" width="600" height="600"></p>
</section>
</section>
<section id="recap" class="level3">
<h3 class="anchored" data-anchor-id="recap">Recap</h3>
<section id="summary-of-value-based-functions-and-strategies-to-train-the-value-functions" class="level4">
<h4 class="anchored" data-anchor-id="summary-of-value-based-functions-and-strategies-to-train-the-value-functions">Summary of Value Based Functions and Strategies to train the value functions</h4>
<ul>
<li>We have two types of value-based functions:
<ul>
<li>State-value function: outputs the expected return if the agent starts at a given state and acts according to the policy forever after.</li>
<li>Action-value function: outputs the expected return if the agent starts in a given state, takes a given action at that state and then acts accordingly to the policy forever after.</li>
<li>In value-based methods, rather than learning the policy, we define the policy by hand and we learn a value function. If we have an optimal value function, we will have an optimal policy.</li>
</ul></li>
<li>There are two types of methods to learn a policy for a value function:
<ul>
<li>With the Monte Carlo method, we update the value function from a complete episode, and so we use the actual discounted return of this episode.</li>
<li>With the TD Learning method, we update the value function from a step, replacing the unknown <span class="math inline">\(G_t\)</span> with an estimated return called the TD target.</li>
</ul></li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg" width="600" height="600"></p>
</section>
<section id="q-learning-recap" class="level4">
<h4 class="anchored" data-anchor-id="q-learning-recap">Q-Learning Recap</h4>
<ul>
<li><p>Q-Learning is the RL algorithm that :</p>
<ul>
<li>Trains a Q-function, an action-value function encoded, in internal memory, by a Q-table containing all the state-action pair values.</li>
<li>Given a state and action, our Q-function will search its Q-table for the corresponding value. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg" width="600" height="600"></li>
</ul></li>
<li><p>When the training is done, we have an optimal Q-function, or, equivalently, an optimal Q-table.</p></li>
<li><p>And if we have an optimal Q-function, we have an optimal policy, since we know, for each state, the best action to take. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" width="600" height="600"></p></li>
<li><p>But, in the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time we initialize the Q-table to 0 values). But, as we explore the environment and update our Q-table it will give us a better and better approximation. <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg" width="600" height="600"></p></li>
<li><p>This is the Q-Learning pseudocode: <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" height="600" width="600"></p></li>
</ul>
</section>
</section>
<section id="glossary" class="level3">
<h3 class="anchored" data-anchor-id="glossary">Glossary</h3>
<ul>
<li><strong>Strategies to find the optimal policy</strong>
<ul>
<li>Policy-based methods. The policy is usually trained with a neural network to select what action to take given a state. In this case it is the neural network which outputs the action that the agent should take instead of using a value function. Depending on the experience received by the environment, the neural network will be re-adjusted and will provide better actions.</li>
<li>Value-based methods. In this case, a value function is trained to output the value of a state or a state-action pair that will represent our policy. However, this value doesn’t define what action the agent should take. In contrast, we need to specify the behavior of the agent given the output of the value function. For example, we could decide to adopt a policy to take the action that always leads to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy (or whatever decision the user takes) that uses the values of the value-function to decide the actions to take.</li>
</ul></li>
<li><strong>Among the value-based methods, we can find two main strategies</strong>
<ul>
<li>The state-value function. For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.</li>
<li>The action-value function. In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state and takes an action. Then it follows the policy forever after.</li>
</ul></li>
<li><strong>Epsilon-greedy strategy:</strong>
<ul>
<li>Common strategy used in reinforcement learning that involves balancing exploration and exploitation.</li>
<li>Chooses the action with the highest expected reward with a probability of 1-epsilon.</li>
<li>Chooses a random action with a probability of epsilon.</li>
<li>Epsilon is typically decreased over time to shift focus towards exploitation.</li>
</ul></li>
<li><strong>Greedy strategy:</strong>
<ul>
<li>Involves always choosing the action that is expected to lead to the highest reward, based on the current knowledge of the environment. (Only exploitation)</li>
<li>Always chooses the action with the highest expected reward.</li>
<li>Does not include any exploration.</li>
<li>Can be disadvantageous in environments with uncertainty or unknown optimal actions.</li>
</ul></li>
<li><strong>Off-policy vs on-policy algorithms</strong>
<ul>
<li>Off-policy algorithms: A different policy is used at training time and inference time</li>
<li>On-policy algorithms: The same policy is used during training and inference</li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="Yuvraj-Dhepe/DataBlog_V1" data-repo-id="R_kgDOJMncgA" data-category="Announcements" data-category-id="DIC_kwDOJMncgM4CVqbZ" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light_protanopia" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">This page is built with ❤️ and <a href="https://quarto.org">Quarto</a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/yuvraj-shivaji-dhepe-22974919a/">
      <i class="bi bi-linkedin" role="img" aria-label="My LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Yuvraj-Dhepe">
      <i class="bi bi-github" role="img" aria-label="My Github">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/@yuvrajdhepe">
      <i class="bi bi-medium" role="img" aria-label="My Medium">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>