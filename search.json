[
  {
    "objectID": "Self_Instructions.html",
    "href": "Self_Instructions.html",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "",
    "text": "References\n\nRead Later\n\nGithub_Actions\nGithub_Pages\n\n\n\n\nPublishing Content\n\nCreate a gh-pages branch\n\ngit checkout ‚Äìorphan gh-pages #Creation of new branch\ngit reset ‚Äìhard # make sure you‚Äôve committed changes before running this! # This makes the gh-pages branch as the current one.\ngit commit ‚Äìallow-empty -m ‚ÄúInitialising gh-pages branch‚Äù\ngit push  gh-pages #Remember for current branch the repo_name is my_repo, but by default the name is origin.\nFor publishing to github pages follow the site:https://quarto.org/docs/publishing/github-pages.html\n\nOnce this is done do gh-configuration on github repo.\n\ngithub-pages_config\n\nPublish first to Quarto Pub\n\nAdd the following content to _publish.yml in the main directory\nRun the command quarto publish with Quarto-Pub and follow the authorization steps.\n\nPublish to gh-pages\n\nCreate a publish.yml file in .github/workflows directory with the following content\nAdd _site and .quarto to .gitignore file to ignore those folders staging.\nTo render codes, setup a virtual env and update the publish.yml.\nRef: Executing Codes, Quarto setting up virtual_envs\n\n\n\nimgs\n\n  \n\n\nYAML Codes\n\n- source: project\n    quarto-pub:\n        - id: 1a303cb3-ce18-42f5-85e8-873316e2d3d8\n        url: 'https://quartopub.com/sites/yuvraj-dhepe/data-science-blog'\n\non:\n  workflow_dispatch:\n  push:\n    branches: master\n\nname: Quarto Publish on Github Pages\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n        with:\n          node-version: 16.x\n        # Update Node.js version to 16\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2 #using quarto-development github pages to use actions for gh-pages posting. \n        with:\n          # to install LaTeX to build PDF book\n          tinytex: true\n\n      - name: Render and Publish #Publishing to github pages and rendering.\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions so no need to add it again in secrets"
  },
  {
    "objectID": "docs/projects/Youtube_API_EDA.html",
    "href": "docs/projects/Youtube_API_EDA.html",
    "title": "Youtube API Scraping of Spiritual Channels",
    "section": "",
    "text": "With millions of users and billions of views, YouTube has become a major platform for spirituality content creators to share their knowledge and insights with a global audience. However, understanding what makes a video successful on YouTube can be a challenge, as the platform‚Äôs algorithm is complex and constantly evolving. Aspiring spirituality content creators can benefit from analyzing successful channels in their niche and identifying trends in their topics and presentation styles. In this project, we will explore the statistics of 9 popular spirituality channels on YouTube to gain insights on their audience, content, and engagement metrics.\n\n\n\nWithin this project, I would like to explore the following:\n\nGetting to know Youtube API and how to obtain video data.\nAnalyzing video data and verify different common ‚Äúmyths‚Äù about what makes a video do well on Youtube, for example:\n\nDoes the number of likes and comments matter for a video to get more views?\nDoes the video duration matter for views and interaction (likes/ comments)?\nDoes title length matter for views?\nHow many tags do good performing videos have? What are the common tags among these videos?\nAcross all the creators I take into consideration, how often do they upload new videos? On which days in the week?\n\nExplore the trending topics using NLP techniques\n\nWhich popular topics are being covered in the videos (e.g.¬†using wordcloud for video titles)?\n\nWhich questions are being asked in the comment sections in the videos\n\n\n\n\n\nObtain video meta data via the API app, from top 10 youtube niche channels.\nPreprocess data and engineer aditional features for analysis\nExploratory data analysis\nConclusions\n\n\n\n\n\nCreated my own dataset usign the Google API version 3.0\nThe channels are included as per my liking and self-thoughts about spirituality.\nAlso I have chosen channels based on their subscriber counts.\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport os\nimport time\nimport numpy as np\nfrom dateutil import parser\nimport isodate\nimport datetime\n\n# Data visualization libraries\nimport matplotlib\n#matplotlib.use('TkAgg') #default backend 'module://matplotlib_inline.backend_inline'\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nsns.set(style = 'darkgrid', color_codes=True)\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150}) #fig = plt.figure(dpi=200,figsize = (16,20)) similar\n#matplotlib.use(\"TkAgg\")\n%matplotlib inline\n# #plt.rcParams['font.family'] = 'Lohit-Devanagari'\n# #plt.rcParams[\"font.path\"] = \"/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf\"\n# english_font = fm.FontProperties(family = 'Arial', size = 14)\n# #mangal_font = fm.FontProperties(fname = \"~/downloads/fonts/mangal.ttf\",size = 14)\n# #%matplotlib inline\n\n#NLP Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom wordcloud import WordCloud\n# To install wordcloud use: python -m pip install -e git+https://github.com/amueller/word_cloud#egg=wordcloud\n# Google API\nfrom googleapiclient.discovery import build\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/yuvi_dh/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/yuvi_dh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n\n\n\nCreated a project on Google Developer Console\nRequested an Authorization Credential API Key\nEnabled Youtube API for the project work to send API requests to Youtube API services.\nGot the channel ID‚Äôs from my favorite channels which I would like to get stats on.\nFinally created the functions for getting the channel stats.\n\n\n\nCode\napi_key_1 = os.environ.get('yt_1')\n#api_key_2 = os.environ.get('yt_2')\n#api_key_3 = os.environ.get('yt_3')\n#api_key = api_key_1\n#print(api_key)\n\n\n\n\nCode\nimport os\napi_key_1 = os.environ.get('yt_1')\napi_key_2 = os.environ.get('yt_2')\napi_key_3 = os.environ.get('yt_3')\napi_key = api_key_1\n#print(api_key)\n\n# channel_ids = ['UCCy2rLnGdwoJcSAtixNdsPQ', # The Sanskrit Channel\n#               'UCzszIh4jH06kYp7k_DxhH5A',  # Chinmaya Channel\n#               'UCtDAJiFT4sy42oNPA8zo0sw',  # Star Bharat\n#               'UCdsQsaeI8pQZtgrMmbjGUug',  # Swaminarayan Aksharpith\n#               'UCqFg6QnwgtVHo1iFgpxrx-A',  # Mayapur TV\n#               'UCutvkeF3tVgItCX31QhJ2Dw',  # Nova Spiritual India\n#               'UCypj9Vvizo4cCERfDFIG3zw',  # Shemaroo Bhakti Darshan\n#               'UCxoQaZS8YdKkyfBwGZay-Xg',  # Hyper Quest\n#               'UC8HRYUBXTHv4mJ67Y5FitSg']  # Rajshri Soul\nchannel_ids = [\n    \"UCgeicB5AuF3MyyUto0-M5Lw\",  # Actualized.org\n    \"UCOnnmKlDZltHAqJLz-XIpGA\",  # Universe Inside You\n    \"UC48MclMZIY_EaOQwatzCpvw\",  # Aaron Doughty\n    \"UCg3F5jxUrSvJQICen48cX4w\",  # Mindvalley\n    \"UCEcMWs6GudljuLw0-Umf97A\",  # Spirit Science\n    \"UCFVqzO9_qHVckKqNC95o9tw\",  # Gaia    \n    \"UC7IcJI8PUf5Z3zKxnZvTBog\",  # The School of Life\n    \"UCz22l7kbce-uFJAoaZqxD1A\",  # Gaur Gopal Das\n    \"UCFJZQtrh5Ksncayy2FaoNbQ\",  # Vishuddha Das\n    \"UCkJEpR7JmS36tajD34Gp4VA\",  # Psych2Go\n]\nyoutube = build('youtube', 'v3',developerKey=api_key)\n\n\n\n\n\n\nCode\ndef get_channel_stats(channel_ids,yt=youtube):\n    '''\n    Get Channel statistics: title subscriber count, view count, video count, upload playlist\n    \n    Params:\n    youtube: the build object from googleapiclient.discovery\n    channel_ids: list of channel IDs\n    \n    Returns:\n    Dataframe containing the channel statistics for all channels in the provided list\n    \n    '''\n    all_data = []\n    request = youtube.channels().list(\n        part = 'snippet,contentDetails,statistics,brandingSettings',\n        id=','.join(channel_ids))\n    response = request.execute()\n    \n    for i in range(len(response['items'])):\n        data = dict(channelName = response['items'][i]['snippet']['title'],\n                    #countryName = response['items'][i]['snippet'][\"country\"],\n                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n                    views = response['items'][i]['statistics']['viewCount'],\n                    totalVideos = response['items'][i]['statistics']['videoCount'],\n                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'],\n                   publishedAt = isodate.parse_datetime(response['items'][i]['snippet']['publishedAt']))\n                    \n        all_data.append(data)\n    return pd.DataFrame(all_data)\n\n\n\n\nCode\ndef get_video_ids(playlist_id, max_results=1500,yt = youtube):\n    \"\"\"\n    Get list of video IDs of all videos in the given playlist, up to a maximum of 1500 videos\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    playlist_id: playlist ID of the channel\n    max_results: maximum number of videos to retrieve (default: 1500)\n    \n    Returns:\n    List of video IDs of all videos in the playlist, up to the maximum number of videos specified\n    \n    \"\"\"\n    \n    request = youtube.playlistItems().list(\n                part='contentDetails',\n                playlistId = playlist_id,\n                maxResults = min(max_results, 50))\n    response = request.execute()\n    \n    video_ids = []\n    num_videos = 0\n    \n    for i in range(len(response['items'])):\n        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n        num_videos += 1\n        if num_videos >= max_results:\n            break\n        \n    next_page_token = response.get('nextPageToken')\n    more_pages = True\n    \n    while more_pages and num_videos < max_results:\n        if next_page_token is None:\n            more_pages = False\n        else:\n            request = youtube.playlistItems().list(\n                        part='contentDetails',\n                        playlistId = playlist_id,\n                        maxResults = min(max_results - num_videos, 50),\n                        pageToken = next_page_token)\n            response = request.execute()\n    \n            for i in range(len(response['items'])):\n                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n                num_videos += 1\n                if num_videos >= max_results:\n                    break\n            \n            next_page_token = response.get('nextPageToken')\n        \n    return video_ids\n\n\n\n\nCode\ndef get_video_details(video_ids,yt = youtube):\n    \"\"\"\n    Get video statistics of all videos with given IDs\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with statistics of videos, i.e.:\n        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n        'duration', 'definition', 'caption'\n    \"\"\"\n        \n    all_video_info = []\n    \n    for i in range(0, len(video_ids), 50):\n        request = youtube.videos().list(\n            part=\"snippet,contentDetails,statistics\",\n            id=','.join(video_ids[i:i+50])\n        )\n        response = request.execute() \n\n        for video in response['items']:\n            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n                             'contentDetails': ['duration', 'definition', 'caption']\n                            }\n            video_info = {}\n            video_info['video_id'] = video['id']\n\n            for k in stats_to_keep.keys():\n                for v in stats_to_keep[k]:\n                    try:\n                        video_info[v] = video[k][v]\n                    except:\n                        video_info[v] = None\n\n            all_video_info.append(video_info)\n            \n    return pd.DataFrame(all_video_info)\n\n\n\n\nCode\ndef get_comments_in_videos(video_ids, yt = youtube):\n    \"\"\"\n    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n    Params:\n    \n    youtube: the build object from googleapiclient.discovery\n    video_ids: list of video IDs\n    \n    Returns:\n    Dataframe with video IDs and associated top level comment in text.\n    \n    \"\"\"\n    all_comments = []\n    \n    for video_id in video_ids:\n        try:   \n            request = youtube.commentThreads().list(\n                part=\"snippet,replies\",\n                videoId=video_id\n            )\n            response = request.execute()\n        \n            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n\n            all_comments.append(comments_in_video_info)\n            \n        except: \n            # When error occurs - most likely because comments are disabled on a video\n            print('Could not get comments for video ' + video_id)\n        \n    return pd.DataFrame(all_comments) \n\n\n\n\n\nUsing the get_channel_stats function defined below, now we are going to obtain the channel statistics for the above channels in scope\n\n\nCode\nchannel_data = get_channel_stats(channel_ids)\nchannel_data.to_csv(\"./files/spiritual_channel_data_original.csv\")\n\n\n\n\nCode\n#channel_data\n\n\n\n\nCode\n# Creation of copy so that I save a dummy df and also a csv, to not keep reusing youtube credits i.e. 10k per day.\nl_channel_data = pd.read_csv(\"./files/spiritual_channel_data_original.csv\",index_col=0)\nl_channel_data\n\n\n\n\n\n\n  \n    \n      \n      channelName\n      subscribers\n      views\n      totalVideos\n      playlistId\n      publishedAt\n    \n  \n  \n    \n      0\n      Aaron Doughty\n      1450000\n      141121587\n      1793\n      UU48MclMZIY_EaOQwatzCpvw\n      2014-07-10 04:24:58+00:00\n    \n    \n      1\n      The School of Life\n      8380000\n      820210489\n      902\n      UU7IcJI8PUf5Z3zKxnZvTBog\n      2010-05-18 16:46:57+00:00\n    \n    \n      2\n      Psych2Go\n      10800000\n      1477376571\n      2380\n      UUkJEpR7JmS36tajD34Gp4VA\n      2014-10-05 06:27:31+00:00\n    \n    \n      3\n      Gaia\n      1520000\n      127649328\n      645\n      UUFVqzO9_qHVckKqNC95o9tw\n      2008-08-06 15:26:41+00:00\n    \n    \n      4\n      Universe Inside You\n      1790000\n      118060252\n      126\n      UUOnnmKlDZltHAqJLz-XIpGA\n      2017-03-30 10:55:39+00:00\n    \n    \n      5\n      Mindvalley\n      2260000\n      468381830\n      1662\n      UUg3F5jxUrSvJQICen48cX4w\n      2014-04-23 08:21:13+00:00\n    \n    \n      6\n      Gaur Gopal Das\n      4860000\n      288878180\n      356\n      UUz22l7kbce-uFJAoaZqxD1A\n      2016-04-12 18:16:24+00:00\n    \n    \n      7\n      Vishuddha Das\n      596000\n      40838427\n      381\n      UUFJZQtrh5Ksncayy2FaoNbQ\n      2014-09-24 14:04:59+00:00\n    \n    \n      8\n      Actualized.org\n      1100000\n      107549192\n      528\n      UUgeicB5AuF3MyyUto0-M5Lw\n      2012-01-23 20:08:34+00:00\n    \n    \n      9\n      Spirit Science\n      1300000\n      79311991\n      306\n      UUEcMWs6GudljuLw0-Umf97A\n      2011-12-29 05:49:29+00:00\n    \n  \n\n\n\n\n\n\nCode\n# Copy used for further manipulation and original of l_channel_data can be used to load this chdd any time.\nchdd = l_channel_data.copy()\nchdd\n\n\n\n\n\n\n  \n    \n      \n      channelName\n      subscribers\n      views\n      totalVideos\n      playlistId\n      publishedAt\n    \n  \n  \n    \n      0\n      Aaron Doughty\n      1450000\n      141121587\n      1793\n      UU48MclMZIY_EaOQwatzCpvw\n      2014-07-10 04:24:58+00:00\n    \n    \n      1\n      The School of Life\n      8380000\n      820210489\n      902\n      UU7IcJI8PUf5Z3zKxnZvTBog\n      2010-05-18 16:46:57+00:00\n    \n    \n      2\n      Psych2Go\n      10800000\n      1477376571\n      2380\n      UUkJEpR7JmS36tajD34Gp4VA\n      2014-10-05 06:27:31+00:00\n    \n    \n      3\n      Gaia\n      1520000\n      127649328\n      645\n      UUFVqzO9_qHVckKqNC95o9tw\n      2008-08-06 15:26:41+00:00\n    \n    \n      4\n      Universe Inside You\n      1790000\n      118060252\n      126\n      UUOnnmKlDZltHAqJLz-XIpGA\n      2017-03-30 10:55:39+00:00\n    \n    \n      5\n      Mindvalley\n      2260000\n      468381830\n      1662\n      UUg3F5jxUrSvJQICen48cX4w\n      2014-04-23 08:21:13+00:00\n    \n    \n      6\n      Gaur Gopal Das\n      4860000\n      288878180\n      356\n      UUz22l7kbce-uFJAoaZqxD1A\n      2016-04-12 18:16:24+00:00\n    \n    \n      7\n      Vishuddha Das\n      596000\n      40838427\n      381\n      UUFJZQtrh5Ksncayy2FaoNbQ\n      2014-09-24 14:04:59+00:00\n    \n    \n      8\n      Actualized.org\n      1100000\n      107549192\n      528\n      UUgeicB5AuF3MyyUto0-M5Lw\n      2012-01-23 20:08:34+00:00\n    \n    \n      9\n      Spirit Science\n      1300000\n      79311991\n      306\n      UUEcMWs6GudljuLw0-Umf97A\n      2011-12-29 05:49:29+00:00\n    \n  \n\n\n\n\n\n\nCode\n# Setting Numeric n Categorical columns\nnumeric_cols = ['subscribers','views','totalVideos']\nchdd[numeric_cols] = chdd[numeric_cols].apply(pd.to_numeric,errors = 'coerce')\n\n# Convert publishedAt column to datetime\nchdd['publishedAt'] =(pd.to_datetime(chdd['publishedAt']))\n\n# Extract year, month, and time into separate columns\nchdd['publishingYear'] = chdd['publishedAt'].dt.year\nchdd['publishingMonth'] = chdd['publishedAt'].dt.month\nchdd['publishingTime'] = chdd['publishedAt'].dt.time\n\n# Get month name\nchdd['publishingMonthName'] = chdd['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nchdd.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\n# chdd['channelName'] = chdd['channelName'].replace('STAR ‡§≠‡§æ‡§∞‡§§','Star Bharat')\n# chdd \n# Was for other spiritual channels, but isn't necessary now.\n\n\n\n\nCode\nchdd.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   channelName          10 non-null     object\n 1   subscribers          10 non-null     int64 \n 2   views                10 non-null     int64 \n 3   totalVideos          10 non-null     int64 \n 4   playlistId           10 non-null     object\n 5   publishingYear       10 non-null     int64 \n 6   publishingMonth      10 non-null     int64 \n 7   publishingTime       10 non-null     object\n 8   publishingMonthName  10 non-null     object\ndtypes: int64(5), object(4)\nmemory usage: 800.0+ bytes\n\n\n\n\n\n\n\nCode\nmatplotlib.get_backend()\n\n\n'module://matplotlib_inline.backend_inline'\n\n\n\n\nCode\n#matplotlib.use??\n#sns.barplot??\n\n\n\n\nCode\n# Fixing colors for each channel\n#palette = sns.color_palette('pastel6', n_colors=10)\ncolors = plt.cm.tab10.colors[:10]\n#colors = sns.color_palette('Set1', 10)\nchannel_colors = {}\nchdd.sort_values('subscribers',ascending=False,inplace=True)\nfor i, channel in enumerate(chdd['channelName']):\n    channel_colors[channel] = colors[i]\n\n\n\n\nCode\nax = sns.barplot(x='channelName', y='subscribers', data=chdd.sort_values('subscribers', ascending=False), palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n#%matplotlib inline\nax = sns.barplot(x='channelName', y='views', data=chdd.sort_values('views', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, some channels have more subscribers but less views and vice versa. For example, GGD channel has significantly more subscribers than Mind Valley channel, but less views in total.\nPsych2Go and The School of Life hold onto their ranks in both views and subscriber count\n\n\n\n\n\nIn the next step, we will obtain the video statistics for all the channels. In total, we obtained 8700 videos as seen in below.\n\n\nCode\n# # Create a dataframe with video statistics and comments from all channels\n# video_df = pd.DataFrame()\n# comments_df = pd.DataFrame()\n\n# for c in channel_data['channelName'].unique():\n#     print(\"Getting video information from channel: \" + c)\n#     playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n#     video_ids = get_video_ids(playlist_id,max_results=2000,yt = youtube)\n    \n#     # get video data\n#     video_data = get_video_details(video_ids,yt = youtube)\n#     # get comment data\n#     comments_data = get_comments_in_videos(video_ids,yt = youtube)\n\n#     # append video data together and comment data toghether\n#     video_df = video_df.append(video_data, ignore_index=True)\n#     comments_df = comments_df.append(comments_data, ignore_index=True)\n\n\n\n\nCode\n# video_df.to_csv(\"./files/spirituality_video_df_original.csv\")\n# comments_df.to_csv(\"./files/spirituality_comments_df_original.csv\")\n\n\n\n\nCode\nl_video_df = pd.read_csv(\"./files/spirituality_video_df_original.csv\",index_col=0)\nl_comments_df = pd.read_csv(\"./files/spirituality_comments_df_original.csv\",index_col=0)\n\n\n\n\nCode\nviddf = l_video_df.copy()\ncomdf = l_comments_df.copy()\n\n\n\n\nCode\n# Create publish day (in the week) column\nviddf['publishedAt'] =  viddf['publishedAt'].apply(lambda x: parser.parse(x)) \nviddf['pushblishDayName'] = viddf['publishedAt'].apply(lambda x: x.strftime(\"%A\"))\n\n# Convert publishedAt column to datetime\nviddf['publishedAt'] =(pd.to_datetime(viddf['publishedAt']))\n\n\n# Extract year, month, and time into separate columns\nviddf['publishingYear'] = viddf['publishedAt'].dt.year\nviddf['publishingMonth'] = viddf['publishedAt'].dt.month\nviddf['publishingTime'] = viddf['publishedAt'].dt.time\n\n# Get month name\nviddf['publishingMonthName'] = viddf['publishedAt'].dt.strftime(\"%B\")\n\n# Dropping the published At column\nviddf.drop(['publishedAt'],axis = 1, inplace = True)\n\n\n\n\nCode\nviddf\n\n\n\n\n\n\n  \n    \n      \n      video_id\n      channelTitle\n      title\n      description\n      tags\n      viewCount\n      likeCount\n      favouriteCount\n      commentCount\n      duration\n      definition\n      caption\n      pushblishDayName\n      publishingYear\n      publishingMonth\n      publishingTime\n      publishingMonthName\n    \n  \n  \n    \n      0\n      Q8dOR0bN-Mw\n      Gaia\n      Man Able to Project Thoughts Through Crystals\n      Learn how crystals can hold a powerful place i...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      7218.0\n      1309.0\n      NaN\n      31.0\n      PT1M\n      hd\n      False\n      Saturday\n      2023\n      4\n      15:00:28\n      April\n    \n    \n      1\n      Ja1m4mHjZJY\n      Gaia\n      FULL EPISODE: Channeling - A Bridge to the Beyond\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      18104.0\n      1394.0\n      NaN\n      113.0\n      PT26M53S\n      hd\n      False\n      Friday\n      2023\n      3\n      16:00:07\n      March\n    \n    \n      2\n      qixrU_pwvD0\n      Gaia\n      This Man Taught Princess Diana to Express Hers...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      9462.0\n      640.0\n      NaN\n      51.0\n      PT4M47S\n      hd\n      False\n      Wednesday\n      2023\n      3\n      16:00:40\n      March\n    \n    \n      3\n      deZsy9GYn8w\n      Gaia\n      Mysterious, Ancient Satellite Is Monitoring Earth\n      Five of the world‚Äôs leading experts unravel my...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      73915.0\n      10072.0\n      NaN\n      338.0\n      PT1M\n      hd\n      False\n      Tuesday\n      2023\n      3\n      15:00:21\n      March\n    \n    \n      4\n      Kbfpd8zp3mk\n      Gaia\n      How Shamanic Dancing Leads to Altered States o...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      14585.0\n      932.0\n      NaN\n      54.0\n      PT3M56S\n      hd\n      False\n      Monday\n      2023\n      3\n      16:00:21\n      March\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8695\n      ZS3sOfs5jBY\n      Actualized.org\n      Get Coached\n      http://www.actualized.org/coaching\\n\\nResults ...\n      ['life coaching', 'Coaching (Profession)']\n      10102.0\n      323.0\n      NaN\n      31.0\n      PT16M23S\n      hd\n      False\n      Saturday\n      2013\n      5\n      11:48:19\n      May\n    \n    \n      8696\n      mlIYVsuIofs\n      Actualized.org\n      Be Different to Be Successful\n      How doing things differently in your life is n...\n      NaN\n      27343.0\n      929.0\n      NaN\n      41.0\n      PT26M5S\n      hd\n      False\n      Friday\n      2013\n      4\n      10:24:43\n      April\n    \n    \n      8697\n      8cbtMhHpLC8\n      Actualized.org\n      Why Life Coaching Works\n      An explanation of how coaching works and why i...\n      NaN\n      20081.0\n      647.0\n      NaN\n      42.0\n      PT21M\n      hd\n      False\n      Friday\n      2013\n      4\n      08:53:35\n      April\n    \n    \n      8698\n      C1QYF5WYzCo\n      Actualized.org\n      How to Invest In Yourself\n      How a long-term investment mindset in yourself...\n      NaN\n      61685.0\n      1805.0\n      NaN\n      193.0\n      PT19M13S\n      hd\n      True\n      Friday\n      2013\n      4\n      07:29:27\n      April\n    \n    \n      8699\n      _874QVgwvEk\n      Actualized.org\n      Mastery Part 1\n      Test video for self-development blog. This vid...\n      ['mastery', 'self-help', 'self-development', '...\n      5496.0\n      217.0\n      NaN\n      63.0\n      PT16M12S\n      hd\n      False\n      Wednesday\n      2012\n      8\n      08:24:31\n      August\n    \n  \n\n8700 rows √ó 17 columns\n\n\n\nLet‚Äôs take a look at the comment_df as well. We only get 8674 comments in total due to the fact that we limited to 10 first comments on the video to avoid exceeding the Youtube API quota limit.\n\n\nCode\ncomdf\n\n\n\n\n\n\n  \n    \n      \n      video_id\n      comments\n    \n  \n  \n    \n      0\n      Q8dOR0bN-Mw\n      ['Intentionally concentrating the mind setting...\n    \n    \n      1\n      Ja1m4mHjZJY\n      ['8,000+ Films, Shows & Classes on Gaia. Start...\n    \n    \n      2\n      qixrU_pwvD0\n      ['8,000+ Films, Shows & Classes on Gaia. Start...\n    \n    \n      3\n      deZsy9GYn8w\n      ['Watch more of the Awakening Conference with ...\n    \n    \n      4\n      Kbfpd8zp3mk\n      ['8,000+ Films, Shows & Classes on Gaia. Start...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      8669\n      ZS3sOfs5jBY\n      ['Thanks', 'A legend is born..', 'Currently go...\n    \n    \n      8670\n      mlIYVsuIofs\n      ['Great video üí´', 'Inspiring', 'Thanks', 'Than...\n    \n    \n      8671\n      8cbtMhHpLC8\n      ['Thanks.', '0:00 a legend was born', 'Magic.'...\n    \n    \n      8672\n      C1QYF5WYzCo\n      ['Thanks', \"If you discovered this/him you've ...\n    \n    \n      8673\n      _874QVgwvEk\n      ['Proud of you man. You helped me a lot', 'Who...\n    \n  \n\n8674 rows √ó 2 columns\n\n\n\n\n\n\n\nTo be able to make use of the data for analysis, we need to perform a few pre-processing steps. Firstly, I would like reformat some columns, especially the date and time columns such as ‚ÄúpushlishedAt‚Äù and ‚Äúduration‚Äù. In addition, I also think it is necessary to enrich the data with some new features that might be useful for understanding the videos‚Äô characteristics. Also I removed the favorite count column as it‚Äôs completely blank, rest other columns having null values weren‚Äôt modified for simplicity.\n\n\n\n\nCode\nviddf.isnull().sum(axis = 0)\n\n\nvideo_id                  0\nchannelTitle              0\ntitle                     0\ndescription               3\ntags                    554\nviewCount                 5\nlikeCount                 6\nfavouriteCount         8700\ncommentCount              8\nduration                  0\ndefinition                0\ncaption                   0\npushblishDayName          0\npublishingYear            0\npublishingMonth           0\npublishingTime            0\npublishingMonthName       0\ndtype: int64\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nax = sns.heatmap(viddf.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nviddf.publishingYear.sort_values().value_counts()\n# Videos are from 2011 to 2023\n#- During the covid time the channels became more active it seems over all.\n\n\n2022    1667\n2021    1355\n2020    1236\n2018    1027\n2019     950\n2017     863\n2016     516\n2023     392\n2015     355\n2014     248\n2013      45\n2012      44\n2011       2\nName: publishingYear, dtype: int64\n\n\n\n\n\n\n\nCode\ncols = ['viewCount', 'likeCount','commentCount']\nviddf[cols] = viddf[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n\n\n\n\nI want to enrich the data for further analyses, for example: - convert video duration to seconds instead of the current default string format - calculate number of tags for each video - calculate comments and likes per 1000 view ratio - calculate title character length\n\n\nCode\n# convert duration to seconds\nviddf['durationSecs'] = viddf['duration'].apply(lambda x: isodate.parse_duration(x))\nviddf['durationSecs'] = viddf['durationSecs'].astype('timedelta64[s]')\n\n\n\n\nCode\n# Add number of tags\nviddf['tagsstr'] = viddf.tags.apply(lambda x: 0 if x is None else str((x))) #tags were not in proper format so converting them to str\nviddf['tagsCount'] = viddf.tagsstr.apply(lambda x: 0 if (x == 0 or x =='nan') else len(eval(x)))\n\n\n\n\nCode\n# Comments and likes per 1000 view ratio\nviddf['likeRatio'] = viddf['likeCount']/ viddf['viewCount'] * 1000\nviddf['commentRatio'] = viddf['commentCount']/ viddf['viewCount'] * 1000\n\n\n\n\nCode\n# Title character length\nviddf['titleLength'] = viddf['title'].apply(lambda x: len(x))\n\n\n\n\nCode\n# Dropping the favourite Count as all of it is empty\nviddf.drop(['favouriteCount'],axis = 1, inplace=True)\n\n\n\n\nCode\n#Observing df before proceeding further\nviddf \n\n\n\n\n\n\n  \n    \n      \n      video_id\n      channelTitle\n      title\n      description\n      tags\n      viewCount\n      likeCount\n      commentCount\n      duration\n      definition\n      ...\n      publishingYear\n      publishingMonth\n      publishingTime\n      publishingMonthName\n      durationSecs\n      tagsstr\n      tagsCount\n      likeRatio\n      commentRatio\n      titleLength\n    \n  \n  \n    \n      0\n      Q8dOR0bN-Mw\n      Gaia\n      Man Able to Project Thoughts Through Crystals\n      Learn how crystals can hold a powerful place i...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      7218.0\n      1309.0\n      31.0\n      PT1M\n      hd\n      ...\n      2023\n      4\n      15:00:28\n      April\n      60.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      181.352175\n      4.294819\n      45\n    \n    \n      1\n      Ja1m4mHjZJY\n      Gaia\n      FULL EPISODE: Channeling - A Bridge to the Beyond\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      18104.0\n      1394.0\n      113.0\n      PT26M53S\n      hd\n      ...\n      2023\n      3\n      16:00:07\n      March\n      1613.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      76.999558\n      6.241715\n      49\n    \n    \n      2\n      qixrU_pwvD0\n      Gaia\n      This Man Taught Princess Diana to Express Hers...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      9462.0\n      640.0\n      51.0\n      PT4M47S\n      hd\n      ...\n      2023\n      3\n      16:00:40\n      March\n      287.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      67.638977\n      5.389981\n      63\n    \n    \n      3\n      deZsy9GYn8w\n      Gaia\n      Mysterious, Ancient Satellite Is Monitoring Earth\n      Five of the world‚Äôs leading experts unravel my...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      73915.0\n      10072.0\n      338.0\n      PT1M\n      hd\n      ...\n      2023\n      3\n      15:00:21\n      March\n      60.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      136.264628\n      4.572820\n      49\n    \n    \n      4\n      Kbfpd8zp3mk\n      Gaia\n      How Shamanic Dancing Leads to Altered States o...\n      8,000+ Films, Shows & Classes on Gaia. Start Y...\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      14585.0\n      932.0\n      54.0\n      PT3M56S\n      hd\n      ...\n      2023\n      3\n      16:00:21\n      March\n      236.0\n      ['gaia', 'yoga on gaia', 'cosmic disclosure', ...\n      26\n      63.901268\n      3.702434\n      61\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8695\n      ZS3sOfs5jBY\n      Actualized.org\n      Get Coached\n      http://www.actualized.org/coaching\\n\\nResults ...\n      ['life coaching', 'Coaching (Profession)']\n      10102.0\n      323.0\n      31.0\n      PT16M23S\n      hd\n      ...\n      2013\n      5\n      11:48:19\n      May\n      983.0\n      ['life coaching', 'Coaching (Profession)']\n      2\n      31.973867\n      3.068699\n      11\n    \n    \n      8696\n      mlIYVsuIofs\n      Actualized.org\n      Be Different to Be Successful\n      How doing things differently in your life is n...\n      NaN\n      27343.0\n      929.0\n      41.0\n      PT26M5S\n      hd\n      ...\n      2013\n      4\n      10:24:43\n      April\n      1565.0\n      nan\n      0\n      33.975789\n      1.499470\n      29\n    \n    \n      8697\n      8cbtMhHpLC8\n      Actualized.org\n      Why Life Coaching Works\n      An explanation of how coaching works and why i...\n      NaN\n      20081.0\n      647.0\n      42.0\n      PT21M\n      hd\n      ...\n      2013\n      4\n      08:53:35\n      April\n      1260.0\n      nan\n      0\n      32.219511\n      2.091529\n      23\n    \n    \n      8698\n      C1QYF5WYzCo\n      Actualized.org\n      How to Invest In Yourself\n      How a long-term investment mindset in yourself...\n      NaN\n      61685.0\n      1805.0\n      193.0\n      PT19M13S\n      hd\n      ...\n      2013\n      4\n      07:29:27\n      April\n      1153.0\n      nan\n      0\n      29.261571\n      3.128800\n      25\n    \n    \n      8699\n      _874QVgwvEk\n      Actualized.org\n      Mastery Part 1\n      Test video for self-development blog. This vid...\n      ['mastery', 'self-help', 'self-development', '...\n      5496.0\n      217.0\n      63.0\n      PT16M12S\n      hd\n      ...\n      2012\n      8\n      08:24:31\n      August\n      972.0\n      ['mastery', 'self-help', 'self-development', '...\n      5\n      39.483261\n      11.462882\n      14\n    \n  \n\n8700 rows √ó 22 columns\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.set(rc={'figure.figsize':(10,12),'figure.dpi':100})\nax = sns.barplot(x='channelName', y='totalVideos', data=chdd.sort_values('totalVideos', ascending=False),palette=channel_colors)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# group by channel and year, count the videos\ncount_data = viddf.groupby(['channelTitle', 'publishingYear'])['video_id'].count().reset_index(name=\"count\")\n\n# plot using seaborn\nsns.set(rc={'figure.figsize':(10,8),'figure.dpi':150,})\nax = sns.barplot(data=count_data, x='publishingYear', y='count', hue='channelTitle', palette=channel_colors)\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., ncol=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWith the video statistics for all channel, now we can see how the views are distributed per channel. - Boxplot provides us with the following insights: - Mindvalley and Psychgo have too many videos that went viral over time. - The School of life and Universe inside you seem to have more broad range of video views over time as their IQR seems more via the box length. - I think the views and subscriber count are correlated since the channel having more subscribers tend to have more views.\n\nViolinplot confirms us that few channels seem to have quite variation among views of the videos, like Aaron Daughty and Vishuddha Das videos have recieved more varied views for the videos.\n\n\n\n\n\n\nCode\nax = sns.violinplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\n#ax.set_ylim(ymin = -1e3, ymax = 1e5)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nax = sns.boxplot(x='channelTitle', y='viewCount', data=viddf.sort_values('viewCount', ascending=False), palette = channel_colors)\nax.set_ylim(ymin = -1e3, ymax = 2.4e6)\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFirstly, I would like to check if comments and likes do correlate with how many views a video would get. In the plots below, it can be observed that the number of views and number of comments/ likes strongly correlated with each other. The number of likes seems to suggest stronger correlation than the number of comments. However, this is expected as the more people watching a video, the more likely this video will get comments and likes. To correct for this factor, we will plot these relationships again using the comments per 1000 view and likes per 1000 view ratios.\n\n\n\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentCount\", y = \"viewCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"likeCount\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\nNow we will take a look at the correlation if we look at the comment ratio and like ratio instead of the absolute number. It seems that more views is leading to more comments and more likes as well, but after a certain point I think, with views viewers, don‚Äôt write comments that much.\n\n\nCode\nfig, ax =plt.subplots(1,2)\nsns.scatterplot(data = viddf, x = \"commentRatio\", y = \"viewCount\", ax=ax[0])\n#ax[0].set_ylim(0,9e6)\n#ax[1].set_ylim(0,9e6)\nsns.scatterplot(data = viddf, x = \"likeRatio\", y = \"viewCount\", ax=ax[1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in the histogram below, most videos are between 1600 to 1800 seconds, which is about 20 to 30 minutes. Here I have to limit the duration to 10,000 because of some really long videos (potentially streaming videos).\n\n\n\n\n\nCode\nax = sns.histplot(data=viddf[viddf['durationSecs'] < 10000], x=\"durationSecs\", bins=30, color=\"#9368b7\")\nplt.show()\n\n\n\n\n\nNow we plot the duration against comment count and like count. It can be seen that actually shorter videos tend to get more likes and comments than very long videos.\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nfig, ax =plt.subplots(1,2)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"commentCount\", ax=ax[0])\nsns.scatterplot(data = viddf, x = \"durationSecs\", y = \"likeCount\", ax=ax[1])\n#ax[0].set_ylim(0,1e5)\nax[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n#ax[1].set_ylim(0,1e5)\nax[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThere is no clear relationship between title length and views as seen the scatterplot below, but most-viewed videos tend to have average title length of 35-60 characters\n\n\nCode\nax = sns.scatterplot(data = viddf, x = \"titleLength\", y = \"viewCount\")\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nax.set_ylim(0,1e7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAs I‚Äôm interested to see what the creators are making videos about and which terms most frequently appear in their video titles, I will create a wordcloud for the most common words. We first need to remove the stopwords such as ‚Äúyou‚Äù, ‚ÄúI‚Äù, ‚Äúthe‚Äù, etc. which do note contribute a lot to the meaning of the title. It can be seen that the main words posted in title are Life, Attraction, Love, Meditation, People and Thing.\n\n\nCode\nstop_words = set(stopwords.words('english'))\nviddf['title_no_stopwords'] = viddf['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in viddf['title_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(30, 20))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n\nwordcloud = WordCloud(width = 1920, height = 780, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()\n\n\n\n\n\n\n\nCode\n#sorted(wordcloud.words_.items(), key = lambda x: x[1],reverse = False)\n\n\n\n\n\nIt seems that most videos have between 10 and 45 tags. The relationship between number of tags and view count is not clearly seen, but too few tags or too many tags do seem to correlate with fewer views.\n\n\nCode\nplot = sns.scatterplot(data = viddf, x = \"tagsCount\", y = \"viewCount\")\nplot.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIt‚Äôs interesting to see that more videos are uploaded on Mondays, Wednesdays and Fridays. It seems the pattern is alternative in uploading the videos. This might be because of maintaining a consistency on channel, like when the user can more expect the videos, on a consistent basis.\n\n\nCode\nday_df = pd.DataFrame(viddf['pushblishDayName'].value_counts())\nweekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nday_df = day_df.reindex(weekdays)\nax = day_df.reset_index().plot.bar(x='index', y='pushblishDayName', rot=0)\nplt.xlabel(\"Days\")\nplt.ylabel(\"Counts\")\nplt.legend(labels = [\"Counts\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nLike the video titles, the video comments also revolve around Love, Life, Sign, Thing, Attraction words\n\n\nCode\nstop_words = set(stopwords.words('english'))\ncomdf['comments_no_stopwords'] = comdf['comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n\nall_words = list([a for b in comdf['comments_no_stopwords'].tolist() for a in b])\nall_words_str = ' '.join(all_words) \n\n\n\n\nCode\n# matplotlib.use('module://matplotlib_inline.backend_inline')\n# %matplotlib inline\nwordcloud = WordCloud(width = 1980, height = 720, random_state=1, background_color='black', \n                      colormap='viridis', collocations=False).generate(all_words_str)\nplot_cloud(wordcloud)\nplt.show()"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html",
    "href": "docs/projects/B1_Sales_Insights.html",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "",
    "text": "AtliQ Hardware supplies computer hardware and peripherals to various clients across India and has a head office in Delhi and regional offices throughout India.\nBhavin Patel, the sales director of the company, is facing challenges in tracking the sales in the dynamically growing market and getting insights into his business.\nThe regional managers tend to give verbal feedback that paints a rosy picture and provide complex Excel files, making it challenging for Bhavin to get a clear picture of the business.\nBhavin wants simple and digestible insights, such as the top five customers, the weakest regions, and the year-to-date revenue, etc,. to make data-driven decisions for the business.\nThe ultimate goal is to improve the declining sales and make informed business decisions based on real-time data analytics.\nPowerBI can provide a solution to visualize the data and present simple and actionable insights for Bhavin.\n\nP.S. The problem statement is virtual and doesn‚Äôt relate to any real-world entity, any such occurence is just a matter of coincidence."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "href": "docs/projects/B1_Sales_Insights.html#project-planning-and-data-discovery",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Project Planning and Data Discovery",
    "text": "Project Planning and Data Discovery\n\nProject planning using AIMS grid.\n\nBhavin patel who is a sales director of at Atliq hardware realizes the pains of getting sales insights and thinks of having a data analytical solution for this problem.\nHe calls a couple of people like an IT head or a head of data department and schedules a meeting on how exactly we should do this data analytics project.\nTogether with IT head, he also gathers other stake holders in the room together and they brainstorm on project planning using AIMS grid.\n\nAIMS grid\n\nAIMS grid is a project management tool having four components.\n\nFirst component is Purpose\n\nDetermining what is the pain point and what needs to be done to resolve the pain point\n\nSecond component is the Stakeholders\n\nIn this project who all will be involved\nThe main stakeholders involved for this data analytical projects are the marketing team ,the sales team who is facing these issues to do proper sales analysis\nIT team (The Falcons) a group of software engineers that AtliQ hardware has who manages all the software requirements of Atliq\nThe data analytics (The Data Masters) team which is inhouse\n\nThird component is the end result i.e., once the project is over what needs to be achieved\n\nFor the data analytics project the end result is to have a power bi dashboard something that our sales director or even Regional Managers can go and look into and it gives you a real-time information on the sales numbers\n\nFourth component is the success criteria after the project is over, how do we define that the project was successful\n\nIn our project scenario the success criteria would be, we want the costs to go down by 10% on management of these sales data\nWe want to utilize the saved business time by the use of dashboard to take data driven descisions and increase the growth of company by let‚Äôs say 10%\nDashboards uncovering sales order insights with latest data available\nSales team able to take better decisions and prove 10% cost savings of total spend\nSales Analysts stop data gathering manually (merging excel files) in order to save business time and reinvest it value added activity\n\n\n\nOther Important points\n\nFalcons team, manages the software of Atliq Sales Management which is keeping track of all the sales number so whenever they sell any computer or any hard days in any local region this software is printing the invoice so it has all the records stored in a MySQL database.\nData Masters team will reach out to Falcons and to use the AtliQ SQL database because this is the database which has all the records that we need for our analytics and what we‚Äôll do is we‚Äôll integrate MySQL. We will use MySQL as a source in our power bi tool and we will build dashboard on top of it.\n\nReal World Scenarios\n\nIn real world direct access of data from IT team is not allowed cause if the data volume is high we want to make sure that MySQL database is not affected by the queries that Falcons are doing in your PowerBi, so many times companies, have a Data Warehouse as part of the Data Ecosystem.\nMySQL which is also known as OLTP which is online transaction processing system it is a very critical system and a company cannot afford for that system to go down otherwise the regular sales operations gets hampered.\nFor any business analytics is important but then it‚Äôs a secondary thing so what companies do is they pull the data from OLTP which is MySQL in our case they do all the transformation which is also called ETL which means extract transform and load and after doing that transformation they store the data in a data warehouse.\nThere are various types of Data Warehouse management tools like Snowflake and there are various ways to transform the data like using Pandas or other data processing libraries.\nMost companies working on huge data analytical projects have another in-house teams who manage the data warehouse known as Data Engineers (The Data Miners).\n\nConclusion & Data Discovery\n\nAssuming our data is not that big, the data is simply taken from the Falcons, by our Data Masters.\nThe Data Masters will plug PowerBI directly to SQL database and build the necessary analytical dashboard. Also some times data analysts will spend their time to capture information for analytics, which might not be available in the organization at all. But for this project we will not do that.\nOnce the Data Masters have the data, they do data cleaning (data wrangling) and data merging (data munging), all this can be done via specialized software. But our data being simple we will do it via PowerBi itself."
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "href": "docs/projects/B1_Sales_Insights.html#data-cleaning-and-data-merging-or-wrangling",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Data Cleaning and Data Merging or Wrangling",
    "text": "Data Cleaning and Data Merging or Wrangling\n\nLoad the data in SQL taken from the Falcons team\n\nFind Basic Insights about the data in SQL.\nFind out few inconsistencies in data in SQL itself and make a note to remove them in PowerBI later.\n\nDo the ETL in PowerBi\n\nFirst Load the data\nDo the data modelling, i.e., link the tables well, along with their relationships.\nStart data transformations on the data to make it good enough for the data analysis.\nOnce the ETL is done, now start building the dashboards\n\n\n\nBasic Insights from DB\n\n\nDB_Schema\n\n\nCode\nfrom IPython import display\ndisplay.Image(\"./images/DB_Schema.jpg\")\n# Can use ![alternative text](path-to-image) but it works out of zsh\n\n\n\n\n\n\n\nDB_Queries\n\nCustomers Table\n\nGet the total number of customers doing business with AtliQ hardwares\n\nInsights: There are total of 38 customers doing business with AtliQ hardwares\n\nSELECT count(*) FROM sales.customers; \nGet count of customers according to their types\n\nInsights: Only 2 customer types i.e.¬†of Brick Mortar and E-Commerce\n\nSELECT count(cs.customer_code) as total_customer_per_type, cs.customer_type \nFROM sales.CUSTOMERS as cs \nGROUP BY cs.customer_type; \nChecking unique customers in the customer category.\n\nInsights: All the 38 customers are unique\n\nSELECT COUNT(DISTINCT tb.custmer_name) as total_unique_customer_name_count\nFROM \n( \n# Seeing whether each customer_code belongs to unique customer name or not. :- It does.\nSELECT cs.custmer_name, COUNT(cs.customer_code) as total_unique_customer_names\nFROM sales.customers as cs\nGROUP BY cs.custmer_name\n) as tb;\n\n\n\nDate Table\n\nInsights: Total unique dates : 1126 and that is the total dates in the table itself.\nSELECT (count(distinct dt.date)) as total_unique_dates  \nFROM sales.date as dt;\nInsight: Seeing the trend of number of transactions each year\nSELECT dt.year, count(dt.year) as total_transactions_in_each_year \nFROM sales.date as dt\nGROUP BY dt.year;\nInsights: We have on average 30 transactions per month in each year\nSELECT \n    dt.month_name,dt.year,count(dt.month_name) as transactions_per_month_per_year\nFROM\n    sales.date AS dt\nGROUP BY dt.year, dt.month_name;\n\n\n\nMarket table\n\nInsights: Total of 17 markets each having it‚Äôs own unique identifier code\nSELECT count(mk.markets_name) \nFROM sales.markets as mk;\nInconsistency:\n\nThere are only 2 international sales in the data, so it‚Äôs better to not consider them as we don‚Äôt have much information about them now.\n\n\n\n\nTransactions Table\n\nInsights: There are a total of 150283 transaction.\nSELECT count(*) FROM sales.transactions;\nInsights: The total_sales_qty: 2444415 and reverified the same via net_sales_per_month_per_year\nSELECT SUM(tr.sales_qty) as net_sales_in_all\nFROM sales.transactions as tr;\nInsights: Finding the different type of currencies used while doing the transactions.\nSELECT distinct(tr.currency) ,count(*) as number_of_transactions_currency_wise\nFROM sales.transactions as tr\nGROUP BY tr.currency;\nInconsistencies:\n\nFew transactions are in USD, they need to be converted to INR\nThe currency column, has an inherent error to show INR and USD 2 times when grouped by currency. The error was because entry in currency field was INR USDhile creating the data.\n\nRemoved inconsistency by formating the table well.\n\n\n\n\n\nProducts Table\n\nInsights: Finding the distribution of product type\nSELECT pr.product_type, count(pr.product_code) AS product_count_forthis_type\nFROM sales.products as pr\nGROUP BY pr.product_type;\nInsights: Every product code is unique\nSELECT count(distinct pr.product_code) \nFROM sales.products as pr;\nSELECT count(*) FROM sales.products;\nSELECT * FROM sales.products;\n\n\n\n\nGeneral Queries\n\nInsights: Getting revenues per year per product\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Getting revenues per year per product in Chennaicustomers\nSELECT st.product_code, SUM(st.sales_amount) as Total_Sales\nFROM sales.transactions as st\nJOIN sales.date as dt\nON st.order_date = dt.date\nWHERE dt.year = 2020 and st.market_code = \"Mark001\"\nGROUP BY st.product_code\nORDER BY Total_sales DESC;\nInsights: Found out which market makes the most of the sales and which one the least.\nSELECT mk.markets_code, sum(tr.sales_qty) as net_sales_per_market, mk.markets_name\nFROM sales.transactions as tr\nJOIN sales.markets as mk\nON tr.market_code = mk.markets_code\nGROUP BY mk.markets_code\nORDER BY net_sales_per_market DESC;\nInsights: Finding which year has made the most of the transactions and in which month\nSELECT sum(net_sales_per_month_per_year)\nFROM \n( \nSELECT dt.year, dt.month_name, sum(tr.sales_qty) AS net_sales_per_month_per_year\nFROM sales.transactions as tr\nJOIN sales.date as dt\nON dt.date = tr.order_date\nGROUP BY dt.year, dt.month_name\nORDER BY net_sales_per_month_per_year\n) AS tb;\nInsights: Finding sales_qty per product, looking at it seems like own brand is in more demand\nSELECT pr.product_code, sum(tr.sales_qty) as net_sales_per_product, pr.product_type\nFROM sales.transactions as tr\nJOIN sales.products as pr\nON pr.product_code = tr.product_code\nGROUP BY pr.product_code\nORDER BY net_sales_per_product DESC;\n\n\n\n\nETL\n\n\nDid a bit of Data Modelling, linking tables well of the DB\nRemoved the inconsistencies in the PowerBI from Market and Transaction table"
  },
  {
    "objectID": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "href": "docs/projects/B1_Sales_Insights.html#dashboard-created-in-powerbi",
    "title": "AtliQ Hardware Dummy Sales Insights",
    "section": "Dashboard Created in PowerBI",
    "text": "Dashboard Created in PowerBI\n\nP.S. Viewing the dashboard requires PowerBI professional account.\n\nIf the viewer doesn‚Äôt have one, kindly use the pdf below to view overall analytical dashboard or the following link to download it.\n\n\nDownload File\n\n\n\n        \n        \n        \n    \n\n\n\n\n\n  \n    \n  \n\n\n\n\nMajor Insights from the Dashboard\n\n\nKey Insights Dashboard\n\nDelhi NCR seems to give major Revenue and Sales Qty followed by Mumbai, Ahemdabad and other city ares.\nBrick n Mortar Seems to contribute to majority of Revenue\nProduct revenue has an error, we can‚Äôt see the major revenue generating product code\nOver the years the revenue trend seems to decline\nRevenue seems to increase in Q3 n Q4 generally\n\nProfit Analysis Dashboard\n\nThough revenue contribution of Delhi n major city markets is more, the profit contribution is not much high for them.\n\nAction points for AtliQ:\n\n\nFocus more on the major profit% generating cities\n\n\nFind why the profit % is less in major revenue generating cities\n\n\n\nFocus on the customers which provide more Profit Margin Contribution by encouraging them like by giving them discounts\n\nPeformance Insights Dashboard\n\nAnalyze the data by breakdown to zones, markets and customers\nObserve the zones that don‚Äôt fulfill the profit margin\nObserve Previous year revenue trend as a comparision with current year revenue trend"
  },
  {
    "objectID": "index.html#recent-blogs",
    "href": "index.html#recent-blogs",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": "üìÆ Recent Blogs",
    "text": "üìÆ Recent Blogs\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nBlog 1\n\n\n\n\n2/9/22\n\n\nBlog 2\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-projects",
    "href": "index.html#latest-projects",
    "title": "Data Science Blog - Yuvraj Dhepe",
    "section": " Latest Projects",
    "text": "Latest Projects\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nAtliQ Hardware Sales Insights\n\n\n\n\n2/9/22\n\n\nSpiritual Youtube Channels Data Scraping\n\n\n\n\n\nNo matching items"
  }
]