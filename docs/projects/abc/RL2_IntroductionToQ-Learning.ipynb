{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59037183-1204-4a34-8152-0d5ade5aeb8d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RL Unit 2: Introduction to Q Learning\"\n",
    "description: \"Unit 2 Learnings from Hugging Face RL Course\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "render-on-save: true\n",
    "execute:\n",
    "    eval: false\n",
    "    echo: true\n",
    "jupyter: python3\n",
    "output:\n",
    "  quarto::html_document:\n",
    "    self_contained: false\n",
    "    keep_md: false\n",
    "\n",
    "categories:\n",
    "    - Re-inforcement Learning\n",
    "    - Regression Project\n",
    "image: ./images/RL2_QLearning.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7a651-5fb4-4e8f-99ca-da5d0a903e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chapter 2: INTRODUCTION TO Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c2209-4b10-4b0b-b4fc-3c2ccd3ca845",
   "metadata": {},
   "source": [
    "- Back in previous class we learned about Reinforcement Learning, the RL process and the different methods to solve an RL problem.\n",
    "- For this Unit we will be learning about:\n",
    "    - Value-based Methods\n",
    "    - Difference between Monte Carlo and Temporal Difference Learning\n",
    "    - Study and implement our first RL algorithm: Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d9069-b9cf-451b-b44d-2fea60dd1c25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RL Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959e20d-5c60-4bf8-b322-50be87fa3c76",
   "metadata": {},
   "source": [
    "- The goal of RL to build an agent that can make smart decisions.\n",
    "- Smart decisions will occur, when the agent will learn from the env, by interacting with it through trial and error and receiving rewards as unique feedback.\n",
    "- It's goal is to maximize it's expected cumulative reward.\n",
    "- Thus we need to train the agent's brain i.e. the policy for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b74696-b943-40d3-b8c0-ac71c84f8a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e0c3d-4990-452c-8676-5f0193ca319b",
   "metadata": {},
   "source": [
    "- Our goal now from maximizing the expected cumulative reward, now changes to learning a policy which maximizes the expected cumulative reward for us\n",
    "- We do this by 2 methods:\n",
    "    - Policy based methods: Train the policy directly to learn which action to take given a state\n",
    "    - Value based methods: Train a value function to learn which state is more valuable and use this value function to take the action that leads to it\n",
    "    \n",
    "- We will be focusing on value based methods for this unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91d5f5-3246-4d38-a6b3-71776789c8af",
   "metadata": {},
   "source": [
    "<img src = 'https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg' height = 600 width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a0564-1aa0-4d2e-ac4f-1c5f38d8c39e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Value-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b377acf-a0b5-4fc9-a2a2-ffab0bee7ac2",
   "metadata": {},
   "source": [
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ffc29-6a29-4192-89e4-94701bdb7618",
   "metadata": {},
   "source": [
    "RL agent's goal is to have an optimal policy $\\pi^*$\n",
    "-  To find this policy we have 2 methods:\n",
    "    - Policy Based Methods: Here we don't need any value function.\n",
    "        -  We don’t define by hand the behavior of our policy; it’s the training that will define it.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb81d5-cd9c-490c-8d36-fe44fce7aebf",
   "metadata": {},
   "source": [
    "- Value Based Methods: Indirectly, by training a value function that outputs the value of a state or a state-action pair. \n",
    "    - Given this value function, our policy will take an action.\n",
    "    - Since the policy is not trained/learned, we need to specify it's behaviour by hand. \n",
    "    - For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we'll have a Greedy Policy\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c5658-f871-4eb3-8a98-7f3d38e1a636",
   "metadata": {},
   "source": [
    "- Consequently, whatever method you use to solve your problem, you will have a policy. In the case of value-based methods, you don’t train the policy: your policy is just a simple pre-specified function (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.\n",
    "\n",
    "- So the difference is:\n",
    "    - In policy-based training, the optimal policy (denoted π*) is found by training the policy directly.\n",
    "    - In value-based training, finding an optimal value function (denoted Q* or V*, we’ll study the difference below) leads to having an optimal policy.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" height = 600 width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1eef3-a7f5-42d6-b70e-48bd10ecc0d6",
   "metadata": {},
   "source": [
    "- In Value based methods we have 2 types of value based functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd8d3f-81c1-4200-81a3-7522e804f94b",
   "metadata": {},
   "source": [
    "- State value function under a policy $\\pi$ \n",
    "    - For each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer)\n",
    "    - In Value based methods we have 2 types of value based functions:- If we take the state with value -7: it's the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.\n",
    "\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" width = 400 height  = 400> <img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" height = 400 width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842e995-83b2-44f0-8921-1371eddcba47",
   "metadata": {},
   "source": [
    "- Action value function \n",
    "In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.\n",
    "\n",
    "- The value of taking action $a$ in the state $s$ under a policy $\\pi$ is :\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" height = 400 width = 400><img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" height = 400 width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ff74d-23bd-4499-8d0d-8395f15e3d33",
   "metadata": {},
   "source": [
    "- We see that the difference is:\n",
    "    - For the state-value function, we calculate the value of a state $S_t$\n",
    "    - For the action-value functions, we calculate the value of the state-action pair $(S_t, A_t)$ hence the value of taking that action at that state\n",
    "\n",
    "<img src = 'https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg' width = 400 height = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b396f1e-ed58-4741-b4f2-74470c4e58ee",
   "metadata": {},
   "source": [
    "- In either case, whichever value function we choose (state-value or action-value function), the returned value is the expected return.\n",
    "- However, the problem is that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.\n",
    "- This can be a computationally expensive process, and that’s where the Bellman equation comes in to help us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6fdd0-46fd-40ff-8914-54ad739f2d8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Bellman Equation to simplify the value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89431783-db29-49bb-b50c-0e9cd77f4142",
   "metadata": {},
   "source": [
    "- With what we have learned so far, we know that if we calculate $V(S_t)$, we need to calculate the return starting at that state and then follow the policy forever after. (The policy we defined in the following example is a Greedy Policy; for simplification, we don't discount the reward).\n",
    "- So to calculate $V(S_t)$, we need to calculate the **sum** of the expected rewards. Hence: \n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "- Then to calculate $V(S_{t+1})$, we need to calculate the return starting at that state $S_{t+1}$\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg\" width = 400 height = 400>\n",
    "\n",
    "- So basically we're repeating the computation for the value of different states, which can be tedious if needs to be done for each state value or state-action value.\n",
    "- So to simplify this we use Bellman equation which is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:\n",
    "    - The immedicate reward $R_{t+1}+$ the discounted value of the state that follows (gamma * $V(S_{t+1})$)\n",
    "    - If we go back to our example, we can say that the value of State 1 is equal to the expected cumulative return if we start at that state.\n",
    "    - To calculate the value of State 1: the sum of rewards if the agent started in that state 1 and then followed the policy for all the time steps.\n",
    "    - This is equivalent to $V(S_t) = $ Immediate reward $R_{t+1}$ + Discounted value of the next state ($\\gamma * V(S_{t+1}))$\n",
    "\n",
    "- In the interest of simplicity, here we don't discount, so gamma= 1. But you'll study an example with gamma = 0.99 in the Q-Learning section of this unit.\n",
    "    - The value of $V(S_{t+1}) = $ Immediate reward $R_{t+2}$ + Discounted value of the next state ($\\gamma * V(S_{t+2}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665e038-cec4-488c-9523-b668e3b4935a",
   "metadata": {},
   "source": [
    "- To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows.\n",
    "- Before going to the next section, think about the role of gamma in the Bellman equation. What happens if the value of gamma is very low (e.g. 0.1 or even 0)? What happens if the value is 1? What happens if the value is very high, such as a million?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4b262-5059-4f77-9c2d-e42d6210ebdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Monte Carlo vs Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95be05-6866-4b96-b0b3-61e81f242717",
   "metadata": {},
   "source": [
    "- Since we know that the RL agent learns by interacting with the environment. \n",
    "- The idea is that given the experience and the received reward, the agent will update it's value function of policy.\n",
    "- There are 2 different strategies on how to train our value function or policy function. \n",
    "    - Both of them use experience to solve the RL problem, i.e. the SARSA\n",
    "    - The 2 strategies are Monte Carlo and Temporal Differnce\n",
    "        - Monte Carlo uses an entire episode of experience before learning. \n",
    "        - Temporal difference uses only a step. $(S_t, A_t, R_{t+1}, S{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa25e87-6015-402e-9a97-e005b0c3b723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Monte Carlo: Learning at the end of the episode\n",
    "    \n",
    "- Monte Carlo waits until the end of the episode, calculates $G_t$ (return) and uses it as a target for updating $V(S_t)$\n",
    "- So it requires a complete episode of interaction before updating our value function.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" height = 400 width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75f0fa-cd38-400d-9653-131998cae6eb",
   "metadata": {},
   "source": [
    "- We always start the episode at the same starting point.\n",
    "- The agent takes actions using the policy. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.\n",
    "- We get the reward and the next state.\n",
    "- We terminate the episode if the cat eats the mouse or if the mouse moves > 10 steps.\n",
    "- At the end of the episode, we have a list of State, Actions, Rewards, and Next States tuples For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]…]\n",
    "- The agent will sum the total rewards $G_t$ (to see how well it did).\n",
    "- It will then update $V(S_t)$ based on this formula\n",
    "- Then start a new game with this new knowledge\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg\" height = 400 width = 400>  <img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" height = 400 width = 400>\n",
    "\n",
    "\n",
    "- For instance, if we train a state-value function using Monte Carlo:\n",
    "\n",
    "- We initialize our value function so that it returns 0 value for each state\n",
    "- Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount)\n",
    "- Our mouse explores the environment and takes random actions\n",
    "- The mouse made more than 10 steps, so the episode ends .\n",
    "\n",
    "- We have a list of state, action rewards, next_state, we need to calculate the return $G_t$\n",
    "- $G_t = R_{t+1} +R_{t+2}+R_{t+3} $....\n",
    "- $G_t = R_{t+1} +R_{t+2}+R_{t+3} $....(for simplicity we don't discount the rewards)\n",
    "- $G_t = 1+0+0+0+0+0+1+1+0+0$\n",
    "- $G_t = 3$ \n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "- **I think it's a hyperparameter, about how many times we iterate before we update the single state**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d997494-b595-43b9-a9f2-c696f2c278bb",
   "metadata": {},
   "source": [
    "#### Temporal Difference Learning: learning at each step\n",
    "- Temporal differnce, on the other hand, waits for only one interaction (one step) $S_{t+1}$ to form a TD target and update $V(S_t)$ using $R_{t+1}$ and $\\gamma*V(S_{t+1})$\n",
    "- The idea with TD is to update the $V(S_t)$ at each step\n",
    "- But because we didn't experience an entire episode, we don't have $G_t$ (expected return). Instead, we estimate $G_t$ by adding $R_{t+1}$(reward that came by current action) and the discounted value of the next state\n",
    "- This is called bootstrapping. It's called this because TD bases it's update in part on an existing estimate $V(S_{t+1})$ and not a complete sample $G_t$\n",
    "- This method is called TD(0) or one-step TD (update the value function after any individual step)\n",
    "\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" height = 400 width = 400> <img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" height = 400 width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c2e9d-c9f8-4eb2-aaa8-26ff24b117a2",
   "metadata": {},
   "source": [
    "- For the mouse cat example we would have something as follows \n",
    "    - We initialize our value function so that it returns 0 value for each state.\n",
    "    - Our learning rate (lr) is 0.1, and our discount rate is 1 (no discount).\n",
    "    - Our mouse begins to explore the environment and takes a random action: going to the left\n",
    "    - It gets a reward $R_{t+1}$ since it eats a piece of cheese\n",
    "    \n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg\" height = 400 width = 400>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg\" height = 400 width = 400>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg\" height = 400 width = 400>\n",
    "\n",
    "- **Here as well, I believe it is a hyper parameter, how many times is a state updated**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b940a7-e3f5-4725-8c57-cdebdd4f273a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### To summarize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bd3f2-f1a3-41d8-8685-c23aebd37567",
   "metadata": {
    "tags": []
   },
   "source": [
    "- So basically first we decide which way to train our policy, once that is decided, we ask how do we train that's where these 2 strategies come into picture.\n",
    "- With Monte Carlo, we update the value function from a complete episode, and so we use the actual accurate discounted return of this episode.\n",
    "- With temporal difference learning, we update the value function from a step, and we replace $G_t$, which we don't know with an estimated return called the TD target.\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg\" height = 400 width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531e996-3b83-4135-88fb-d79b8fbfb0a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introducing Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccc5e4-0584-4669-a124-e5352588ed5f",
   "metadata": {},
   "source": [
    "What is Q-Learning ?\n",
    "- Q learning is an off-policy value-based method that uses a temporal difference approach to train it's action value function:\n",
    "    - Off-policy: We'll see this at the end.\n",
    "    - Value-based method: Finds the optimal policy indirectly by training a value or action-value function that will tell us the value of each state or each state-action pair.\n",
    "    - TD approach: updates its action-value function at each step intead of at the end of the episode.\n",
    "\n",
    "- Q-Learning is the algorithm we use to train our Q-function, an action-value function that determines the value of being at a particular state and taking a specific action at that state.\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" width = 400 height = 400>\n",
    "\n",
    "- In Q-Learning the Q stands for quality (the value) of that action at that state.\n",
    "Also to recap here is the difference between value and reward:\n",
    "- The value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to it's policy.\n",
    "- The reward is the feedback the agent gets' from the environment after performing an action at a state.\n",
    "\n",
    "- Internally, our Q-function is encoded by a **Q-table, a table where each cell corresponds to a state-action pair value.** Think of this Q-table as **the memory or cheat sheet of our Q-function**.\n",
    "\n",
    "- So overall in Q-learning we train our action value function known as Q-function. This Q-function is encoded as a Q-table, where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d64173-2879-4ef1-b7c1-7ef3a9666eb3",
   "metadata": {},
   "source": [
    "- Let's take an example with this simple maze:\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg\" height = 400 width = 400>\n",
    "\n",
    "- The Q-table is initialized. That's why all the values are = 0. This table contains, for each state and action, the corresponding state-action values.\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg\" height = 400 width = 400>\n",
    "\n",
    "- Here we see that the state-action value of the initial state and going up is 0:\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg\" height = 400 width = 400>\n",
    "\n",
    "- So: the Q-function uses a Q-table that has the value of each state-action pair. Given a state and action, our Q-function will seacrch inside it's Q-table to output the value.\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" height = 400 width = 400>\n",
    "\n",
    "If we recap, Q-Learning is the RL algorithm that:\n",
    "\n",
    "- Trains a Q-function (an action-value function), which internally is a Q-table that contains all the state-action pair values.\n",
    "- Given a state and action, our Q-function will search its Q-table for the corresponding value.\n",
    "- When the training is done, we have an optimal Q-function, which means we have optimal Q-table.\n",
    "- And if we have an optimal Q-function, we have an optimal policy since we know the best action to take at each state.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" height = 400 width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090657b9-8945-419b-8f60-4d1acacff1a0",
   "metadata": {},
   "source": [
    "- In the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time, we initialize the Q-table to 0). As the agent explores the environment and we update the Q-table, it will give us a better and better approximation to the optimal policy.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg\" width = 400 height = 400>\n",
    "\n",
    "- Now that we understand what Q-Learning, Q-functions, and Q-tables are, let’s dive deeper into the Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf26a6-f889-4f8e-b166-758de6b22b8a",
   "metadata": {},
   "source": [
    "#### The Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a246b-080a-4c71-bb9f-ad4c02148e8a",
   "metadata": {},
   "source": [
    "- This is the Q-Learning pseudocode; let’s study each part and see how it works with a simple example before implementing it. Don’t be intimidated by it, it’s simpler than it looks! We’ll go over each step.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" width = 400 height = 400>\n",
    "- Step 1: We initialize the Q-table, **most of the time, we initialize with values of 0**\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg\" width = 400 height = 400>\n",
    "\n",
    "- Step 2: Choose an action using the epsilon-greedy strategy\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" width = 400 height = 400>\n",
    "\n",
    "- The idea is that, with an initial value of ɛ = 1.0:\n",
    "    - With probability 1 — ɛ : we do exploitation (aka our agent selects the action with the highest state-action pair value).\n",
    "    - With probability ɛ: we do exploration (trying random action).\n",
    "\n",
    "- At the beginning of the training, the probability of doing exploration will be huge since ɛ is very high, so most of the time, we’ll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "- Step 3: Perform action $A_t$, get reward $R_{t+1}$ and the next state $S_{t+1}$\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg\" width = 400 height = 400>\n",
    "\n",
    "- Step 4: Update Q($S_t, A_t$)\n",
    "\n",
    "Remember that in TD Learning, we update our policy or value function (depending on the RL method we choose) after one step of the interaction.\n",
    "\n",
    "- To produce our TD target, we used the immediate reward $R_{t+1}$ plus the discounted value of the next state, computed by **finding the action** that **maximizes the current Q-function at the next state.** (We call that bootstrap).\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" width = 400 height = 400>\n",
    "\n",
    "- Therefore, our Q($S_t, A_t$) update formula goes like this:\n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" width = 400 height = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40119039-dab5-4cd6-b628-a6ceb2c8e445",
   "metadata": {},
   "source": [
    "- This means to update our $Q(S_t, A_t)$:\n",
    "    - We need $S_t, A_t, R_{t+1}, S_{t+1}$\n",
    "    - To update our Q-value at a given state-action pair, we use the TD target.\n",
    "\n",
    "How do we form the TD target?\n",
    "- We obtain the reward after taking the action $R_{t+1}$\n",
    "- To get this best state-action pair value for the next state, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value (So there is no probability involved here simply choose that action which will take us to the next state having max Q-value and thus our Q-value for this state becomes optimal)\n",
    "- Then when the update of this Q-value is done, we start in a new state (which will come by the action that leads us to that state which has the best Q-value) and select our action using a epsilon-greedy policy again\n",
    "- This is why we say that Q Learning is an off-policy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d38f1-6e0d-4bf6-a279-2a9dc7eb67c5",
   "metadata": {},
   "source": [
    "#### Off-policy vs On-policy\n",
    "- The difference is subtle:\n",
    "    - Off-policy: using a different policy for acting (inference) and updating (training).\n",
    "        - For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).\n",
    "        -  Each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained. \n",
    "\n",
    "- Acting Policy:\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg\" width = 600 height =600>\n",
    "\n",
    "Is different from the policy we use during the training part:\n",
    "\n",
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg\" width = 300 height = 300>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc41515-a953-4606-b471-f0c40a76ed69",
   "metadata": {},
   "source": [
    "- On-policy: using the same policy for acting and updating.\n",
    "    - For instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy\n",
    "    - Each update only usees data collected while acting according to the most recent version of the policy\n",
    "    \n",
    "\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fbb92-a1be-4142-b7fb-5b2985464133",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589beb3-66d8-4523-83a7-4be141b4ac4a",
   "metadata": {},
   "source": [
    "#### Let's discuss this with an example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6df38-0f8a-4210-b808-d857eff82a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-Example-2.jpg\" width = 600 height = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-1.jpg\" height = 600 width = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-2.jpg\" height = 600 width = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-1.jpg\" height = 600 width = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-3.jpg\" height = 600 width = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-4.jpg\" height = 600 width = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" height = 600 width = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-4.jpg\" width = 600 height = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-6.jpg\" width = 600 height = 600>|\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-7.jpg\" width = 600 height = 600>\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-8.jpg\" width = 600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e062228-1a55-40c7-8aed-091ec86e10aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12220f2-19e9-4692-b37c-1006a1cdf1ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summary of Value Based Functions and Strategies to train the value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816061a9-665d-4ffb-a138-a867e3680a3d",
   "metadata": {},
   "source": [
    "- We have two types of value-based functions:\n",
    "    - State-value function: outputs the expected return if the agent starts at a given state and acts according to the policy forever after.\n",
    "    - Action-value function: outputs the expected return if the agent starts in a given state, takes a given action at that state and then acts accordingly to the policy forever after.\n",
    "    - In value-based methods, rather than learning the policy, we define the policy by hand and we learn a value function. If we have an optimal value function, we will have an optimal policy.\n",
    "\n",
    "- There are two types of methods to learn a policy for a value function:\n",
    "    - With the Monte Carlo method, we update the value function from a complete episode, and so we use the actual discounted return of this episode.\n",
    "    - With the TD Learning method, we update the value function from a step, replacing the unknown $G_t$ with an estimated return called the TD target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e3716-3f12-476a-85c3-2833419acd97",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg\" width =600 height = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d5fc7-e47c-444e-ae26-2b490e3565db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Q-Learning Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3c9cf-96e4-435f-9c48-87fae806a0d8",
   "metadata": {},
   "source": [
    "- Q-Learning is the RL algorithm that :\n",
    "    - Trains a Q-function, an action-value function encoded, in internal memory, by a Q-table containing all the state-action pair values.\n",
    "    - Given a state and action, our Q-function will search its Q-table for the corresponding value.\n",
    "<img src = \"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" width = 600 height = 600>    \n",
    "\n",
    "- When the training is done, we have an optimal Q-function, or, equivalently, an optimal Q-table.\n",
    "- And if we have an optimal Q-function, we have an optimal policy, since we know, for each state, the best action to take.\n",
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" width = 600 height =600>\n",
    "\n",
    "- But, in the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time we initialize the Q-table to 0 values). But, as we explore the environment and update our Q-table it will give us a better and better approximation.\n",
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" width =600 height =600 >\n",
    "\n",
    "- This is the Q-Learning pseudocode:\n",
    "<img src =\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" height = 600  width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f728768-94a2-45b8-a253-d762ab86b25c",
   "metadata": {},
   "source": [
    "### Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abe2b7-77f4-4699-891a-a7af4d7b67e5",
   "metadata": {},
   "source": [
    "- **Strategies to find the optimal policy**\n",
    "    - Policy-based methods. The policy is usually trained with a neural network to select what action to take given a state. In this case it is the neural network which outputs the action that the agent should take instead of using a value function. Depending on the experience received by the environment, the neural network will be re-adjusted and will provide better actions.\n",
    "    - Value-based methods. In this case, a value function is trained to output the value of a state or a state-action pair that will represent our policy. However, this value doesn’t define what action the agent should take. In contrast, we need to specify the behavior of the agent given the output of the value function. For example, we could decide to adopt a policy to take the action that always leads to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy (or whatever decision the user takes) that uses the values of the value-function to decide the actions to take.\n",
    "    \n",
    "- **Among the value-based methods, we can find two main strategies**\n",
    "    - The state-value function. For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.\n",
    "    - The action-value function. In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state and takes an action. Then it follows the policy forever after.\n",
    "\n",
    "- **Epsilon-greedy strategy:**\n",
    "    - Common strategy used in reinforcement learning that involves balancing exploration and exploitation.\n",
    "    - Chooses the action with the highest expected reward with a probability of 1-epsilon.\n",
    "    - Chooses a random action with a probability of epsilon.\n",
    "    - Epsilon is typically decreased over time to shift focus towards exploitation.\n",
    "\n",
    "- **Greedy strategy:**\n",
    "    - Involves always choosing the action that is expected to lead to the highest reward, based on the current knowledge of the environment. (Only exploitation)\n",
    "    - Always chooses the action with the highest expected reward.\n",
    "    - Does not include any exploration.\n",
    "    - Can be disadvantageous in environments with uncertainty or unknown optimal actions.\n",
    "\n",
    "- **Off-policy vs on-policy algorithms**\n",
    "    - Off-policy algorithms: A different policy is used at training time and inference time\n",
    "    - On-policy algorithms: The same policy is used during training and inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
